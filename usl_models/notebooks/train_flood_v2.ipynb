{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloodML Training Notebook\n",
    "\n",
    "Clean, single-purpose notebook for training the FloodConvLSTM model.\n",
    "\n",
    "**Sections:**\n",
    "1. Configuration - all settings in one place\n",
    "2. Environment Setup - imports, GPU, seeds\n",
    "3. Download Data (Optional) - skip if data already cached\n",
    "4. Load Data - from local cache\n",
    "5. Model Building - create FloodModel with params\n",
    "6. Training - fit with callbacks\n",
    "7. Evaluation - quick validation metrics\n",
    "8. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration\n",
    "\n",
    "Edit these values to customize your training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these values\n",
    "# ============================================================================\n",
    "\n",
    "# --- Data paths ---\n",
    "FILECACHE_DIR = \"/home/shared/climateiq/filecache\"\n",
    "\n",
    "# --- Download settings ---\n",
    "# Set to True ONLY if you need to download data from GCS\n",
    "# Set to False to skip download (use existing cache)\n",
    "DOWNLOAD_DATA = False\n",
    "\n",
    "# --- Cities and rainfall scenarios ---\n",
    "# Format: {\"CityName\": \"config_folder\"}\n",
    "CITY_CONFIG = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "    # \"Atlanta\": \"Atlanta_config\",\n",
    "    # \"Phoenix_SM\": \"PHX_SM\",\n",
    "    # \"Phoenix_PV\": \"PHX_PV\",\n",
    "    # \"Phoenix_central\": \"PHX_CCC\",\n",
    "}\n",
    "\n",
    "# Rainfall scenario IDs to train on\n",
    "RAINFALL_IDS = [5, 7, 9, 10, 11, 12, 13, 15, 16]  # Add more as needed\n",
    "\n",
    "# Dataset splits to download/use\n",
    "DATASET_SPLITS = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# --- Model hyperparameters ---\n",
    "LSTM_UNITS = 128          # ConvLSTM hidden units (64, 128, or 256)\n",
    "LSTM_KERNEL_SIZE = 3      # ConvLSTM kernel size (3 or 5)\n",
    "LSTM_DROPOUT = 0.2        # Dropout rate\n",
    "LSTM_RECURRENT_DROPOUT = 0.2\n",
    "N_FLOOD_MAPS = 5          # Timesteps of flood history as input\n",
    "M_RAINFALL = 6            # Number of temporal features\n",
    "\n",
    "# --- Training settings ---\n",
    "BATCH_SIZE = 4            # Reduce if OOM errors occur\n",
    "EPOCHS = 50               # Number of training epochs\n",
    "EARLY_STOPPING_PATIENCE = 10  # Stop if val_loss doesn't improve\n",
    "MAX_CHUNKS = None         # None = use all chunks, or set integer for quick tests\n",
    "\n",
    "# --- Output ---\n",
    "import time\n",
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "LOG_DIR = f\"logs/flood_training_{TIMESTAMP}\"\n",
    "\n",
    "print(f\"Download data: {DOWNLOAD_DATA}\")\n",
    "print(f\"Log directory: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules during development\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "# Suppress TF info messages\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "keras.utils.set_random_seed(SEED)\n",
    "\n",
    "# Enable GPU memory growth (prevents OOM)\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import flood model components\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.dataset import (\n",
    "    load_dataset_windowed_cached,\n",
    "    load_dataset_cached,\n",
    "    download_dataset,\n",
    ")\n",
    "from usl_models.flood_ml import constants\n",
    "\n",
    "print(f\"Constants: MAP_SIZE={constants.MAP_HEIGHT}x{constants.MAP_WIDTH}, \"\n",
    "      f\"N_FLOOD_MAPS={constants.N_FLOOD_MAPS}, M_RAINFALL={constants.M_RAINFALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of simulation names (used for both download and training)\n",
    "sim_names = []\n",
    "for city, config in CITY_CONFIG.items():\n",
    "    for rain_id in RAINFALL_IDS:\n",
    "        sim_names.append(f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\")\n",
    "\n",
    "print(f\"Configured {len(sim_names)} simulations:\")\n",
    "for s in sim_names:\n",
    "    exists = (pathlib.Path(FILECACHE_DIR) / s).exists()\n",
    "    status = \"CACHED\" if exists else \"NOT CACHED\"\n",
    "    print(f\"  [{status}] {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Download Data (Optional)\n",
    "\n",
    "**Skip this section if data is already cached.**\n",
    "\n",
    "Set `DOWNLOAD_DATA = True` in Configuration to enable download from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from GCS to local filecache\n",
    "# This cell is skipped if DOWNLOAD_DATA = False\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DOWNLOADING DATA FROM GCS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Target directory: {FILECACHE_DIR}\")\n",
    "    print(f\"Simulations: {len(sim_names)}\")\n",
    "    print(f\"Splits: {DATASET_SPLITS}\")\n",
    "    print()\n",
    "    \n",
    "    download_dataset(\n",
    "        sim_names=sim_names,\n",
    "        output_path=pathlib.Path(FILECACHE_DIR),\n",
    "        dataset_splits=DATASET_SPLITS,\n",
    "        include_labels=True,  # Training mode needs labels\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDownload complete!\")\n",
    "else:\n",
    "    print(\"Skipping download (DOWNLOAD_DATA=False)\")\n",
    "    print(\"Using existing cached data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Data\n",
    "\n",
    "Load cached datasets for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data exists before loading\n",
    "missing = []\n",
    "for s in sim_names:\n",
    "    if not (pathlib.Path(FILECACHE_DIR) / s).exists():\n",
    "        missing.append(s)\n",
    "\n",
    "if missing:\n",
    "    print(\"ERROR: The following simulations are not cached:\")\n",
    "    for s in missing:\n",
    "        print(f\"  - {s}\")\n",
    "    print(\"\\nSet DOWNLOAD_DATA=True and re-run, or check FILECACHE_DIR path.\")\n",
    "    raise FileNotFoundError(f\"{len(missing)} simulations not found in cache\")\n",
    "else:\n",
    "    print(f\"All {len(sim_names)} simulations found in cache.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset (windowed for teacher-forcing)\n",
    "print(\"Loading training dataset...\")\n",
    "train_dataset = load_dataset_windowed_cached(\n",
    "    filecache_dir=pathlib.Path(FILECACHE_DIR),\n",
    "    sim_names=sim_names,\n",
    "    dataset_split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_flood_maps=N_FLOOD_MAPS,\n",
    "    m_rainfall=M_RAINFALL,\n",
    "    max_chunks=MAX_CHUNKS,\n",
    "    shuffle=True,\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Load validation dataset\n",
    "print(\"Loading validation dataset...\")\n",
    "val_dataset = load_dataset_windowed_cached(\n",
    "    filecache_dir=pathlib.Path(FILECACHE_DIR),\n",
    "    sim_names=sim_names,\n",
    "    dataset_split=\"val\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_flood_maps=N_FLOOD_MAPS,\n",
    "    m_rainfall=M_RAINFALL,\n",
    "    max_chunks=MAX_CHUNKS,\n",
    "    shuffle=False,\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset shapes\n",
    "for inputs, labels in train_dataset.take(1):\n",
    "    print(\"Sample batch shapes:\")\n",
    "    print(f\"  geospatial:    {inputs['geospatial'].shape}\")\n",
    "    print(f\"  temporal:      {inputs['temporal'].shape}\")\n",
    "    print(f\"  spatiotemporal:{inputs['spatiotemporal'].shape}\")\n",
    "    print(f\"  labels:        {labels.shape}\")\n",
    "    \n",
    "    # Verify temporal features are distinct (bug fix check)\n",
    "    t = inputs['temporal'].numpy()\n",
    "    if t.max() > 0:  # Skip if all-zero window\n",
    "        cols_distinct = not np.allclose(t[0, :, 0], t[0, :, 1])\n",
    "        print(f\"  Temporal features distinct: {cols_distinct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model parameters\n",
    "params = FloodModel.Params(\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    lstm_kernel_size=LSTM_KERNEL_SIZE,\n",
    "    lstm_dropout=LSTM_DROPOUT,\n",
    "    lstm_recurrent_dropout=LSTM_RECURRENT_DROPOUT,\n",
    "    n_flood_maps=N_FLOOD_MAPS,\n",
    "    m_rainfall=M_RAINFALL,\n",
    ")\n",
    "\n",
    "# Build model\n",
    "model = FloodModel(params=params)\n",
    "\n",
    "print(\"Model parameters:\")\n",
    "for k, v in params.to_dict().items():\n",
    "    if k != \"optimizer\":\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model summary\n",
    "model._model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # TensorBoard logging\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=LOG_DIR,\n",
    "        histogram_freq=0,\n",
    "        profile_batch=0,\n",
    "    ),\n",
    "    # Save best model\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{LOG_DIR}/best_model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "    # Early stopping\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Training for up to {EPOCHS} epochs...\")\n",
    "print(f\"Logs will be saved to: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "if 'mean_absolute_error' in history.history:\n",
    "    axes[1].plot(history.history['mean_absolute_error'], label='Train')\n",
    "    axes[1].plot(history.history['val_mean_absolute_error'], label='Validation')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE (m)')\n",
    "    axes[1].set_title('Mean Absolute Error')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{LOG_DIR}/training_history.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal metrics:\")\n",
    "print(f\"  Train loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"  Val loss:   {history.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Quick Evaluation\n",
    "\n",
    "Run a quick prediction to verify the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one validation sample for quick test\n",
    "from usl_models.flood_ml.dataset import _iter_model_inputs_cached\n",
    "\n",
    "# Get one raw (non-windowed) sample for call_n test\n",
    "sim_dir = pathlib.Path(FILECACHE_DIR) / sim_names[0]\n",
    "raw_gen = _iter_model_inputs_cached(\n",
    "    sim_dir=sim_dir,\n",
    "    dataset_split=\"val\",\n",
    "    n_flood_maps=N_FLOOD_MAPS,\n",
    "    m_rainfall=M_RAINFALL,\n",
    "    max_chunks=1,\n",
    "    include_labels=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "raw_input, raw_labels, chunk_name = next(raw_gen)\n",
    "print(f\"Testing on chunk: {chunk_name}\")\n",
    "print(f\"Labels shape: {raw_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run autoregressive prediction\n",
    "N_STEPS = raw_labels.shape[0]  # Match label timesteps\n",
    "\n",
    "batch_input = FloodModel.Input(\n",
    "    geospatial=raw_input[\"geospatial\"][tf.newaxis],\n",
    "    temporal=raw_input[\"temporal\"][tf.newaxis],\n",
    "    spatiotemporal=raw_input[\"spatiotemporal\"][tf.newaxis],\n",
    ")\n",
    "\n",
    "predictions = model.call_n(batch_input, n=N_STEPS)\n",
    "print(f\"Predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare prediction vs ground truth\n",
    "pred_np = np.clip(predictions.numpy()[0], 0, None)  # Clip negatives\n",
    "label_np = raw_labels.numpy()\n",
    "\n",
    "print(\"Per-timestep comparison:\")\n",
    "print(f\"{'t':>3} | {'GT max':>8} {'GT mean':>10} | {'Pred max':>8} {'Pred mean':>10} | {'MAE':>8}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for t in range(min(N_STEPS, label_np.shape[0])):\n",
    "    gt = label_np[t]\n",
    "    pd = pred_np[t]\n",
    "    mae = np.abs(gt - pd).mean()\n",
    "    print(f\"{t:>3} | {gt.max():>8.3f} {gt.mean():>10.6f} | {pd.max():>8.3f} {pd.mean():>10.6f} | {mae:>8.5f}\")\n",
    "\n",
    "overall_mae = np.abs(label_np[:N_STEPS] - pred_np[:N_STEPS]).mean()\n",
    "print(\"-\" * 70)\n",
    "print(f\"Overall MAE: {overall_mae:.5f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a few timesteps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timesteps_to_show = [0, 4, 8, 12] if N_STEPS > 12 else list(range(min(4, N_STEPS)))\n",
    "fig, axes = plt.subplots(len(timesteps_to_show), 2, figsize=(10, 4*len(timesteps_to_show)))\n",
    "\n",
    "for i, t in enumerate(timesteps_to_show):\n",
    "    if t >= N_STEPS:\n",
    "        continue\n",
    "    gt = label_np[t]\n",
    "    pd = pred_np[t]\n",
    "    vmax = max(gt.max(), pd.max(), 0.1)\n",
    "    \n",
    "    axes[i, 0].imshow(gt, cmap='Blues', vmin=0, vmax=vmax)\n",
    "    axes[i, 0].set_title(f't={t} Ground Truth (max={gt.max():.2f}m)')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    im = axes[i, 1].imshow(pd, cmap='Blues', vmin=0, vmax=vmax)\n",
    "    axes[i, 1].set_title(f't={t} Prediction (max={pd.max():.2f}m)')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{LOG_DIR}/prediction_sample.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "final_model_path = f\"{LOG_DIR}/final_model.keras\"\n",
    "model.save_model(final_model_path)\n",
    "\n",
    "print(f\"\\nModel saved to: {final_model_path}\")\n",
    "print(f\"Best checkpoint: {LOG_DIR}/best_model.keras\")\n",
    "print(f\"\\nTo load this model for prediction:\")\n",
    "print(f'  model = FloodModel.from_checkpoint(\"{final_model_path}\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training config for reference\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    \"sim_names\": sim_names,\n",
    "    \"params\": params.to_dict(),\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs_trained\": len(history.history['loss']),\n",
    "    \"final_train_loss\": float(history.history['loss'][-1]),\n",
    "    \"final_val_loss\": float(history.history['val_loss'][-1]),\n",
    "}\n",
    "\n",
    "with open(f\"{LOG_DIR}/training_config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Config saved to: {LOG_DIR}/training_config.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
