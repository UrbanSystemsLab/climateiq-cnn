{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloodML Training Notebook\n",
    "\n",
    "Train the FloodConvLSTM model with support for:\n",
    "- **Full maps** (1000x1000) - original approach\n",
    "- **Patches** (e.g., 256x256) - sliding window with flood filtering\n",
    "\n",
    "**Sections:**\n",
    "1. Configuration\n",
    "2. Environment Setup\n",
    "3. Download Data (Optional)\n",
    "4. Load Data\n",
    "5. Model Building\n",
    "6. Training\n",
    "7. Evaluation\n",
    "8. Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURATION - Edit these values\n# ============================================================================\n\n# --- Data paths ---\nFILECACHE_DIR = \"/home/shared/climateiq/filecache\"\n\n# --- Download settings ---\nDOWNLOAD_DATA = False  # Set True to download from GCS\n\n# --- Cities and rainfall scenarios ---\nCITY_CONFIG = {\n    \"Manhattan\": \"Manhattan_config\",\n    # \"Atlanta\": \"Atlanta_config\",\n    # \"Phoenix_SM\": \"PHX_SM\",\n}\n\nRAINFALL_IDS = [5, 7, 9, 10, 11, 12, 13, 15, 16]\nDATASET_SPLITS = [\"train\", \"val\", \"test\"]\n\n# ==========================================================================\n# PATCH MODE vs FULL MAP MODE\n# ==========================================================================\nUSE_PATCHES = True\n\n# --- Patch settings (only used if USE_PATCHES=True) ---\nPATCH_SIZE = 256\nPATCH_STRIDE = 128\nMIN_FLOOD_FRACTION = 0.01\nMIN_MAX_DEPTH = 0.1\nMAX_PATCHES_PER_CHUNK = 20\n\n# ==========================================================================\n# HYPERPARAMETER TUNING vs FIXED PARAMS\n# ==========================================================================\n# RUN_HYPERTUNING = True  -> Run Bayesian optimization to find best params\n# RUN_HYPERTUNING = False -> Use fixed hyperparameters below\nRUN_HYPERTUNING = True\n\n# --- Hypertuning settings (only used if RUN_HYPERTUNING=True) ---\nHTUNE_MAX_TRIALS = 5           # Number of hyperparameter combinations to try\nHTUNE_EPOCHS = 10              # Epochs per trial\nHTUNE_TRAIN_SAMPLES = 200      # Training samples per trial (for speed)\nHTUNE_VAL_SAMPLES = 50         # Validation samples per trial\n\n# --- Hyperparameter search space ---\nHTUNE_LSTM_UNITS = [32, 64, 128]\nHTUNE_LSTM_KERNEL_SIZE = [3, 5]\nHTUNE_LSTM_DROPOUT = [0.2, 0.3]\nHTUNE_LSTM_RECURRENT_DROPOUT = [0.2, 0.3]\n\n# --- Fixed hyperparameters (used if RUN_HYPERTUNING=False) ---\nLSTM_UNITS = 128\nLSTM_KERNEL_SIZE = 3\nLSTM_DROPOUT = 0.2\nLSTM_RECURRENT_DROPOUT = 0.2\n\n# --- Constants (don't change unless you know what you're doing) ---\nN_FLOOD_MAPS = 5\nM_RAINFALL = 6\n\n# --- Training settings ---\nBATCH_SIZE = 4 if USE_PATCHES else 2\nEPOCHS = 50\nEARLY_STOPPING_PATIENCE = 10\nMAX_CHUNKS = None  # None = all, or integer for quick test\n\n# --- Output ---\nimport time\nTIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\nmode_str = f\"patches{PATCH_SIZE}\" if USE_PATCHES else \"fullmap\"\nhtune_str = \"htune_\" if RUN_HYPERTUNING else \"\"\nLOG_DIR = f\"logs/flood_{htune_str}{mode_str}_{TIMESTAMP}\"\n\n# Derived: spatial dimensions for model\nSPATIAL_SIZE = PATCH_SIZE if USE_PATCHES else 1000\n\nprint(f\"Mode: {'PATCHES' if USE_PATCHES else 'FULL MAPS'} ({SPATIAL_SIZE}x{SPATIAL_SIZE})\")\nprint(f\"Hypertuning: {'ON' if RUN_HYPERTUNING else 'OFF'}\")\nprint(f\"Log directory: {LOG_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2\n\nimport gc\nimport os\nimport pathlib\nimport logging\n\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\nlogging.getLogger().setLevel(logging.WARNING)\n\nimport tensorflow as tf\nimport keras\nimport keras_tuner\nimport numpy as np\n\nSEED = 42\nkeras.utils.set_random_seed(SEED)\n\nfor gpu in tf.config.list_physical_devices(\"GPU\"):\n    tf.config.experimental.set_memory_growth(gpu, True)\n\nprint(f\"TensorFlow: {tf.__version__}\")\nprint(f\"Keras Tuner: {keras_tuner.__version__}\")\nprint(f\"GPUs: {len(tf.config.list_physical_devices('GPU'))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.dataset import (\n",
    "    load_dataset_windowed_cached,\n",
    "    load_dataset_windowed_patches,\n",
    "    download_dataset,\n",
    ")\n",
    "from usl_models.flood_ml import constants\n",
    "\n",
    "print(f\"Map size: {constants.MAP_HEIGHT}x{constants.MAP_WIDTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simulation names\n",
    "sim_names = []\n",
    "for city, config in CITY_CONFIG.items():\n",
    "    for rain_id in RAINFALL_IDS:\n",
    "        sim_names.append(f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\")\n",
    "\n",
    "print(f\"Configured {len(sim_names)} simulations:\")\n",
    "for s in sim_names:\n",
    "    exists = (pathlib.Path(FILECACHE_DIR) / s).exists()\n",
    "    print(f\"  [{'OK' if exists else 'MISSING'}] {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Download Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_DATA:\n",
    "    print(\"Downloading from GCS...\")\n",
    "    download_dataset(\n",
    "        sim_names=sim_names,\n",
    "        output_path=pathlib.Path(FILECACHE_DIR),\n",
    "        dataset_splits=DATASET_SPLITS,\n",
    "        include_labels=True,\n",
    "    )\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(\"Skipping download (DOWNLOAD_DATA=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data exists\n",
    "missing = [s for s in sim_names if not (pathlib.Path(FILECACHE_DIR) / s).exists()]\n",
    "if missing:\n",
    "    print(f\"ERROR: {len(missing)} simulations not cached\")\n",
    "    for s in missing:\n",
    "        print(f\"  - {s}\")\n",
    "    raise FileNotFoundError(\"Set DOWNLOAD_DATA=True\")\n",
    "print(f\"All {len(sim_names)} simulations found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets based on mode\n",
    "filecache = pathlib.Path(FILECACHE_DIR)\n",
    "\n",
    "if USE_PATCHES:\n",
    "    print(f\"Loading PATCH datasets ({PATCH_SIZE}x{PATCH_SIZE}, stride={PATCH_STRIDE})...\")\n",
    "    print(f\"  Filters: min_flood_fraction={MIN_FLOOD_FRACTION}, min_max_depth={MIN_MAX_DEPTH}m\")\n",
    "    \n",
    "    train_dataset = load_dataset_windowed_patches(\n",
    "        filecache_dir=filecache,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=\"train\",\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=PATCH_STRIDE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "        max_chunks=MAX_CHUNKS,\n",
    "        max_patches_per_chunk=MAX_PATCHES_PER_CHUNK,\n",
    "        min_flood_fraction=MIN_FLOOD_FRACTION,\n",
    "        min_max_depth=MIN_MAX_DEPTH,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    val_dataset = load_dataset_windowed_patches(\n",
    "        filecache_dir=filecache,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=\"val\",\n",
    "        patch_size=PATCH_SIZE,\n",
    "        stride=PATCH_STRIDE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "        max_chunks=MAX_CHUNKS,\n",
    "        max_patches_per_chunk=MAX_PATCHES_PER_CHUNK,\n",
    "        min_flood_fraction=MIN_FLOOD_FRACTION,\n",
    "        min_max_depth=MIN_MAX_DEPTH,\n",
    "        shuffle=False,\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading FULL MAP datasets (1000x1000)...\")\n",
    "    \n",
    "    train_dataset = load_dataset_windowed_cached(\n",
    "        filecache_dir=filecache,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=\"train\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "        max_chunks=MAX_CHUNKS,\n",
    "        shuffle=True,\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = load_dataset_windowed_cached(\n",
    "        filecache_dir=filecache,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=\"val\",\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "        max_chunks=MAX_CHUNKS,\n",
    "        shuffle=False,\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Datasets loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Diagnostic: Count patches and show sample values\nif USE_PATCHES:\n    print(\"Analyzing patch selection...\")\n    \n    # Count total windows (samples) in train dataset\n    train_count = 0\n    sample_labels = []\n    for inputs, labels in train_dataset:\n        train_count += labels.shape[0]\n        if len(sample_labels) < 5:  # Store first 5 batches for analysis\n            sample_labels.append(labels.numpy())\n    \n    val_count = sum(labels.shape[0] for _, labels in val_dataset)\n    \n    print(f\"\\n=== PATCH STATISTICS ===\")\n    print(f\"Training samples (windows): {train_count}\")\n    print(f\"Validation samples (windows): {val_count}\")\n    print(f\"Total: {train_count + val_count}\")\n    \n    # Analyze sample patches\n    if sample_labels:\n        all_samples = np.concatenate(sample_labels, axis=0)\n        print(f\"\\n=== SAMPLE PATCH VALUES (first {len(all_samples)} patches) ===\")\n        for i in range(min(10, len(all_samples))):\n            patch = all_samples[i]\n            max_depth = patch.max()\n            flooded_pct = (patch > 0).mean() * 100\n            print(f\"  Patch {i}: max_depth={max_depth:.3f}m, flooded={flooded_pct:.1f}%\")\n        \n        print(f\"\\n  Overall: max={all_samples.max():.3f}m, \"\n              f\"mean={all_samples[all_samples>0].mean():.4f}m (flooded pixels only)\")\nelse:\n    print(\"Full map mode - no patch filtering\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset shapes\n",
    "for inputs, labels in train_dataset.take(1):\n",
    "    print(\"Sample batch:\")\n",
    "    print(f\"  geospatial:    {inputs['geospatial'].shape}\")\n",
    "    print(f\"  temporal:      {inputs['temporal'].shape}\")\n",
    "    print(f\"  spatiotemporal:{inputs['spatiotemporal'].shape}\")\n",
    "    print(f\"  labels:        {labels.shape}\")\n",
    "    \n",
    "    # Check temporal features are distinct\n",
    "    t = inputs['temporal'].numpy()\n",
    "    if t.max() > 0:\n",
    "        distinct = not np.allclose(t[0, :, 0], t[0, :, 1])\n",
    "        print(f\"  Temporal cols distinct: {distinct}\")\n",
    "    \n",
    "    # Show label stats\n",
    "    lbl = labels.numpy()\n",
    "    print(f\"  Label max: {lbl.max():.3f}m, flooded: {(lbl>0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Hyperparameter Tuning & Model Building"
  },
  {
   "cell_type": "code",
   "source": "# Helper function to get fresh datasets (needed for tuner)\ndef get_datasets_for_tuning(batch_size):\n    \"\"\"Load datasets for hyperparameter tuning.\"\"\"\n    filecache = pathlib.Path(FILECACHE_DIR)\n    \n    if USE_PATCHES:\n        train_ds = load_dataset_windowed_patches(\n            filecache_dir=filecache,\n            sim_names=sim_names,\n            dataset_split=\"train\",\n            patch_size=PATCH_SIZE,\n            stride=PATCH_STRIDE,\n            batch_size=batch_size,\n            n_flood_maps=N_FLOOD_MAPS,\n            m_rainfall=M_RAINFALL,\n            max_chunks=MAX_CHUNKS,\n            max_patches_per_chunk=MAX_PATCHES_PER_CHUNK,\n            min_flood_fraction=MIN_FLOOD_FRACTION,\n            min_max_depth=MIN_MAX_DEPTH,\n            shuffle=True,\n        )\n        val_ds = load_dataset_windowed_patches(\n            filecache_dir=filecache,\n            sim_names=sim_names,\n            dataset_split=\"val\",\n            patch_size=PATCH_SIZE,\n            stride=PATCH_STRIDE,\n            batch_size=batch_size,\n            n_flood_maps=N_FLOOD_MAPS,\n            m_rainfall=M_RAINFALL,\n            max_chunks=MAX_CHUNKS,\n            max_patches_per_chunk=MAX_PATCHES_PER_CHUNK,\n            min_flood_fraction=MIN_FLOOD_FRACTION,\n            min_max_depth=MIN_MAX_DEPTH,\n            shuffle=False,\n        )\n    else:\n        train_ds = load_dataset_windowed_cached(\n            filecache_dir=filecache,\n            sim_names=sim_names,\n            dataset_split=\"train\",\n            batch_size=batch_size,\n            n_flood_maps=N_FLOOD_MAPS,\n            m_rainfall=M_RAINFALL,\n            max_chunks=MAX_CHUNKS,\n            shuffle=True,\n        ).prefetch(tf.data.AUTOTUNE)\n        val_ds = load_dataset_windowed_cached(\n            filecache_dir=filecache,\n            sim_names=sim_names,\n            dataset_split=\"val\",\n            batch_size=batch_size,\n            n_flood_maps=N_FLOOD_MAPS,\n            m_rainfall=M_RAINFALL,\n            max_chunks=MAX_CHUNKS,\n            shuffle=False,\n        ).prefetch(tf.data.AUTOTUNE)\n    \n    return train_ds, val_ds\n\nprint(\"Dataset loader ready for tuning.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run Bayesian Optimization hyperparameter search\nbest_hp = None\n\nif RUN_HYPERTUNING:\n    print(\"=\" * 60)\n    print(\"HYPERPARAMETER TUNING - Bayesian Optimization\")\n    print(\"=\" * 60)\n    \n    os.makedirs(LOG_DIR, exist_ok=True)\n    \n    # Create tuner\n    tuner = keras_tuner.BayesianOptimization(\n        FloodModel.get_hypermodel(\n            lstm_units=HTUNE_LSTM_UNITS,\n            lstm_kernel_size=HTUNE_LSTM_KERNEL_SIZE,\n            lstm_dropout=HTUNE_LSTM_DROPOUT,\n            lstm_recurrent_dropout=HTUNE_LSTM_RECURRENT_DROPOUT,\n            n_flood_maps=[N_FLOOD_MAPS],\n            m_rainfall=[M_RAINFALL],\n        ),\n        objective=\"val_loss\",\n        max_trials=HTUNE_MAX_TRIALS,\n        project_name=f\"{LOG_DIR}/tuner\",\n    )\n    \n    # Show search space\n    print(\"\\nSearch space:\")\n    tuner.search_space_summary()\n    \n    # TensorBoard callback for tuning\n    tb_callback = keras.callbacks.TensorBoard(\n        log_dir=f\"{LOG_DIR}/tuner_tb\",\n        histogram_freq=0,\n        profile_batch=0,\n    )\n    \n    def run_tuner_search(batch_size=BATCH_SIZE):\n        \"\"\"Run tuner with limited samples for speed.\"\"\"\n        gc.collect()\n        tf.keras.backend.clear_session()\n        \n        train_ds, val_ds = get_datasets_for_tuning(batch_size)\n        \n        # Convert sample counts to batch counts\n        num_train_batches = max(1, HTUNE_TRAIN_SAMPLES // batch_size)\n        num_val_batches = max(1, HTUNE_VAL_SAMPLES // batch_size)\n        \n        print(f\"\\nUsing {num_train_batches} train batches ({num_train_batches * batch_size} samples)\")\n        print(f\"Using {num_val_batches} val batches ({num_val_batches * batch_size} samples)\")\n        print(f\"Epochs per trial: {HTUNE_EPOCHS}\")\n        print(f\"Max trials: {HTUNE_MAX_TRIALS}\")\n        \n        tuner.search(\n            train_ds.take(num_train_batches),\n            validation_data=val_ds.take(num_val_batches),\n            epochs=HTUNE_EPOCHS,\n            callbacks=[tb_callback],\n            verbose=1,\n        )\n    \n    # Run the search\n    run_tuner_search()\n    \n    # Get best hyperparameters\n    best_hp = tuner.get_best_hyperparameters()[0]\n    print(\"\\n\" + \"=\" * 60)\n    print(\"BEST HYPERPARAMETERS FOUND:\")\n    print(\"=\" * 60)\n    for k, v in best_hp.values.items():\n        print(f\"  {k}: {v}\")\n    \n    # Update the fixed values for display\n    LSTM_UNITS = best_hp.get(\"lstm_units\")\n    LSTM_KERNEL_SIZE = best_hp.get(\"lstm_kernel_size\")\n    LSTM_DROPOUT = best_hp.get(\"lstm_dropout\")\n    LSTM_RECURRENT_DROPOUT = best_hp.get(\"lstm_recurrent_dropout\")\n    \nelse:\n    print(\"Skipping hyperparameter tuning (RUN_HYPERTUNING=False)\")\n    print(f\"Using fixed params: lstm_units={LSTM_UNITS}, kernel={LSTM_KERNEL_SIZE}, \"\n          f\"dropout={LSTM_DROPOUT}, rec_dropout={LSTM_RECURRENT_DROPOUT}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with correct spatial dimensions\n",
    "params = FloodModel.Params(\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    lstm_kernel_size=LSTM_KERNEL_SIZE,\n",
    "    lstm_dropout=LSTM_DROPOUT,\n",
    "    lstm_recurrent_dropout=LSTM_RECURRENT_DROPOUT,\n",
    "    n_flood_maps=N_FLOOD_MAPS,\n",
    "    m_rainfall=M_RAINFALL,\n",
    ")\n",
    "\n",
    "# IMPORTANT: Set spatial_dims to match data\n",
    "model = FloodModel(\n",
    "    params=params,\n",
    "    spatial_dims=(SPATIAL_SIZE, SPATIAL_SIZE),\n",
    ")\n",
    "\n",
    "print(f\"Model spatial dims: {SPATIAL_SIZE}x{SPATIAL_SIZE}\")\n",
    "print(f\"Parameters:\")\n",
    "for k, v in params.to_dict().items():\n",
    "    if k != \"optimizer\":\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=0),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{LOG_DIR}/best_model.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        verbose=1,\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Training for up to {EPOCHS} epochs...\")\n",
    "print(f\"Logs: {LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train')\n",
    "axes[0].plot(history.history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "if 'mean_absolute_error' in history.history:\n",
    "    axes[1].plot(history.history['mean_absolute_error'], label='Train')\n",
    "    axes[1].plot(history.history['val_mean_absolute_error'], label='Val')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('MAE (m)')\n",
    "    axes[1].set_title('MAE')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{LOG_DIR}/history.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final train loss: {history.history['loss'][-1]:.6f}\")\n",
    "print(f\"Final val loss:   {history.history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Quick Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick prediction test on one batch\n",
    "for inputs, labels in val_dataset.take(1):\n",
    "    # Single-step prediction\n",
    "    pred = model.call(inputs)\n",
    "    pred_np = np.clip(pred.numpy(), 0, None)\n",
    "    label_np = labels.numpy()\n",
    "    \n",
    "    print(\"Single-step prediction test:\")\n",
    "    print(f\"  Pred shape: {pred_np.shape}\")\n",
    "    print(f\"  Label shape: {label_np.shape}\")\n",
    "    print(f\"  Pred range: [{pred_np.min():.3f}, {pred_np.max():.3f}]\")\n",
    "    print(f\"  Label range: [{label_np.min():.3f}, {label_np.max():.3f}]\")\n",
    "    mae = np.abs(pred_np.squeeze() - label_np).mean()\n",
    "    print(f\"  MAE: {mae:.5f} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction vs ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_show = min(4, pred_np.shape[0])\n",
    "fig, axes = plt.subplots(n_show, 2, figsize=(10, 4*n_show))\n",
    "if n_show == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(n_show):\n",
    "    gt = label_np[i]\n",
    "    pd = pred_np[i].squeeze()\n",
    "    vmax = max(gt.max(), pd.max(), 0.1)\n",
    "    \n",
    "    axes[i, 0].imshow(gt, cmap='Blues', vmin=0, vmax=vmax)\n",
    "    axes[i, 0].set_title(f'GT (max={gt.max():.2f}m)')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(pd, cmap='Blues', vmin=0, vmax=vmax)\n",
    "    axes[i, 1].set_title(f'Pred (max={pd.max():.2f}m)')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{LOG_DIR}/prediction_sample.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = f\"{LOG_DIR}/final_model.keras\"\n",
    "model.save_model(final_path)\n",
    "\n",
    "print(f\"Model saved to: {final_path}\")\n",
    "print(f\"Best checkpoint: {LOG_DIR}/best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save config\nimport json\n\nconfig = {\n    \"mode\": \"patches\" if USE_PATCHES else \"full_map\",\n    \"spatial_size\": SPATIAL_SIZE,\n    \"patch_size\": PATCH_SIZE if USE_PATCHES else None,\n    \"patch_stride\": PATCH_STRIDE if USE_PATCHES else None,\n    \"sim_names\": sim_names,\n    \"hypertuning_used\": RUN_HYPERTUNING,\n    \"best_hyperparameters\": best_hp.values if best_hp else None,\n    \"params\": params.to_dict(),\n    \"batch_size\": BATCH_SIZE,\n    \"epochs_trained\": len(history.history['loss']),\n    \"final_train_loss\": float(history.history['loss'][-1]),\n    \"final_val_loss\": float(history.history['val_loss'][-1]),\n}\n\nwith open(f\"{LOG_DIR}/config.json\", \"w\") as f:\n    json.dump(config, f, indent=2)\n\nprint(f\"Config saved to: {LOG_DIR}/config.json\")\nprint(f\"\\nSummary:\")\nprint(f\"  Hypertuning: {'Yes' if RUN_HYPERTUNING else 'No'}\")\nprint(f\"  Best params: lstm_units={LSTM_UNITS}, kernel={LSTM_KERNEL_SIZE}\")\nprint(f\"  Final val loss: {config['final_val_loss']:.6f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}