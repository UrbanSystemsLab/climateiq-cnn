{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Model Training Notebook\n",
    "\n",
    "Train a Flood ConvLSTM Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2 simulations.\n",
      "Manhattan-Manhattan_config/Rainfall_Data_5.txt\n",
      "Atlanta-Atlanta_config/Rainfall_Data_5.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import time\n",
    "import keras\n",
    "import logging\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.model_params import FloodModelParams\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed, load_dataset\n",
    "from usl_models.flood_ml import customloss, dataset\n",
    "\n",
    "# Setup\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Cities and their config folders\n",
    "city_config_mapping = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "    \"Atlanta\": \"Atlanta_config\",\n",
    "    # \"Phoenix_SM\": \"PHX_SM\",\n",
    "    # \"Phoenix_PV\": \"PHX_PV\",\n",
    "}\n",
    "\n",
    "# Rainfall files you want\n",
    "rainfall_files = [5]  # Only 5 and 6\n",
    "\n",
    "# Generate sim_names\n",
    "sim_names = []\n",
    "for city, config in city_config_mapping.items():\n",
    "    for rain_id in rainfall_files:\n",
    "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
    "        sim_names.append(sim_name)\n",
    "\n",
    "print(f\"Training on {len(sim_names)} simulations.\")\n",
    "for s in sim_names:\n",
    "    print(s)\n",
    "\n",
    "# Now load dataset\n",
    "ds_config = dataset.Config(\n",
    "    input_height=10, input_width=10,\n",
    "    output_height=10, output_width=10\n",
    ")\n",
    "\n",
    "train_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"train\", ds_config=ds_config\n",
    ").cache()\n",
    "\n",
    "validation_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 24s]\n",
      "val_loss: 0.006333703175187111\n",
      "\n",
      "Best val_loss So Far: 0.006333703175187111\n",
      "Total elapsed time: 00h 01m 24s\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).pre_attention.conv.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).pre_attention.conv.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).pre_attention.conv.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).pre_attention.conv.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).attention.conv.kernel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).attention.conv.kernel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).attention.conv.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).attention.conv.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lstm_units': 64,\n",
       " 'lstm_kernel_size': 3,\n",
       " 'lstm_dropout': 0.3,\n",
       " 'lstm_recurrent_dropout': 0.3,\n",
       " 'n_flood_maps': 5,\n",
       " 'm_rainfall': 6,\n",
       " 'loss_scale': 200.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = f\"logs/htune_project_{timestamp}\"\n",
    "print(log_dir)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "        loss_scale=[50.0, 100.0, 200.0],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=1,\n",
    "    project_name=log_dir,\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "        loss_scale=[50.0, 100.0, 200.0],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=10,\n",
    "    project_name=f\"logs/htune_project_{timestamp}\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "lstm_units (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32, 64, 128], 'ordered': True}\n",
      "lstm_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
      "lstm_dropout (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3], 'ordered': True}\n",
      "lstm_recurrent_dropout (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3], 'ordered': True}\n",
      "n_flood_maps (Choice)\n",
      "{'default': 5, 'conditions': [], 'values': [5], 'ordered': True}\n",
      "m_rainfall (Choice)\n",
      "{'default': 6, 'conditions': [], 'values': [6], 'ordered': True}\n",
      "loss_scale (Choice)\n",
      "{'default': 50.0, 'conditions': [], 'values': [50.0, 100.0, 200.0], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/htune_project_{timestamp}\"\n",
    "print(log_dir)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def from_checkpoint(artifact_uri: str, **kwargs):\n",
    "    \"\"\"Loads the FloodModel from a checkpoint, allowing new spatial shapes.\"\"\"\n",
    "\n",
    "    # Load the model to get weights only\n",
    "    loaded_model = keras.models.load_model(artifact_uri)\n",
    "\n",
    "    # Rebuild model architecture dynamically\n",
    "    params = FloodModel.Params.from_config(loaded_model.get_config())\n",
    "    print(f\"Loaded model params: {params}\")\n",
    "    model = FloodModel(params=params, **kwargs)\n",
    "\n",
    "    # Now set weights\n",
    "    model._model.set_weights(loaded_model.get_weights())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:25:10.881894: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inflood_conv_lstm_1/conv_lstm/conv_lstm2d_1/while/body/_1/flood_conv_lstm_1/conv_lstm/conv_lstm2d_1/while/dropout_7/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     89/Unknown - 8s 25ms/step - loss: 0.0061 - mean_absolute_error: 0.0292 - root_mean_squared_error: 0.1137"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:25:15.047877: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15534628773044783567\n",
      "2025-06-26 18:25:15.047929: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13295164407447007945\n",
      "2025-06-26 18:25:15.047939: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 12771502634948723015\n",
      "2025-06-26 18:25:15.047947: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8480965922338745377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c1127d0>, 140317996690080), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c1127d0>, 140317996690080), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c371310>, 140317885506240), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c371310>, 140317885506240), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c342150>, 140317994654544), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c342150>, 140317994654544), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c322d10>, 140317994654880), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c322d10>, 140317994654880), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c1127d0>, 140317996690080), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c1127d0>, 140317996690080), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c371310>, 140317885506240), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c371310>, 140317885506240), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c342150>, 140317994654544), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c342150>, 140317994654544), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c322d10>, 140317994654880), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f9e7c322d10>, 140317994654880), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250626-182308/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250626-182308/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 13s 79ms/step - loss: 0.0059 - mean_absolute_error: 0.0286 - root_mean_squared_error: 0.1124 - val_loss: 0.0063 - val_mean_absolute_error: 0.0244 - val_root_mean_squared_error: 0.1340\n",
      "Epoch 2/2\n",
      "91/91 [==============================] - 2s 27ms/step - loss: 0.0059 - mean_absolute_error: 0.0284 - root_mean_squared_error: 0.1124 - val_loss: 0.0063 - val_mean_absolute_error: 0.0244 - val_root_mean_squared_error: 0.1340\n",
      "INFO:tensorflow:Assets written to: logs/htune_project_20250626-182308/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250626-182308/model/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define final parameters and model\n",
    "final_params_dict = best_hp.values.copy()\n",
    "loss_scale = final_params_dict.pop(\"loss_scale\", 100.0)\n",
    "final_params = FloodModel.Params(**final_params_dict)\n",
    "model = FloodModel(params=final_params, loss_scale=loss_scale)\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    ModelCheckpoint(\n",
    "        filepath=log_dir + \"/checkpoint\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_format=\"tf\",\n",
    "    ),\n",
    "    EarlyStopping(  # <--- ADD THIS\n",
    "        monitor=\"val_loss\",  # What to monitor\n",
    "        patience=100,  # Number of epochs with no improvement to wait\n",
    "        restore_best_weights=True,  # Restore model weights from best epoch\n",
    "        mode=\"min\",  # \"min\" because lower val_loss is better\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train\n",
    "model.fit(train_dataset, validation_dataset, epochs=2, callbacks=callbacks)\n",
    "\n",
    "model.save_model(log_dir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm_units': 64,\n",
       " 'lstm_kernel_size': 3,\n",
       " 'lstm_dropout': 0.3,\n",
       " 'lstm_recurrent_dropout': 0.3,\n",
       " 'n_flood_maps': 5,\n",
       " 'm_rainfall': 6,\n",
       " 'loss_scale': 200.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model params: FloodModel.Params(lstm_units=64, lstm_kernel_size=3, lstm_dropout=0.3, lstm_recurrent_dropout=0.3, m_rainfall=6, n_flood_maps=5, num_features=22, pad_mode='REFLECT', optimizer=<keras.src.optimizers.adam.Adam object at 0x7f9e4f73f490>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"flood_conv_lstm_3\" with a weight list of length 19, but the layer was expecting 15 weights. Provided weights: [array([[[[-0.0218656 ,  0.10016118,  0.12838762, ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m ds_config \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mConfig(\n\u001b[1;32m      2\u001b[0m     input_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, input_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m load_dataset_windowed(\n\u001b[1;32m      5\u001b[0m     sim_names\u001b[38;5;241m=\u001b[39msim_names, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, dataset_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, ds_config\u001b[38;5;241m=\u001b[39mds_config\n\u001b[1;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mcache()\n\u001b[0;32m----> 7\u001b[0m modello \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m input_batch, label_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_ds))\n\u001b[1;32m      9\u001b[0m pred_batch \u001b[38;5;241m=\u001b[39m modello\u001b[38;5;241m.\u001b[39mcall(input_batch)\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mfrom_checkpoint\u001b[0;34m(artifact_uri, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m FloodModel(params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Now set weights\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/base_layer.py:1797\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         expected_num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_num_weights \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m-> 1797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou called `set_weights(weights)` on layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1799\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a weight list of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but the layer was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1800\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m weights. Provided weights: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m   1802\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1803\u001b[0m             \u001b[38;5;28mlen\u001b[39m(weights),\n\u001b[1;32m   1804\u001b[0m             expected_num_weights,\n\u001b[1;32m   1805\u001b[0m             \u001b[38;5;28mstr\u001b[39m(weights)[:\u001b[38;5;241m50\u001b[39m],\n\u001b[1;32m   1806\u001b[0m         )\n\u001b[1;32m   1807\u001b[0m     )\n\u001b[1;32m   1809\u001b[0m weight_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1810\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"flood_conv_lstm_3\" with a weight list of length 19, but the layer was expecting 15 weights. Provided weights: [array([[[[-0.0218656 ,  0.10016118,  0.12838762, ..."
     ]
    }
   ],
   "source": [
    "ds_config = dataset.Config(\n",
    "    input_width=5, input_height=5, output_width=5, output_height=5\n",
    ")\n",
    "val_ds = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()\n",
    "modello = from_checkpoint(log_dir + \"/model\")\n",
    "input_batch, label_batch = next(iter(val_ds))\n",
    "pred_batch = modello.call(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = \"logs/htune_project_20250625-184752/model_dynamic\"\n",
    "loaded_model = tf.saved_model.load(saved_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = loaded_model.signatures[\"serving_default\"]\n",
    "inputs = {\n",
    "    \"geospatial\": tf.random.uniform((1, 64, 64, constants.GEO_FEATURES)),  # shape can vary\n",
    "    \"spatiotemporal\": tf.random.uniform((1, final_params.n_flood_maps, 64, 64, 1)),\n",
    "    \"temporal\": tf.random.uniform((1, constants.MAX_RAINFALL_DURATION, final_params.m_rainfall)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds_config \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mConfig(\n\u001b[1;32m      2\u001b[0m     input_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, input_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, output_height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m val_ds \u001b[38;5;241m=\u001b[39m load_dataset_windowed(\n\u001b[1;32m      5\u001b[0m     sim_names\u001b[38;5;241m=\u001b[39msim_names, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dataset_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, ds_config\u001b[38;5;241m=\u001b[39mds_config\n\u001b[1;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m      8\u001b[0m input_batch, label_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_ds))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "ds_config = dataset.Config(\n",
    "    input_width=5, input_height=5, output_width=5, output_height=5\n",
    ")\n",
    "val_ds = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=2, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()\n",
    "\n",
    "input_batch, label_batch = next(iter(val_ds))\n",
    "pred_batch = model.call(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 150.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "# Assuming validation_dataset is already defined\n",
    "# Example:\n",
    "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "n_samples = 20\n",
    "timestep = 2\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
    "\n",
    "    # Mask out NaNs\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_flat = pred_t[mask].flatten()\n",
    "\n",
    "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
    "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
    "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum())\n",
    "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
    "\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Bias\": bias,\n",
    "        \"IoU > 0.1\": iou,\n",
    "        \"SSIM\": ssim_val\n",
    "    })\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_model=model._model\n",
    "@tf.function(\n",
    "    input_signature=[{\n",
    "        \"geospatial\": tf.TensorSpec(shape=(None, None, None, constants.GEO_FEATURES), dtype=tf.float32),\n",
    "        \"spatiotemporal\": tf.TensorSpec(shape=(None, final_params.n_flood_maps, None, None, 1), dtype=tf.float32),\n",
    "        \"temporal\": tf.TensorSpec(shape=(None, constants.MAX_RAINFALL_DURATION, final_params.m_rainfall), dtype=tf.float32)\n",
    "    }]\n",
    ")\n",
    "def dynamic_call(inputs):\n",
    "    return flood_model(inputs, training=False)\n",
    "tf.saved_model.save(\n",
    "    flood_model,\n",
    "    export_dir=log_dir + \"/model_dynamic\",\n",
    "    signatures={\"serving_default\": dynamic_call}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modello = tf.saved_model.load(log_dir + \"/model_dynamic\")\n",
    "predict_fn = modello.signatures[\"serving_default\"]\n",
    "input_batch, label_batch = next(iter(val_ds))\n",
    "output = predict_fn(input_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test calling the model on some data.\n",
    "inputs, labels_ = next(iter(train_dataset))\n",
    "prediction = model.call(inputs)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test calling the model for n predictions\n",
    "# full_dataset = load_dataset(sim_names=sim_names, batch_size=1)\n",
    "# inputs, labels = next(iter(full_dataset))\n",
    "# predictions = model.call_n(inputs, n=4)\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FloodModel.from_checkpoint(\"/home/elhajjas/climateiq-cnn-11/logs/htune_project_20250623-225804/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/home/elhajjas/climateiq-cnn-11/logs/htune_project_20250623-223109/model\"\n",
    "\n",
    "# # Create the loss function with the correct scale\n",
    "# loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# # Load model with custom loss function\n",
    "# model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "# model = AtmoModel.from_checkpoint(\"logs/main_20250319-204950/model\")\n",
    "\n",
    "\n",
    "# After refactor you already have this\n",
    "model = FloodModel.from_checkpoint(\"/home/elhajjas/climateiq-cnn-11/logs/htune_project_20250625-184752/model\")\n",
    "\n",
    "\n",
    "ds_config = dataset.Config(\n",
    "    input_width=5, input_height=5, output_width=5, output_height=5\n",
    ")\n",
    "val_ds = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()\n",
    "\n",
    "input_batch, label_batch = next(iter(val_ds))\n",
    "pred_batch = model.call(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 200.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"logs/htune_project_20250625-184752/model_dynamic\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "\n",
    "# Number of samples to visualize\n",
    "n_samples = 20\n",
    "\n",
    "# Loop through the dataset and predict\n",
    "for i, (input_data, ground_truth) in enumerate(val_ds.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    print(f\"\\nSample {i+1} Prediction Stats:\")\n",
    "    print(\"  Min:\", prediction.min())\n",
    "    print(\"  Max:\", prediction.max())\n",
    "    print(\"  Mean:\", prediction.mean())\n",
    "\n",
    "    # Choose timestep to plot\n",
    "    timestep = 3\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = max(gt_t.max(), pred_t.max())\n",
    "\n",
    "    # Plot Ground Truth and Prediction\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 150.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "# Assuming validation_dataset is already defined\n",
    "# Example:\n",
    "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "n_samples = 20\n",
    "timestep = 2\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
    "\n",
    "    # Mask out NaNs\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_flat = pred_t[mask].flatten()\n",
    "\n",
    "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
    "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
    "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum())\n",
    "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
    "\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Bias\": bias,\n",
    "        \"IoU > 0.1\": iou,\n",
    "        \"SSIM\": ssim_val\n",
    "    })\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml import customloss\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "loss_scale = 200.0\n",
    "timestep = 3\n",
    "n_samples = 20\n",
    "\n",
    "# Paths to models\n",
    "model_path_1 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/attention/model\"\n",
    "model_path_2 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250612-010926/model\"\n",
    "\n",
    "# Loss function\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load models\n",
    "model_1 = tf.keras.models.load_model(model_path_1, custom_objects={\"loss_fn\": loss_fn})\n",
    "model_2 = tf.keras.models.load_model(model_path_2, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "# Load validation dataset (ensure it's already prepared)\n",
    "# Example:\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(train_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "\n",
    "    pred_1 = model_1(input_data).numpy().squeeze()\n",
    "    pred_2 = model_2(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_1_t = pred_1[timestep]\n",
    "    pred_2_t = pred_2[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_1_t, pred_2_t], 99.5)\n",
    "\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_1_flat = pred_1_t[mask].flatten()\n",
    "    pred_2_flat = pred_2_t[mask].flatten()\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE_1\": mean_absolute_error(gt_flat, pred_1_flat),\n",
    "        \"RMSE_1\": np.sqrt(mean_squared_error(gt_flat, pred_1_flat)),\n",
    "        \"Bias_1\": np.mean(pred_1_flat) - np.mean(gt_flat),\n",
    "        \"IoU_1\": np.logical_and(gt_flat > 0.1, pred_1_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_1_flat > 0.1).sum()),\n",
    "        \"SSIM_1\": ssim(gt_t, pred_1_t, data_range=gt_t.max() - gt_t.min()),\n",
    "\n",
    "        \"MAE_2\": mean_absolute_error(gt_flat, pred_2_flat),\n",
    "        \"RMSE_2\": np.sqrt(mean_squared_error(gt_flat, pred_2_flat)),\n",
    "        \"Bias_2\": np.mean(pred_2_flat) - np.mean(gt_flat),\n",
    "        \"IoU_2\": np.logical_and(gt_flat > 0.1, pred_2_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_2_flat > 0.1).sum()),\n",
    "        \"SSIM_2\": ssim(gt_t, pred_2_t, data_range=gt_t.max() - gt_t.min()),\n",
    "    })\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pred_1_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"attention\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(pred_2_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[2].set_title(\"without attention\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary metrics\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
