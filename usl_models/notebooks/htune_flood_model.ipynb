{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Flood Model Training Notebook\n",
                "\n",
                "Train a Flood ConvLSTM Model using `usl_models` lib."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-15 21:51:47.539372: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
                        "2025-08-15 21:51:47.590155: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
                        "2025-08-15 21:51:47.590186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
                        "2025-08-15 21:51:47.591431: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
                        "2025-08-15 21:51:47.599068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
                        "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training on 1 simulations.\n",
                        "Manhattan-Manhattan_config/Rainfall_Data_5.txt\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-15 21:51:52.622128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38364 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "import keras_tuner\n",
                "import time\n",
                "import keras\n",
                "import logging\n",
                "from usl_models.flood_ml import constants\n",
                "from usl_models.flood_ml.model import FloodModel\n",
                "from usl_models.flood_ml.model_params import FloodModelParams\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed, load_dataset\n",
                "from usl_models.flood_ml import customloss\n",
                "\n",
                "# Setup\n",
                "logging.getLogger().setLevel(logging.WARNING)\n",
                "keras.utils.set_random_seed(812)\n",
                "\n",
                "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
                "    tf.config.experimental.set_memory_growth(gpu, True)\n",
                "\n",
                "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
                "\n",
                "# Cities and their config folders\n",
                "city_config_mapping = {\n",
                "    \"Manhattan\": \"Manhattan_config\",\n",
                "    # \"Atlanta\": \"Atlanta_config\",\n",
                "    # \"Atlanta\": \"Atlanta_config\",\n",
                "    # \"Phoenix_SM\": \"PHX_SM\",\n",
                "    # \"Phoenix_PV\": \"PHX_PV\",\n",
                "}\n",
                "\n",
                "# Rainfall files you want\n",
                "rainfall_files = [5]  # Only 5 and 6\n",
                "\n",
                "# Generate sim_names\n",
                "sim_names = []\n",
                "for city, config in city_config_mapping.items():\n",
                "    for rain_id in rainfall_files:\n",
                "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
                "        sim_names.append(sim_name)\n",
                "\n",
                "print(f\"Training on {len(sim_names)} simulations.\")\n",
                "for s in sim_names:\n",
                "    print(s)\n",
                "\n",
                "# Now load dataset\n",
                "train_dataset = load_dataset_windowed(\n",
                "    sim_names=sim_names, batch_size=4, dataset_split=\"train\"\n",
                ").cache()\n",
                "\n",
                "validation_dataset = load_dataset_windowed(\n",
                "    sim_names=sim_names, batch_size=4, dataset_split=\"val\"\n",
                ").cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,    # ← use ALL chunks (no train/val/test filter)\n",
                "    batch_size=4 \n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Chunks in TRAIN only: 13\n",
                        "Chunks in ALL splits: 26\n",
                        "Windows in TRAIN only: 169\n",
                        "Windows in ALL splits: 338\n",
                        "Example CHUNK shapes:\n",
                        "  spatiotemporal: (5, 1000, 1000, 1)\n",
                        "  geospatial:     (1000, 1000, 9)\n",
                        "  temporal:       (864, 6)\n",
                        "  labels:         (13, 1000, 1000)\n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.dataset import load_dataset, load_dataset_windowed\n",
                "\n",
                "# --- your existing sim_names here ---\n",
                "# sim_names = [...]\n",
                "\n",
                "def count_dataset(ds):\n",
                "    return sum(1 for _ in ds)\n",
                "\n",
                "# 1) CHUNKS (non-windowed) --------------------------\n",
                "train_chunks_ds = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=\"train\",\n",
                "    batch_size=None,          # <- no batching: 1 element == 1 chunk\n",
                ")\n",
                "all_chunks_ds = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,       # <- ALL splits combined\n",
                "    batch_size=None,\n",
                ")\n",
                "\n",
                "train_chunks = count_dataset(train_chunks_ds)\n",
                "all_chunks = count_dataset(all_chunks_ds)\n",
                "\n",
                "print(f\"Chunks in TRAIN only: {train_chunks}\")\n",
                "print(f\"Chunks in ALL splits: {all_chunks}\")\n",
                "\n",
                "# 2) WINDOWS (windowed for teacher-forcing) --------\n",
                "train_windows_ds = load_dataset_windowed(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=\"train\",\n",
                "    batch_size=None,          # <- no batching: 1 element == 1 window\n",
                ")\n",
                "all_windows_ds = load_dataset_windowed(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,       # <- ALL splits combined (works with your updated code)\n",
                "    batch_size=None,\n",
                ")\n",
                "\n",
                "train_windows = count_dataset(train_windows_ds)\n",
                "all_windows = count_dataset(all_windows_ds)\n",
                "\n",
                "print(f\"Windows in TRAIN only: {train_windows}\")\n",
                "print(f\"Windows in ALL splits: {all_windows}\")\n",
                "\n",
                "# 3) (Optional) peek at one example shape ----------\n",
                "ex_inputs, ex_labels = next(iter(all_chunks_ds.take(1)))\n",
                "print(\"Example CHUNK shapes:\")\n",
                "print(\"  spatiotemporal:\", ex_inputs[\"spatiotemporal\"].shape)  # [N, H, W, 1]\n",
                "print(\"  geospatial:    \", ex_inputs[\"geospatial\"].shape)       # [H, W, F]\n",
                "print(\"  temporal:      \", ex_inputs[\"temporal\"].shape)         # [T_MAX, M]\n",
                "print(\"  labels:        \", ex_labels.shape)                     # [T_label, H, W]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "First 20 chunk indices from full dataset:\n",
                        "Sample 0: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 1: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 2: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 3: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 4: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 5: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 6: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 7: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 8: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 9: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 10: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 11: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 12: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 13: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 14: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 15: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 16: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 17: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 18: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n",
                        "Sample 19: feature shape = (1000, 1000, 9), label shape = (13, 1000, 1000)\n"
                    ]
                }
            ],
            "source": [
                "# This will iterate the dataset WITHOUT batching so you see the raw order\n",
                "debug_dataset = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,   # full (train+val+test)\n",
                "    batch_size=None       # no batching so we see per-chunk order\n",
                ")\n",
                "\n",
                "print(\"First 20 chunk indices from full dataset:\")\n",
                "for i, (features, labels) in enumerate(debug_dataset.take(20)):\n",
                "    # Pull the chunk's position from Firestore metadata order\n",
                "    # Features are shape [H, W, ...], but we don't know the indices unless we print in _iter_geo_feature_label_tensors\n",
                "    print(f\"Sample {i}: feature shape = {features['geospatial'].shape}, label shape = {labels.shape}\")\n",
                "\n",
                "# If you want exact (x_index, y_index) printed:\n",
                "# Add a debug print in _iter_geo_feature_label_tensors before yield:\n",
                "# print(\"Yielding chunk\", index)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Chunk 0: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/1_7.npy\n",
                        "Chunk 1: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/1_8.npy\n",
                        "Chunk 2: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/1_9.npy\n",
                        "Chunk 3: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/1_10.npy\n",
                        "Chunk 4: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/2_5.npy\n",
                        "Chunk 5: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/2_6.npy\n",
                        "Chunk 6: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/2_7.npy\n",
                        "Chunk 7: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/2_8.npy\n",
                        "Chunk 8: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/2_9.npy\n",
                        "Chunk 9: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/3_3.npy\n",
                        "Chunk 10: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/3_4.npy\n",
                        "Chunk 11: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/3_5.npy\n",
                        "Chunk 12: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/3_6.npy\n",
                        "Chunk 13: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/3_7.npy\n",
                        "Chunk 14: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/3_8.npy\n",
                        "Chunk 15: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_0.npy\n",
                        "Chunk 16: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_1.npy\n",
                        "Chunk 17: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_2.npy\n",
                        "Chunk 18: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_3.npy\n",
                        "Chunk 19: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_4.npy\n",
                        "Chunk 20: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_5.npy\n",
                        "Chunk 21: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/4_6.npy\n",
                        "Chunk 22: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/5_0.npy\n",
                        "Chunk 23: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/5_1.npy\n",
                        "Chunk 24: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/5_4.npy\n",
                        "Chunk 25: gs://test-climateiq-study-area-label-chunks/Manhattan/Manhattan_config/Rainfall_Data_5.txt/5_5.npy\n"
                    ]
                }
            ],
            "source": [
                "full_dataset = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,  # Load all splits combined\n",
                "    batch_size=None      # No batching, one chunk at a time\n",
                ")\n",
                "\n",
                "from usl_models.flood_ml import metastore\n",
                "from google.cloud import firestore, storage\n",
                "\n",
                "firestore_client = firestore.Client()\n",
                "storage_client = storage.Client()\n",
                "\n",
                "for sim_name in sim_names:\n",
                "    # Get ALL label chunks without split filter\n",
                "    label_chunks_collection = metastore._get_simulation_doc(\n",
                "        firestore_client, sim_name\n",
                "    ).collection(\"label_chunks\")\n",
                "    label_metadata = [doc.to_dict() for doc in label_chunks_collection.stream()]\n",
                "\n",
                "    # Sort by (x_index, y_index) to match dataset order\n",
                "    ordered_labels = sorted(\n",
                "        label_metadata, key=lambda l: (l[\"x_index\"], l[\"y_index\"])\n",
                "    )\n",
                "\n",
                "    # Iterate over ALL chunks (no slicing)\n",
                "    for i, label in enumerate(ordered_labels):\n",
                "        print(f\"Chunk {i}: {label['gcs_uri']}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train chunks: 13\n",
                        "Val chunks:   6\n",
                        "Test chunks:  7\n",
                        "Full chunks:  26\n",
                        "Sum of splits: 26\n",
                        "MATCH: True\n"
                    ]
                }
            ],
            "source": [
                "def count_dataset(ds):\n",
                "    return sum(1 for _ in ds)\n",
                "\n",
                "# No batching so we count raw elements\n",
                "train_ds = load_dataset(sim_names=sim_names, dataset_split=\"train\", batch_size=None)\n",
                "val_ds   = load_dataset(sim_names=sim_names, dataset_split=\"val\", batch_size=None)\n",
                "test_ds  = load_dataset(sim_names=sim_names, dataset_split=\"test\", batch_size=None)\n",
                "full_ds  = load_dataset(sim_names=sim_names, dataset_split=None,    batch_size=None)\n",
                "\n",
                "train_count = count_dataset(train_ds)\n",
                "val_count   = count_dataset(val_ds)\n",
                "test_count  = count_dataset(test_ds)\n",
                "full_count  = count_dataset(full_ds)\n",
                "\n",
                "print(f\"Train chunks: {train_count}\")\n",
                "print(f\"Val chunks:   {val_count}\")\n",
                "print(f\"Test chunks:  {test_count}\")\n",
                "print(f\"Full chunks:  {full_count}\")\n",
                "print(f\"Sum of splits: {train_count + val_count + test_count}\")\n",
                "print(\"MATCH:\", full_count == (train_count + val_count + test_count))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train batches: 43\n",
                        "Full batches: 26\n"
                    ]
                }
            ],
            "source": [
                "train_count = sum(1 for _ in train_dataset)\n",
                "full_count = sum(1 for _ in full_dataset)\n",
                "\n",
                "print(f\"Train batches: {train_count}\")\n",
                "print(f\"Full batches: {full_count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Full chunks (all splits combined): 26000\n"
                    ]
                }
            ],
            "source": [
                "full_chunks = 0\n",
                "for inputs, labels in full_dataset:\n",
                "    full_chunks += int(inputs[\"geospatial\"].shape[0])  # batch size for this batch\n",
                "print(\"Full chunks (all splits combined):\", full_chunks)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train windows: 169\n",
                        "≈ Train chunks: 1\n"
                    ]
                }
            ],
            "source": [
                "T_max = 169  # adjust\n",
                "train_windows = 0\n",
                "for inputs, label in train_dataset:  # windowed: label shape [B, H, W]\n",
                "    train_windows += int(label.shape[0])\n",
                "approx_train_chunks = train_windows // T_max\n",
                "print(\"Train windows:\", train_windows)\n",
                "print(\"≈ Train chunks:\", approx_train_chunks)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Search space summary\n",
                        "Default search space size: 6\n",
                        "lstm_units (Choice)\n",
                        "{'default': 32, 'conditions': [], 'values': [32, 64, 128], 'ordered': True}\n",
                        "lstm_kernel_size (Choice)\n",
                        "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
                        "lstm_dropout (Choice)\n",
                        "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3], 'ordered': True}\n",
                        "lstm_recurrent_dropout (Choice)\n",
                        "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3], 'ordered': True}\n",
                        "n_flood_maps (Choice)\n",
                        "{'default': 5, 'conditions': [], 'values': [5], 'ordered': True}\n",
                        "m_rainfall (Choice)\n",
                        "{'default': 6, 'conditions': [], 'values': [6], 'ordered': True}\n"
                    ]
                }
            ],
            "source": [
                "tuner = keras_tuner.BayesianOptimization(\n",
                "    FloodModel.get_hypermodel(\n",
                "        lstm_units=[32, 64, 128],\n",
                "        lstm_kernel_size=[3, 5],\n",
                "        lstm_dropout=[0.2, 0.3],\n",
                "        lstm_recurrent_dropout=[0.2, 0.3],\n",
                "        n_flood_maps=[5],\n",
                "        m_rainfall=[6],\n",
                "    ),\n",
                "    objective=\"val_loss\",\n",
                "    max_trials=1,\n",
                "    project_name=f\"logs/htune_project_{timestamp}\",\n",
                ")\n",
                "\n",
                "tuner.search_space_summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Trial 1 Complete [00h 00m 49s]\n",
                        "val_loss: 0.0028904567006975412\n",
                        "\n",
                        "Best val_loss So Far: 0.0028904567006975412\n",
                        "Total elapsed time: 00h 00m 49s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "{'lstm_units': 32,\n",
                            " 'lstm_kernel_size': 3,\n",
                            " 'lstm_dropout': 0.2,\n",
                            " 'lstm_recurrent_dropout': 0.2,\n",
                            " 'n_flood_maps': 5,\n",
                            " 'm_rainfall': 6}"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "log_dir = f\"logs/htune_project_{timestamp}\"\n",
                "print(log_dir)\n",
                "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
                "tuner.search(\n",
                "    train_dataset,\n",
                "    epochs=2,\n",
                "    validation_data=validation_dataset,\n",
                "    callbacks=[tb_callback],\n",
                ")\n",
                "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
                "best_hp.values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/2\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-15 22:00:13.310409: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inflood_conv_lstm_1/conv_lstm/conv_lstm2d_1/while/body/_1/flood_conv_lstm_1/conv_lstm/conv_lstm2d_1/while/dropout_7/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "     43/Unknown - 13s 182ms/step - loss: 0.0040 - mean_absolute_error: 0.0254 - root_mean_squared_error: 0.1021"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-15 22:00:22.939146: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 216111812316947242\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: logs/htune_project_20250815-215150/checkpoint/assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: logs/htune_project_20250815-215150/checkpoint/assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "43/43 [==============================] - 20s 342ms/step - loss: 0.0040 - mean_absolute_error: 0.0254 - root_mean_squared_error: 0.1021 - val_loss: 0.0035 - val_mean_absolute_error: 0.0185 - val_root_mean_squared_error: 0.0959\n",
                        "Epoch 2/2\n",
                        "43/43 [==============================] - ETA: 0s - loss: 0.0033 - mean_absolute_error: 0.0230 - root_mean_squared_error: 0.0923INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: logs/htune_project_20250815-215150/checkpoint/assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: logs/htune_project_20250815-215150/checkpoint/assets\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "43/43 [==============================] - 13s 315ms/step - loss: 0.0033 - mean_absolute_error: 0.0230 - root_mean_squared_error: 0.0923 - val_loss: 0.0029 - val_mean_absolute_error: 0.0192 - val_root_mean_squared_error: 0.0876\n",
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f045fc210>, 140115259971760), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f046070d0>, 140115259972096), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0464f210>, 140115258687056), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6f0465a2d0>, 140115258785840), {}).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: logs/htune_project_20250815-215150/model/assets\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:tensorflow:Assets written to: logs/htune_project_20250815-215150/model/assets\n"
                    ]
                }
            ],
            "source": [
                "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "\n",
                "# Define final parameters and model\n",
                "final_params_dict = best_hp.values.copy()\n",
                "final_params = FloodModel.Params(**final_params_dict)\n",
                "model = FloodModel(params=final_params)\n",
                "# Define callbacks\n",
                "callbacks = [\n",
                "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
                "    ModelCheckpoint(\n",
                "        filepath=log_dir + \"/checkpoint\",\n",
                "        save_best_only=True,\n",
                "        monitor=\"val_loss\",\n",
                "        mode=\"min\",\n",
                "        save_format=\"tf\",\n",
                "    ),\n",
                "    EarlyStopping(  # <--- ADD THIS\n",
                "        monitor=\"val_loss\",  # What to monitor\n",
                "        patience=100,  # Number of epochs with no improvement to wait\n",
                "        restore_best_weights=True,  # Restore model weights from best epoch\n",
                "        mode=\"min\",  # \"min\" because lower val_loss is better\n",
                "    ),\n",
                "]\n",
                "\n",
                "# Train\n",
                "model.fit(train_dataset, validation_dataset, epochs=2, callbacks=callbacks)\n",
                "\n",
                "# Save final model\n",
                "model.save_model(log_dir + \"/model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Test calling the model on some data.\n",
                "inputs, labels_ = next(iter(train_dataset))\n",
                "prediction = model.call(inputs)\n",
                "prediction.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "TensorShape([4, 10, 1000, 1000])"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
                "# Path to your saved model\n",
                "model_path = \"/home/se2890/climateiq-cnn-6/logs/htune_project_20250815-215150/model\"\n",
                "#loaded_model = tf.keras.models.load_model(model_path)\n",
                "#loaded_model.summary()\n",
                "# Load the model\n",
                "model = tf.keras.models.load_model(model_path)\n",
                "\n",
                "from usl_models.flood_ml.model import SpatialAttention\n",
                "custom_objects = {'SpatialAttention': SpatialAttention}\n",
                "loaded_model = tf.keras.models.load_model(\n",
                "    model_path,\n",
                "    custom_objects=custom_objects,\n",
                "    compile=False\n",
                ")\n",
                "model.set_weights(loaded_model.get_weights())\n",
                "\n",
                "# # # Test calling the model for n predictions\n",
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
                "inputs, labels = next(iter(full_dataset))\n",
                "predictions = model.call_n(inputs, n=10)\n",
                "predictions.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ref_shapes = None\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_shapes = (\n",
                "        inputs[\"spatiotemporal\"].shape,\n",
                "        inputs[\"geospatial\"].shape,\n",
                "        inputs[\"temporal\"].shape,\n",
                "        labels.shape,\n",
                "    )\n",
                "\n",
                "    if ref_shapes is None:\n",
                "        ref_shapes = current_shapes\n",
                "    else:\n",
                "        assert current_shapes == ref_shapes, f\"Mismatch at batch {i}: {current_shapes} ≠ {ref_shapes}\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split=\"train\")\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    print(f\"\\n--- Batch {i} ---\")\n",
                "    \n",
                "    st = inputs[\"spatiotemporal\"]\n",
                "    geo = inputs[\"geospatial\"]\n",
                "    temp = inputs[\"temporal\"]\n",
                "\n",
                "    print(f\"spatiotemporal shape: {st.shape}\")\n",
                "    print(f\"geospatial shape:     {geo.shape}\")\n",
                "    print(f\"temporal shape:       {temp.shape}\")\n",
                "    print(f\"labels shape:         {labels.shape}\")\n",
                "\n",
                "    try:\n",
                "        preds = model.call_n(inputs, n=10)\n",
                "        print(f\"predictions shape:    {preds.shape}\")\n",
                "        all_preds.append(preds)\n",
                "        all_labels.append(labels)\n",
                "    except Exception as e:\n",
                "        print(f\"Error at batch {i}: {e}\")\n",
                "        break\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 4\n",
                "N_STEPS = 10\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_bs = inputs[\"spatiotemporal\"].shape[0]\n",
                "\n",
                "    if current_bs < BATCH_SIZE:\n",
                "        print(f\"[Batch {i}] Incomplete batch of size {current_bs}, padding to {BATCH_SIZE}\")\n",
                "\n",
                "        # Repeat the last sample to pad\n",
                "        repeats = BATCH_SIZE - current_bs\n",
                "\n",
                "        def pad_tensor(t):\n",
                "            return tf.concat([t, tf.repeat(t[-1:], repeats=repeats, axis=0)], axis=0)\n",
                "\n",
                "        padded_inputs = {\n",
                "            k: pad_tensor(v) for k, v in inputs.items()\n",
                "        }\n",
                "\n",
                "        # Predict on padded batch\n",
                "        preds_padded = model.call_n(padded_inputs, n=N_STEPS)  # [B, T, H, W]\n",
                "\n",
                "        # Remove the extra samples\n",
                "        preds = preds_padded[:current_bs]\n",
                "    else:\n",
                "        preds = model.call_n(inputs, n=N_STEPS)\n",
                "\n",
                "    print(f\"[Batch {i}] Prediction shape: {preds.shape}\")\n",
                "    all_preds.append(preds)\n",
                "    all_labels.append(labels)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# After running all batches\n",
                "final_preds = tf.concat(all_preds, axis=0)  # shape: [N, T, H, W]\n",
                "max_preds_all = tf.reduce_max(final_preds, axis=1)  # shape: [N, H, W]\n",
                "max_preds_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all = []\n",
                "\n",
                "for labels in all_labels:\n",
                "    max_labels = tf.reduce_max(labels, axis=1)  # shape: [B, H, W]\n",
                "    max_labels_all.append(max_labels)\n",
                "\n",
                "# Now stack (uses less memory)\n",
                "max_labels_all = tf.concat(max_labels_all, axis=0)  # shape: [N, H, W]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "i = 10  # sample index\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.imshow(max_preds_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Predicted Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.imshow(max_labels_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Ground Truth Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Initialize GCS client\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over batches\n",
                "for i, preds in enumerate(all_preds):  # each preds: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]  # shape: [H, W]\n",
                "\n",
                "        # Save to temporary .npy file\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".npy\") as tmp:\n",
                "            np.save(tmp.name, sample)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictions/max_flood_batch{i}_sample{j}.npy\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install rasterio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import rasterio\n",
                "from rasterio.transform import from_origin\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Define metadata — adjust as needed\n",
                "pixel_size = 1  # in meters or units per pixel\n",
                "top_left_x = 0  # e.g., UTM x or longitude\n",
                "top_left_y = 0  # e.g., UTM y or latitude\n",
                "\n",
                "transform = from_origin(top_left_x, top_left_y, pixel_size, pixel_size)\n",
                "crs = \"EPSG:4326\"  # or your local UTM projection\n",
                "\n",
                "# Setup GCS\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over prediction batches\n",
                "for i, preds in enumerate(all_preds):  # shape: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]\n",
                "\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".tif\") as tmp:\n",
                "            with rasterio.open(\n",
                "                tmp.name,\n",
                "                \"w\",\n",
                "                driver=\"GTiff\",\n",
                "                height=sample.shape[0],\n",
                "                width=sample.shape[1],\n",
                "                count=1,\n",
                "                dtype=sample.dtype,\n",
                "                crs=crs,\n",
                "                transform=transform,\n",
                "            ) as dst:\n",
                "                dst.write(sample, 1)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictionstiff/max_flood_batch{i}_sample{j}.tif\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded GeoTIFF: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
                "# Path to your saved model\n",
                "model_path = \"/home/se2890/climateiq-cnn-6/logs/htune_project_20250815-144148/model\"\n",
                "#loaded_model = tf.keras.models.load_model(model_path)\n",
                "#loaded_model.summary()\n",
                "# Load the model\n",
                "model = tf.keras.models.load_model(model_path)\n",
                "\n",
                "from usl_models.flood_ml.model import SpatialAttention\n",
                "custom_objects = {'SpatialAttention': SpatialAttention}\n",
                "loaded_model = tf.keras.models.load_model(\n",
                "    model_path,\n",
                "    custom_objects=custom_objects,\n",
                "    compile=False\n",
                ")\n",
                "model.set_weights(loaded_model.get_weights())\n",
                "\n",
                "# # # Test calling the model for n predictions\n",
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
                "inputs, labels = next(iter(full_dataset))\n",
                "predictions = model.call_n(inputs, n=10)\n",
                "predictions.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ref_shapes = None\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_shapes = (\n",
                "        inputs[\"spatiotemporal\"].shape,\n",
                "        inputs[\"geospatial\"].shape,\n",
                "        inputs[\"temporal\"].shape,\n",
                "        labels.shape,\n",
                "    )\n",
                "\n",
                "    if ref_shapes is None:\n",
                "        ref_shapes = current_shapes\n",
                "    else:\n",
                "        assert current_shapes == ref_shapes, f\"Mismatch at batch {i}: {current_shapes} ≠ {ref_shapes}\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split=\"train\")\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    print(f\"\\n--- Batch {i} ---\")\n",
                "    \n",
                "    st = inputs[\"spatiotemporal\"]\n",
                "    geo = inputs[\"geospatial\"]\n",
                "    temp = inputs[\"temporal\"]\n",
                "\n",
                "    print(f\"spatiotemporal shape: {st.shape}\")\n",
                "    print(f\"geospatial shape:     {geo.shape}\")\n",
                "    print(f\"temporal shape:       {temp.shape}\")\n",
                "    print(f\"labels shape:         {labels.shape}\")\n",
                "\n",
                "    try:\n",
                "        preds = model.call_n(inputs, n=10)\n",
                "        print(f\"predictions shape:    {preds.shape}\")\n",
                "        all_preds.append(preds)\n",
                "        all_labels.append(labels)\n",
                "    except Exception as e:\n",
                "        print(f\"Error at batch {i}: {e}\")\n",
                "        break\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 4\n",
                "N_STEPS = 10\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_bs = inputs[\"spatiotemporal\"].shape[0]\n",
                "\n",
                "    if current_bs < BATCH_SIZE:\n",
                "        print(f\"[Batch {i}] Incomplete batch of size {current_bs}, padding to {BATCH_SIZE}\")\n",
                "\n",
                "        # Repeat the last sample to pad\n",
                "        repeats = BATCH_SIZE - current_bs\n",
                "\n",
                "        def pad_tensor(t):\n",
                "            return tf.concat([t, tf.repeat(t[-1:], repeats=repeats, axis=0)], axis=0)\n",
                "\n",
                "        padded_inputs = {\n",
                "            k: pad_tensor(v) for k, v in inputs.items()\n",
                "        }\n",
                "\n",
                "        # Predict on padded batch\n",
                "        preds_padded = model.call_n(padded_inputs, n=N_STEPS)  # [B, T, H, W]\n",
                "\n",
                "        # Remove the extra samples\n",
                "        preds = preds_padded[:current_bs]\n",
                "    else:\n",
                "        preds = model.call_n(inputs, n=N_STEPS)\n",
                "\n",
                "    print(f\"[Batch {i}] Prediction shape: {preds.shape}\")\n",
                "    all_preds.append(preds)\n",
                "    all_labels.append(labels)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# After running all batches\n",
                "final_preds = tf.concat(all_preds, axis=0)  # shape: [N, T, H, W]\n",
                "max_preds_all = tf.reduce_max(final_preds, axis=1)  # shape: [N, H, W]\n",
                "max_preds_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all = []\n",
                "\n",
                "for labels in all_labels:\n",
                "    max_labels = tf.reduce_max(labels, axis=1)  # shape: [B, H, W]\n",
                "    max_labels_all.append(max_labels)\n",
                "\n",
                "# Now stack (uses less memory)\n",
                "max_labels_all = tf.concat(max_labels_all, axis=0)  # shape: [N, H, W]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "i = 10  # sample index\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.imshow(max_preds_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Predicted Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.imshow(max_labels_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Ground Truth Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Initialize GCS client\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over batches\n",
                "for i, preds in enumerate(all_preds):  # each preds: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]  # shape: [H, W]\n",
                "\n",
                "        # Save to temporary .npy file\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".npy\") as tmp:\n",
                "            np.save(tmp.name, sample)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictions/max_flood_batch{i}_sample{j}.npy\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install rasterio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import rasterio\n",
                "from rasterio.transform import from_origin\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Define metadata — adjust as needed\n",
                "pixel_size = 1  # in meters or units per pixel\n",
                "top_left_x = 0  # e.g., UTM x or longitude\n",
                "top_left_y = 0  # e.g., UTM y or latitude\n",
                "\n",
                "transform = from_origin(top_left_x, top_left_y, pixel_size, pixel_size)\n",
                "crs = \"EPSG:4326\"  # or your local UTM projection\n",
                "\n",
                "# Setup GCS\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over prediction batches\n",
                "for i, preds in enumerate(all_preds):  # shape: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]\n",
                "\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".tif\") as tmp:\n",
                "            with rasterio.open(\n",
                "                tmp.name,\n",
                "                \"w\",\n",
                "                driver=\"GTiff\",\n",
                "                height=sample.shape[0],\n",
                "                width=sample.shape[1],\n",
                "                count=1,\n",
                "                dtype=sample.dtype,\n",
                "                crs=crs,\n",
                "                transform=transform,\n",
                "            ) as dst:\n",
                "                dst.write(sample, 1)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictionstiff/max_flood_batch{i}_sample{j}.tif\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded GeoTIFF: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
                "# Path to your saved model\n",
                "model_path = \"/home/se2890/climateiq-cnn-5/logs/htune_project_20250804-182136/model\"\n",
                "loaded_model = tf.keras.models.load_model(model_path)\n",
                "loaded_model.summary()\n",
                "# Load the model\n",
                "# model = tf.keras.models.load_model(model_path)\n",
                "# model = FloodModel.from_checkpoint(model_path)\n",
                "\n",
                "from usl_models.flood_ml.model import SpatialAttention\n",
                "custom_objects = {'SpatialAttention': SpatialAttention}\n",
                "loaded_model = tf.keras.models.load_model(\n",
                "    model_path,\n",
                "    custom_objects=custom_objects,\n",
                "    compile=False\n",
                ")\n",
                "model.set_weights(loaded_model.get_weights())\n",
                "\n",
                "# # Test calling the model for n predictions\n",
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
                "inputs, labels = next(iter(full_dataset))\n",
                "predictions = model.call_n(inputs, n=4)\n",
                "predictions.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loss_scale = best_hp.get(\"loss_scale\")\n",
                "print(\"Loss scale used during training:\", loss_scale)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "from usl_models.flood_ml import constants\n",
                "\n",
                "# Path to trained model\n",
                "# Known value used during training\n",
                "loss_scale = 200.0\n",
                "\n",
                "# Path to trained model\n",
                "model_path = \"/home/se2890/climateiq-cnn-5/logs/htune_project_20250801-155126/model\"\n",
                "\n",
                "# Create the loss function with the correct scale\n",
                "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
                "\n",
                "# Load model with custom loss function\n",
                "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
                "# Number of samples to visualize\n",
                "n_samples = 20\n",
                "\n",
                "# Loop through the dataset and predict\n",
                "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
                "    ground_truth = ground_truth.numpy().squeeze()\n",
                "    prediction = model(input_data).numpy().squeeze()\n",
                "\n",
                "    print(f\"\\nSample {i+1} Prediction Stats:\")\n",
                "    print(\"  Min:\", prediction.min())\n",
                "    print(\"  Max:\", prediction.max())\n",
                "    print(\"  Mean:\", prediction.mean())\n",
                "\n",
                "    # Choose timestep to plot\n",
                "    timestep = 3\n",
                "    gt_t = ground_truth[timestep]\n",
                "    pred_t = prediction[timestep]\n",
                "    vmax_val = max(gt_t.max(), pred_t.max())\n",
                "\n",
                "    # Plot Ground Truth and Prediction\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
                "\n",
                "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[0].set_title(\"Ground Truth\")\n",
                "    axes[0].axis(\"off\")\n",
                "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
                "\n",
                "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[1].set_title(\"Prediction\")\n",
                "    axes[1].axis(\"off\")\n",
                "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "from usl_models.flood_ml import constants\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from skimage.metrics import structural_similarity as ssim\n",
                "import pandas as pd\n",
                "\n",
                "# Path to trained model\n",
                "# Known value used during training\n",
                "loss_scale = 150.0\n",
                "\n",
                "# Path to trained model\n",
                "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
                "\n",
                "# Create the loss function with the correct scale\n",
                "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
                "\n",
                "# Load model with custom loss function\n",
                "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
                "\n",
                "\n",
                "# Assuming validation_dataset is already defined\n",
                "# Example:\n",
                "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "# validation_dataset = load_dataset_windowed(...)\n",
                "\n",
                "n_samples = 20\n",
                "timestep = 2\n",
                "metrics_list = []\n",
                "\n",
                "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
                "    ground_truth = ground_truth.numpy().squeeze()\n",
                "    prediction = model(input_data).numpy().squeeze()\n",
                "\n",
                "    gt_t = ground_truth[timestep]\n",
                "    pred_t = prediction[timestep]\n",
                "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
                "\n",
                "    # Mask out NaNs\n",
                "    mask = ~np.isnan(gt_t)\n",
                "    gt_flat = gt_t[mask].flatten()\n",
                "    pred_flat = pred_t[mask].flatten()\n",
                "\n",
                "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
                "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
                "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
                "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum())\n",
                "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
                "\n",
                "    metrics_list.append({\n",
                "        \"Sample\": i+1,\n",
                "        \"MAE\": mae,\n",
                "        \"RMSE\": rmse,\n",
                "        \"Bias\": bias,\n",
                "        \"IoU > 0.1\": iou,\n",
                "        \"SSIM\": ssim_val\n",
                "    })\n",
                "\n",
                "    # Plot\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
                "\n",
                "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[0].set_title(\"Ground Truth\")\n",
                "    axes[0].axis(\"off\")\n",
                "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
                "\n",
                "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[1].set_title(\"Prediction\")\n",
                "    axes[1].axis(\"off\")\n",
                "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Convert to DataFrame\n",
                "df = pd.DataFrame(metrics_list)\n",
                "print(\"\\n=== Metrics Summary ===\")\n",
                "print(df.describe())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "from usl_models.flood_ml import constants\n",
                "from usl_models.flood_ml import customloss\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from skimage.metrics import structural_similarity as ssim\n",
                "import pandas as pd\n",
                "\n",
                "# Parameters\n",
                "loss_scale = 200.0\n",
                "timestep = 3\n",
                "n_samples = 20\n",
                "\n",
                "# Paths to models\n",
                "model_path_1 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/attention/model\"\n",
                "model_path_2 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250612-010926/model\"\n",
                "\n",
                "# Loss function\n",
                "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
                "\n",
                "# Load models\n",
                "model_1 = tf.keras.models.load_model(model_path_1, custom_objects={\"loss_fn\": loss_fn})\n",
                "model_2 = tf.keras.models.load_model(model_path_2, custom_objects={\"loss_fn\": loss_fn})\n",
                "\n",
                "# Load validation dataset (ensure it's already prepared)\n",
                "# Example:\n",
                "# validation_dataset = load_dataset_windowed(...)\n",
                "\n",
                "metrics_list = []\n",
                "\n",
                "for i, (input_data, ground_truth) in enumerate(train_dataset.take(n_samples)):\n",
                "    ground_truth = ground_truth.numpy().squeeze()\n",
                "\n",
                "    pred_1 = model_1(input_data).numpy().squeeze()\n",
                "    pred_2 = model_2(input_data).numpy().squeeze()\n",
                "\n",
                "    gt_t = ground_truth[timestep]\n",
                "    pred_1_t = pred_1[timestep]\n",
                "    pred_2_t = pred_2[timestep]\n",
                "    vmax_val = np.nanpercentile([gt_t, pred_1_t, pred_2_t], 99.5)\n",
                "\n",
                "    mask = ~np.isnan(gt_t)\n",
                "    gt_flat = gt_t[mask].flatten()\n",
                "    pred_1_flat = pred_1_t[mask].flatten()\n",
                "    pred_2_flat = pred_2_t[mask].flatten()\n",
                "\n",
                "    # Compute metrics\n",
                "    metrics_list.append({\n",
                "        \"Sample\": i+1,\n",
                "        \"MAE_1\": mean_absolute_error(gt_flat, pred_1_flat),\n",
                "        \"RMSE_1\": np.sqrt(mean_squared_error(gt_flat, pred_1_flat)),\n",
                "        \"Bias_1\": np.mean(pred_1_flat) - np.mean(gt_flat),\n",
                "        \"IoU_1\": np.logical_and(gt_flat > 0.1, pred_1_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_1_flat > 0.1).sum()),\n",
                "        \"SSIM_1\": ssim(gt_t, pred_1_t, data_range=gt_t.max() - gt_t.min()),\n",
                "\n",
                "        \"MAE_2\": mean_absolute_error(gt_flat, pred_2_flat),\n",
                "        \"RMSE_2\": np.sqrt(mean_squared_error(gt_flat, pred_2_flat)),\n",
                "        \"Bias_2\": np.mean(pred_2_flat) - np.mean(gt_flat),\n",
                "        \"IoU_2\": np.logical_and(gt_flat > 0.1, pred_2_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_2_flat > 0.1).sum()),\n",
                "        \"SSIM_2\": ssim(gt_t, pred_2_t, data_range=gt_t.max() - gt_t.min()),\n",
                "    })\n",
                "\n",
                "    # Plotting\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
                "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
                "\n",
                "    axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[0].set_title(\"Ground Truth\")\n",
                "    axes[0].axis(\"off\")\n",
                "\n",
                "    axes[1].imshow(pred_1_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[1].set_title(\"attention\")\n",
                "    axes[1].axis(\"off\")\n",
                "\n",
                "    axes[2].imshow(pred_2_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[2].set_title(\"without attention\")\n",
                "    axes[2].axis(\"off\")\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Summary metrics\n",
                "df = pd.DataFrame(metrics_list)\n",
                "print(\"\\n=== Metrics Summary ===\")\n",
                "print(df.describe())\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
