{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Flood Model Training Notebook\n",
                "\n",
                "Train a Flood ConvLSTM Model using `usl_models` lib."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training on 1 simulations.\n",
                        "Atlanta_Prediction-Atlanta_config\n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "import keras_tuner\n",
                "import time\n",
                "import keras\n",
                "import logging\n",
                "from usl_models.flood_ml import constants\n",
                "from usl_models.flood_ml.model import FloodModel\n",
                "from usl_models.flood_ml.model_params import FloodModelParams\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed, load_dataset\n",
                "from usl_models.flood_ml import customloss\n",
                "\n",
                "# Setup\n",
                "logging.getLogger().setLevel(logging.WARNING)\n",
                "keras.utils.set_random_seed(812)\n",
                "\n",
                "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
                "    tf.config.experimental.set_memory_growth(gpu, True)\n",
                "\n",
                "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
                "\n",
                "# Cities and their config folders\n",
                "city_config_mapping = {\n",
                "    # \"Manhattan\": \"Manhattan_config\",\n",
                "    # \"Atlanta_Prediction\": \"Atlanta_config\",\n",
                "    \"Phoenix_SM\": \"PHX_SM\",\n",
                "    \"Phoenix_PV\": \"PHX_PV\",\n",
                "    # \"Navarre_FL\": \"Navarre_config\", \n",
                "    # \"Denton_TX\":\"Denton_config\",\n",
                "    # \"Boise_ID\" : \"Boise_config\",\n",
                "}\n",
                "\n",
                "# Rainfall files you want\n",
                "rainfall_files = [19]  # Only 5 and 6\n",
                "\n",
                "# Generate sim_names\n",
                "sim_names = []\n",
                "for city, config in city_config_mapping.items():\n",
                "    for rain_id in rainfall_files:\n",
                "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
                "        sim_names.append(sim_name)\n",
                "\n",
                "print(f\"Training on {len(sim_names)} simulations.\")\n",
                "for s in sim_names:\n",
                "    print(s)\n",
                "\n",
                "# Now load dataset\n",
                "train_dataset = load_dataset_windowed(\n",
                "    sim_names=sim_names, batch_size=4, dataset_split=\"train\"\n",
                ").cache()\n",
                "\n",
                "validation_dataset = load_dataset_windowed(\n",
                "    sim_names=sim_names, batch_size=4, dataset_split=\"val\"\n",
                ").cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-22 18:27:50.002752: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n",
                        "Traceback (most recent call last):\n",
                        "\n",
                        "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
                        "    ret = func(*args)\n",
                        "          ^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
                        "    return func(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
                        "    values = next(generator_state.get_iterator(iterator_id))\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 124, in generator\n",
                        "    for model_input, labels in _iter_model_inputs(\n",
                        "\n",
                        "  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 290, in _iter_model_inputs\n",
                        "    metastore.get_temporal_feature_metadata(firestore_client, sim_name),\n",
                        "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/metastore.py\", line 64, in get_temporal_feature_metadata\n",
                        "    raise ValueError(f\"No such simulation {sim_name} found.\")\n",
                        "\n",
                        "ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n",
                        "\n",
                        "\n"
                    ]
                },
                {
                    "ename": "InvalidArgumentError",
                    "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 124, in generator\n    for model_input, labels in _iter_model_inputs(\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 290, in _iter_model_inputs\n    metastore.get_temporal_feature_metadata(firestore_client, sim_name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/metastore.py\", line 64, in get_temporal_feature_metadata\n    raise ValueError(f\"No such simulation {sim_name} found.\")\n\nValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count training samples\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Count validation samples\u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count training samples\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal training samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Count validation samples\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 124, in generator\n    for model_input, labels in _iter_model_inputs(\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 290, in _iter_model_inputs\n    metastore.get_temporal_feature_metadata(firestore_client, sim_name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/metastore.py\", line 64, in get_temporal_feature_metadata\n    raise ValueError(f\"No such simulation {sim_name} found.\")\n\nValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "\n",
                "# Count training samples\n",
                "train_count = sum(\n",
                "    tf.shape(batch[1])[0].numpy() for batch in train_dataset\n",
                ")\n",
                "print(f\"Total training samples: {train_count}\")\n",
                "\n",
                "# Count validation samples\n",
                "val_count = sum(\n",
                "    tf.shape(batch[1])[0].numpy() for batch in validation_dataset\n",
                ")\n",
                "print(f\"Total validation samples: {val_count}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total samples in train_dataset: 0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-08-22 18:27:56.734059: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n",
                        "Traceback (most recent call last):\n",
                        "\n",
                        "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
                        "    ret = func(*args)\n",
                        "          ^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
                        "    return func(*args, **kwargs)\n",
                        "           ^^^^^^^^^^^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
                        "    values = next(generator_state.get_iterator(iterator_id))\n",
                        "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 124, in generator\n",
                        "    for model_input, labels in _iter_model_inputs(\n",
                        "\n",
                        "  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 290, in _iter_model_inputs\n",
                        "    metastore.get_temporal_feature_metadata(firestore_client, sim_name),\n",
                        "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "\n",
                        "  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/metastore.py\", line 64, in get_temporal_feature_metadata\n",
                        "    raise ValueError(f\"No such simulation {sim_name} found.\")\n",
                        "\n",
                        "ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n",
                        "\n",
                        "\n"
                    ]
                },
                {
                    "ename": "InvalidArgumentError",
                    "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 124, in generator\n    for model_input, labels in _iter_model_inputs(\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 290, in _iter_model_inputs\n    metastore.get_temporal_feature_metadata(firestore_client, sim_name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/metastore.py\", line 64, in get_temporal_feature_metadata\n    raise ValueError(f\"No such simulation {sim_name} found.\")\n\nValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m train_dataset)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal samples in train_dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, count)\n\u001b[0;32m----> 4\u001b[0m val_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal samples in validation_dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_count)\n",
                        "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m train_dataset)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal samples in train_dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, count)\n\u001b[0;32m----> 4\u001b[0m val_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal samples in validation_dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_count)\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 124, in generator\n    for model_input, labels in _iter_model_inputs(\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/dataset.py\", line 290, in _iter_model_inputs\n    metastore.get_temporal_feature_metadata(firestore_client, sim_name),\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/se2890/climateiq-cnn-6/usl_models/usl_models/flood_ml/metastore.py\", line 64, in get_temporal_feature_metadata\n    raise ValueError(f\"No such simulation {sim_name} found.\")\n\nValueError: No such simulation Atlanta_Prediction-Atlanta_config found.\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
                    ]
                }
            ],
            "source": [
                "count = sum(1 for _ in train_dataset)\n",
                "print(\"Total samples in train_dataset:\", count)\n",
                "\n",
                "val_count = sum(1 for _ in validation_dataset)\n",
                "print(\"Total samples in validation_dataset:\", val_count)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "count = train_dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
                "print(\"Total samples in train_dataset:\", count)\n",
                "\n",
                "val_count = validation_dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
                "print(\"Total samples in validation_dataset:\", val_count)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,    # ← use ALL chunks (no train/val/test filter)\n",
                "    batch_size=4 \n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.dataset import load_dataset, load_dataset_windowed\n",
                "\n",
                "# --- your existing sim_names here ---\n",
                "# sim_names = [...]\n",
                "\n",
                "def count_dataset(ds):\n",
                "    return sum(1 for _ in ds)\n",
                "\n",
                "# 1) CHUNKS (non-windowed) --------------------------\n",
                "train_chunks_ds = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=\"train\",\n",
                "    batch_size=None,          # <- no batching: 1 element == 1 chunk\n",
                ")\n",
                "all_chunks_ds = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,       # <- ALL splits combined\n",
                "    batch_size=None,\n",
                ")\n",
                "\n",
                "train_chunks = count_dataset(train_chunks_ds)\n",
                "all_chunks = count_dataset(all_chunks_ds)\n",
                "\n",
                "print(f\"Chunks in TRAIN only: {train_chunks}\")\n",
                "print(f\"Chunks in ALL splits: {all_chunks}\")\n",
                "\n",
                "# 2) WINDOWS (windowed for teacher-forcing) --------\n",
                "train_windows_ds = load_dataset_windowed(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=\"train\",\n",
                "    batch_size=None,          # <- no batching: 1 element == 1 window\n",
                ")\n",
                "all_windows_ds = load_dataset_windowed(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,       # <- ALL splits combined (works with your updated code)\n",
                "    batch_size=None,\n",
                ")\n",
                "\n",
                "train_windows = count_dataset(train_windows_ds)\n",
                "all_windows = count_dataset(all_windows_ds)\n",
                "\n",
                "print(f\"Windows in TRAIN only: {train_windows}\")\n",
                "print(f\"Windows in ALL splits: {all_windows}\")\n",
                "\n",
                "# 3) (Optional) peek at one example shape ----------\n",
                "ex_inputs, ex_labels = next(iter(all_chunks_ds.take(1)))\n",
                "print(\"Example CHUNK shapes:\")\n",
                "print(\"  spatiotemporal:\", ex_inputs[\"spatiotemporal\"].shape)  # [N, H, W, 1]\n",
                "print(\"  geospatial:    \", ex_inputs[\"geospatial\"].shape)       # [H, W, F]\n",
                "print(\"  temporal:      \", ex_inputs[\"temporal\"].shape)         # [T_MAX, M]\n",
                "print(\"  labels:        \", ex_labels.shape)                     # [T_label, H, W]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This will iterate the dataset WITHOUT batching so you see the raw order\n",
                "debug_dataset = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,   # full (train+val+test)\n",
                "    batch_size=None       # no batching so we see per-chunk order\n",
                ")\n",
                "\n",
                "print(\"First 20 chunk indices from full dataset:\")\n",
                "for i, (features, labels) in enumerate(debug_dataset.take(20)):\n",
                "    # Pull the chunk's position from Firestore metadata order\n",
                "    # Features are shape [H, W, ...], but we don't know the indices unless we print in _iter_geo_feature_label_tensors\n",
                "    print(f\"Sample {i}: feature shape = {features['geospatial'].shape}, label shape = {labels.shape}\")\n",
                "\n",
                "# If you want exact (x_index, y_index) printed:\n",
                "# Add a debug print in _iter_geo_feature_label_tensors before yield:\n",
                "# print(\"Yielding chunk\", index)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(\n",
                "    sim_names=sim_names,\n",
                "    dataset_split=None,  # Load all splits combined\n",
                "    batch_size=None      # No batching, one chunk at a time\n",
                ")\n",
                "\n",
                "from usl_models.flood_ml import metastore\n",
                "from google.cloud import firestore, storage\n",
                "\n",
                "firestore_client = firestore.Client()\n",
                "storage_client = storage.Client()\n",
                "\n",
                "for sim_name in sim_names:\n",
                "    # Get ALL label chunks without split filter\n",
                "    label_chunks_collection = metastore._get_simulation_doc(\n",
                "        firestore_client, sim_name\n",
                "    ).collection(\"label_chunks\")\n",
                "    label_metadata = [doc.to_dict() for doc in label_chunks_collection.stream()]\n",
                "\n",
                "    # Sort by (x_index, y_index) to match dataset order\n",
                "    ordered_labels = sorted(\n",
                "        label_metadata, key=lambda l: (l[\"x_index\"], l[\"y_index\"])\n",
                "    )\n",
                "\n",
                "    # Iterate over ALL chunks (no slicing)\n",
                "    for i, label in enumerate(ordered_labels):\n",
                "        print(f\"Chunk {i}: {label['gcs_uri']}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_dataset(ds):\n",
                "    return sum(1 for _ in ds)\n",
                "\n",
                "# No batching so we count raw elements\n",
                "train_ds = load_dataset(sim_names=sim_names, dataset_split=\"train\", batch_size=None)\n",
                "val_ds   = load_dataset(sim_names=sim_names, dataset_split=\"val\", batch_size=None)\n",
                "test_ds  = load_dataset(sim_names=sim_names, dataset_split=\"test\", batch_size=None)\n",
                "full_ds  = load_dataset(sim_names=sim_names, dataset_split=None,    batch_size=None)\n",
                "\n",
                "train_count = count_dataset(train_ds)\n",
                "val_count   = count_dataset(val_ds)\n",
                "test_count  = count_dataset(test_ds)\n",
                "full_count  = count_dataset(full_ds)\n",
                "\n",
                "print(f\"Train chunks: {train_count}\")\n",
                "print(f\"Val chunks:   {val_count}\")\n",
                "print(f\"Test chunks:  {test_count}\")\n",
                "print(f\"Full chunks:  {full_count}\")\n",
                "print(f\"Sum of splits: {train_count + val_count + test_count}\")\n",
                "print(\"MATCH:\", full_count == (train_count + val_count + test_count))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_count = sum(1 for _ in train_dataset)\n",
                "full_count = sum(1 for _ in full_dataset)\n",
                "\n",
                "print(f\"Train batches: {train_count}\")\n",
                "print(f\"Full batches: {full_count}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_chunks = 0\n",
                "for inputs, labels in full_dataset:\n",
                "    full_chunks += int(inputs[\"geospatial\"].shape[0])  # batch size for this batch\n",
                "print(\"Full chunks (all splits combined):\", full_chunks)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "T_max = 169  # adjust\n",
                "train_windows = 0\n",
                "for inputs, label in train_dataset:  # windowed: label shape [B, H, W]\n",
                "    train_windows += int(label.shape[0])\n",
                "approx_train_chunks = train_windows // T_max\n",
                "print(\"Train windows:\", train_windows)\n",
                "print(\"≈ Train chunks:\", approx_train_chunks)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "tuner = keras_tuner.BayesianOptimization(\n",
                "    FloodModel.get_hypermodel(\n",
                "        lstm_units=[32, 64, 128],\n",
                "        lstm_kernel_size=[3, 5],\n",
                "        lstm_dropout=[0.2, 0.3],\n",
                "        lstm_recurrent_dropout=[0.2, 0.3],\n",
                "        n_flood_maps=[5],\n",
                "        m_rainfall=[6],\n",
                "    ),\n",
                "    objective=\"val_loss\",\n",
                "    max_trials=1,\n",
                "    project_name=f\"logs/htune_project_{timestamp}\",\n",
                ")\n",
                "\n",
                "tuner.search_space_summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "log_dir = f\"logs/htune_project_{timestamp}\"\n",
                "print(log_dir)\n",
                "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
                "tuner.search(\n",
                "    train_dataset,\n",
                "    epochs=2,\n",
                "    validation_data=validation_dataset,\n",
                "    callbacks=[tb_callback],\n",
                ")\n",
                "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
                "best_hp.values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "\n",
                "# Define final parameters and model\n",
                "final_params_dict = best_hp.values.copy()\n",
                "final_params = FloodModel.Params(**final_params_dict)\n",
                "model = FloodModel(params=final_params)\n",
                "# Define callbacks\n",
                "callbacks = [\n",
                "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
                "    ModelCheckpoint(\n",
                "        filepath=log_dir + \"/checkpoint\",\n",
                "        save_best_only=True,\n",
                "        monitor=\"val_loss\",\n",
                "        mode=\"min\",\n",
                "        save_format=\"tf\",\n",
                "    ),\n",
                "    EarlyStopping(  # <--- ADD THIS\n",
                "        monitor=\"val_loss\",  # What to monitor\n",
                "        patience=100,  # Number of epochs with no improvement to wait\n",
                "        restore_best_weights=True,  # Restore model weights from best epoch\n",
                "        mode=\"min\",  # \"min\" because lower val_loss is better\n",
                "    ),\n",
                "]\n",
                "\n",
                "# Train\n",
                "model.fit(train_dataset, validation_dataset, epochs=2, callbacks=callbacks)\n",
                "\n",
                "# Save final model\n",
                "model.save_model(log_dir + \"/model\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Test calling the model on some data.\n",
                "inputs, labels_ = next(iter(train_dataset))\n",
                "prediction = model.call(inputs)\n",
                "prediction.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
                "# Path to your saved model\n",
                "model_path = \"/home/se2890/climateiq-cnn-6/logs/htune_project_20250817-153929/model\"\n",
                "#loaded_model = tf.keras.models.load_model(model_path)\n",
                "#loaded_model.summary()\n",
                "# Load the model\n",
                "model = tf.keras.models.load_model(model_path)\n",
                "\n",
                "from usl_models.flood_ml.model import SpatialAttention\n",
                "custom_objects = {'SpatialAttention': SpatialAttention}\n",
                "loaded_model = tf.keras.models.load_model(\n",
                "    model_path,\n",
                "    custom_objects=custom_objects,\n",
                "    compile=False\n",
                ")\n",
                "model.set_weights(loaded_model.get_weights())\n",
                "\n",
                "# # # Test calling the model for n predictions\n",
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
                "inputs, labels = next(iter(full_dataset))\n",
                "predictions = model.call_n(inputs, n=10)\n",
                "predictions.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ref_shapes = None\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_shapes = (\n",
                "        inputs[\"spatiotemporal\"].shape,\n",
                "        inputs[\"geospatial\"].shape,\n",
                "        inputs[\"temporal\"].shape,\n",
                "        labels.shape,\n",
                "    )\n",
                "\n",
                "    if ref_shapes is None:\n",
                "        ref_shapes = current_shapes\n",
                "    else:\n",
                "        assert current_shapes == ref_shapes, f\"Mismatch at batch {i}: {current_shapes} ≠ {ref_shapes}\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split=\"train\")\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    print(f\"\\n--- Batch {i} ---\")\n",
                "    \n",
                "    st = inputs[\"spatiotemporal\"]\n",
                "    geo = inputs[\"geospatial\"]\n",
                "    temp = inputs[\"temporal\"]\n",
                "\n",
                "    print(f\"spatiotemporal shape: {st.shape}\")\n",
                "    print(f\"geospatial shape:     {geo.shape}\")\n",
                "    print(f\"temporal shape:       {temp.shape}\")\n",
                "    print(f\"labels shape:         {labels.shape}\")\n",
                "\n",
                "    try:\n",
                "        preds = model.call_n(inputs, n=10)\n",
                "        print(f\"predictions shape:    {preds.shape}\")\n",
                "        all_preds.append(preds)\n",
                "        all_labels.append(labels)\n",
                "    except Exception as e:\n",
                "        print(f\"Error at batch {i}: {e}\")\n",
                "        break\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 4\n",
                "N_STEPS = 10\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_bs = inputs[\"spatiotemporal\"].shape[0]\n",
                "\n",
                "    if current_bs < BATCH_SIZE:\n",
                "        print(f\"[Batch {i}] Incomplete batch of size {current_bs}, padding to {BATCH_SIZE}\")\n",
                "\n",
                "        # Repeat the last sample to pad\n",
                "        repeats = BATCH_SIZE - current_bs\n",
                "\n",
                "        def pad_tensor(t):\n",
                "            return tf.concat([t, tf.repeat(t[-1:], repeats=repeats, axis=0)], axis=0)\n",
                "\n",
                "        padded_inputs = {\n",
                "            k: pad_tensor(v) for k, v in inputs.items()\n",
                "        }\n",
                "\n",
                "        # Predict on padded batch\n",
                "        preds_padded = model.call_n(padded_inputs, n=N_STEPS)  # [B, T, H, W]\n",
                "\n",
                "        # Remove the extra samples\n",
                "        preds = preds_padded[:current_bs]\n",
                "    else:\n",
                "        preds = model.call_n(inputs, n=N_STEPS)\n",
                "\n",
                "    print(f\"[Batch {i}] Prediction shape: {preds.shape}\")\n",
                "    all_preds.append(preds)\n",
                "    all_labels.append(labels)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# After running all batches\n",
                "final_preds = tf.concat(all_preds, axis=0)  # shape: [N, T, H, W]\n",
                "max_preds_all = tf.reduce_max(final_preds, axis=1)  # shape: [N, H, W]\n",
                "max_preds_all.shape\n",
                "max_labels_all = []\n",
                "\n",
                "for labels in all_labels:\n",
                "    max_labels = tf.reduce_max(labels, axis=1)  # shape: [B, H, W]\n",
                "    max_labels_all.append(max_labels)\n",
                "\n",
                "# Now stack (uses less memory)\n",
                "max_labels_all = tf.concat(max_labels_all, axis=0)  # shape: [N, H, W]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "i = 10  # sample index\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.imshow(max_preds_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Predicted Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.imshow(max_labels_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Ground Truth Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Initialize GCS client\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over batches\n",
                "for i, preds in enumerate(all_preds):  # each preds: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]  # shape: [H, W]\n",
                "\n",
                "        # Save to temporary .npy file\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".npy\") as tmp:\n",
                "            np.save(tmp.name, sample)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictions/max_flood_batch{i}_sample{j}.npy\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install rasterio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import rasterio\n",
                "from rasterio.transform import from_origin\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Define metadata — adjust as needed\n",
                "pixel_size = 1  # in meters or units per pixel\n",
                "top_left_x = 0  # e.g., UTM x or longitude\n",
                "top_left_y = 0  # e.g., UTM y or latitude\n",
                "\n",
                "transform = from_origin(top_left_x, top_left_y, pixel_size, pixel_size)\n",
                "crs = \"EPSG:4326\"  # or your local UTM projection\n",
                "\n",
                "# Setup GCS\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over prediction batches\n",
                "for i, preds in enumerate(all_preds):  # shape: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]\n",
                "\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".tif\") as tmp:\n",
                "            with rasterio.open(\n",
                "                tmp.name,\n",
                "                \"w\",\n",
                "                driver=\"GTiff\",\n",
                "                height=sample.shape[0],\n",
                "                width=sample.shape[1],\n",
                "                count=1,\n",
                "                dtype=sample.dtype,\n",
                "                crs=crs,\n",
                "                transform=transform,\n",
                "            ) as dst:\n",
                "                dst.write(sample, 1)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictionstiff/max_flood_batch{i}_sample{j}.tif\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded GeoTIFF: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
                "# Path to your saved model\n",
                "model_path = \"/home/se2890/climateiq-cnn-6/logs/htune_project_20250815-144148/model\"\n",
                "#loaded_model = tf.keras.models.load_model(model_path)\n",
                "#loaded_model.summary()\n",
                "# Load the model\n",
                "model = tf.keras.models.load_model(model_path)\n",
                "\n",
                "from usl_models.flood_ml.model import SpatialAttention\n",
                "custom_objects = {'SpatialAttention': SpatialAttention}\n",
                "loaded_model = tf.keras.models.load_model(\n",
                "    model_path,\n",
                "    custom_objects=custom_objects,\n",
                "    compile=False\n",
                ")\n",
                "model.set_weights(loaded_model.get_weights())\n",
                "\n",
                "# # # Test calling the model for n predictions\n",
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
                "inputs, labels = next(iter(full_dataset))\n",
                "predictions = model.call_n(inputs, n=10)\n",
                "predictions.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ref_shapes = None\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_shapes = (\n",
                "        inputs[\"spatiotemporal\"].shape,\n",
                "        inputs[\"geospatial\"].shape,\n",
                "        inputs[\"temporal\"].shape,\n",
                "        labels.shape,\n",
                "    )\n",
                "\n",
                "    if ref_shapes is None:\n",
                "        ref_shapes = current_shapes\n",
                "    else:\n",
                "        assert current_shapes == ref_shapes, f\"Mismatch at batch {i}: {current_shapes} ≠ {ref_shapes}\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split=\"train\")\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    print(f\"\\n--- Batch {i} ---\")\n",
                "    \n",
                "    st = inputs[\"spatiotemporal\"]\n",
                "    geo = inputs[\"geospatial\"]\n",
                "    temp = inputs[\"temporal\"]\n",
                "\n",
                "    print(f\"spatiotemporal shape: {st.shape}\")\n",
                "    print(f\"geospatial shape:     {geo.shape}\")\n",
                "    print(f\"temporal shape:       {temp.shape}\")\n",
                "    print(f\"labels shape:         {labels.shape}\")\n",
                "\n",
                "    try:\n",
                "        preds = model.call_n(inputs, n=10)\n",
                "        print(f\"predictions shape:    {preds.shape}\")\n",
                "        all_preds.append(preds)\n",
                "        all_labels.append(labels)\n",
                "    except Exception as e:\n",
                "        print(f\"Error at batch {i}: {e}\")\n",
                "        break\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "BATCH_SIZE = 4\n",
                "N_STEPS = 10\n",
                "\n",
                "all_preds = []\n",
                "all_labels = []\n",
                "\n",
                "for i, (inputs, labels) in enumerate(full_dataset):\n",
                "    current_bs = inputs[\"spatiotemporal\"].shape[0]\n",
                "\n",
                "    if current_bs < BATCH_SIZE:\n",
                "        print(f\"[Batch {i}] Incomplete batch of size {current_bs}, padding to {BATCH_SIZE}\")\n",
                "\n",
                "        # Repeat the last sample to pad\n",
                "        repeats = BATCH_SIZE - current_bs\n",
                "\n",
                "        def pad_tensor(t):\n",
                "            return tf.concat([t, tf.repeat(t[-1:], repeats=repeats, axis=0)], axis=0)\n",
                "\n",
                "        padded_inputs = {\n",
                "            k: pad_tensor(v) for k, v in inputs.items()\n",
                "        }\n",
                "\n",
                "        # Predict on padded batch\n",
                "        preds_padded = model.call_n(padded_inputs, n=N_STEPS)  # [B, T, H, W]\n",
                "\n",
                "        # Remove the extra samples\n",
                "        preds = preds_padded[:current_bs]\n",
                "    else:\n",
                "        preds = model.call_n(inputs, n=N_STEPS)\n",
                "\n",
                "    print(f\"[Batch {i}] Prediction shape: {preds.shape}\")\n",
                "    all_preds.append(preds)\n",
                "    all_labels.append(labels)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# After running all batches\n",
                "final_preds = tf.concat(all_preds, axis=0)  # shape: [N, T, H, W]\n",
                "max_preds_all = tf.reduce_max(final_preds, axis=1)  # shape: [N, H, W]\n",
                "max_preds_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all = []\n",
                "\n",
                "for labels in all_labels:\n",
                "    max_labels = tf.reduce_max(labels, axis=1)  # shape: [B, H, W]\n",
                "    max_labels_all.append(max_labels)\n",
                "\n",
                "# Now stack (uses less memory)\n",
                "max_labels_all = tf.concat(max_labels_all, axis=0)  # shape: [N, H, W]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_labels_all.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "i = 10  # sample index\n",
                "\n",
                "plt.figure(figsize=(10, 4))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.imshow(max_preds_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Predicted Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.imshow(max_labels_all[i], cmap=\"Blues\")\n",
                "plt.title(\"Ground Truth Max Flood\")\n",
                "plt.colorbar()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Initialize GCS client\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over batches\n",
                "for i, preds in enumerate(all_preds):  # each preds: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]  # shape: [H, W]\n",
                "\n",
                "        # Save to temporary .npy file\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".npy\") as tmp:\n",
                "            np.save(tmp.name, sample)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictions/max_flood_batch{i}_sample{j}.npy\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pip install rasterio"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import rasterio\n",
                "from rasterio.transform import from_origin\n",
                "from google.cloud import storage\n",
                "import tempfile\n",
                "\n",
                "# Define metadata — adjust as needed\n",
                "pixel_size = 1  # in meters or units per pixel\n",
                "top_left_x = 0  # e.g., UTM x or longitude\n",
                "top_left_y = 0  # e.g., UTM y or latitude\n",
                "\n",
                "transform = from_origin(top_left_x, top_left_y, pixel_size, pixel_size)\n",
                "crs = \"EPSG:4326\"  # or your local UTM projection\n",
                "\n",
                "# Setup GCS\n",
                "client = storage.Client()\n",
                "bucket_name = \"mloutputstest\"\n",
                "bucket = client.bucket(bucket_name)\n",
                "\n",
                "# Loop over prediction batches\n",
                "for i, preds in enumerate(all_preds):  # shape: [B, T, H, W]\n",
                "    max_preds = tf.reduce_max(preds, axis=1).numpy()  # shape: [B, H, W]\n",
                "\n",
                "    for j in range(max_preds.shape[0]):\n",
                "        sample = max_preds[j]\n",
                "\n",
                "        with tempfile.NamedTemporaryFile(suffix=\".tif\") as tmp:\n",
                "            with rasterio.open(\n",
                "                tmp.name,\n",
                "                \"w\",\n",
                "                driver=\"GTiff\",\n",
                "                height=sample.shape[0],\n",
                "                width=sample.shape[1],\n",
                "                count=1,\n",
                "                dtype=sample.dtype,\n",
                "                crs=crs,\n",
                "                transform=transform,\n",
                "            ) as dst:\n",
                "                dst.write(sample, 1)\n",
                "\n",
                "            # Upload to GCS\n",
                "            blob_name = f\"predictionstiff/max_flood_batch{i}_sample{j}.tif\"\n",
                "            blob = bucket.blob(blob_name)\n",
                "            blob.upload_from_filename(tmp.name)\n",
                "\n",
                "            print(f\"Uploaded GeoTIFF: gs://{bucket_name}/{blob_name}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
                "# Path to your saved model\n",
                "model_path = \"/home/se2890/climateiq-cnn-5/logs/htune_project_20250804-182136/model\"\n",
                "loaded_model = tf.keras.models.load_model(model_path)\n",
                "loaded_model.summary()\n",
                "# Load the model\n",
                "# model = tf.keras.models.load_model(model_path)\n",
                "# model = FloodModel.from_checkpoint(model_path)\n",
                "\n",
                "from usl_models.flood_ml.model import SpatialAttention\n",
                "custom_objects = {'SpatialAttention': SpatialAttention}\n",
                "loaded_model = tf.keras.models.load_model(\n",
                "    model_path,\n",
                "    custom_objects=custom_objects,\n",
                "    compile=False\n",
                ")\n",
                "model.set_weights(loaded_model.get_weights())\n",
                "\n",
                "# # Test calling the model for n predictions\n",
                "full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
                "inputs, labels = next(iter(full_dataset))\n",
                "predictions = model.call_n(inputs, n=4)\n",
                "predictions.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loss_scale = best_hp.get(\"loss_scale\")\n",
                "print(\"Loss scale used during training:\", loss_scale)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "from usl_models.flood_ml import constants\n",
                "\n",
                "# Path to trained model\n",
                "# Known value used during training\n",
                "loss_scale = 200.0\n",
                "\n",
                "# Path to trained model\n",
                "model_path = \"/home/se2890/climateiq-cnn-5/logs/htune_project_20250801-155126/model\"\n",
                "\n",
                "# Create the loss function with the correct scale\n",
                "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
                "\n",
                "# Load model with custom loss function\n",
                "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
                "# Number of samples to visualize\n",
                "n_samples = 20\n",
                "\n",
                "# Loop through the dataset and predict\n",
                "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
                "    ground_truth = ground_truth.numpy().squeeze()\n",
                "    prediction = model(input_data).numpy().squeeze()\n",
                "\n",
                "    print(f\"\\nSample {i+1} Prediction Stats:\")\n",
                "    print(\"  Min:\", prediction.min())\n",
                "    print(\"  Max:\", prediction.max())\n",
                "    print(\"  Mean:\", prediction.mean())\n",
                "\n",
                "    # Choose timestep to plot\n",
                "    timestep = 3\n",
                "    gt_t = ground_truth[timestep]\n",
                "    pred_t = prediction[timestep]\n",
                "    vmax_val = max(gt_t.max(), pred_t.max())\n",
                "\n",
                "    # Plot Ground Truth and Prediction\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
                "\n",
                "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[0].set_title(\"Ground Truth\")\n",
                "    axes[0].axis(\"off\")\n",
                "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
                "\n",
                "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[1].set_title(\"Prediction\")\n",
                "    axes[1].axis(\"off\")\n",
                "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "from usl_models.flood_ml import constants\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from skimage.metrics import structural_similarity as ssim\n",
                "import pandas as pd\n",
                "\n",
                "# Path to trained model\n",
                "# Known value used during training\n",
                "loss_scale = 150.0\n",
                "\n",
                "# Path to trained model\n",
                "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
                "\n",
                "# Create the loss function with the correct scale\n",
                "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
                "\n",
                "# Load model with custom loss function\n",
                "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
                "\n",
                "\n",
                "# Assuming validation_dataset is already defined\n",
                "# Example:\n",
                "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "# validation_dataset = load_dataset_windowed(...)\n",
                "\n",
                "n_samples = 20\n",
                "timestep = 2\n",
                "metrics_list = []\n",
                "\n",
                "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
                "    ground_truth = ground_truth.numpy().squeeze()\n",
                "    prediction = model(input_data).numpy().squeeze()\n",
                "\n",
                "    gt_t = ground_truth[timestep]\n",
                "    pred_t = prediction[timestep]\n",
                "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
                "\n",
                "    # Mask out NaNs\n",
                "    mask = ~np.isnan(gt_t)\n",
                "    gt_flat = gt_t[mask].flatten()\n",
                "    pred_flat = pred_t[mask].flatten()\n",
                "\n",
                "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
                "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
                "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
                "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum())\n",
                "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
                "\n",
                "    metrics_list.append({\n",
                "        \"Sample\": i+1,\n",
                "        \"MAE\": mae,\n",
                "        \"RMSE\": rmse,\n",
                "        \"Bias\": bias,\n",
                "        \"IoU > 0.1\": iou,\n",
                "        \"SSIM\": ssim_val\n",
                "    })\n",
                "\n",
                "    # Plot\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
                "\n",
                "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[0].set_title(\"Ground Truth\")\n",
                "    axes[0].axis(\"off\")\n",
                "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
                "\n",
                "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[1].set_title(\"Prediction\")\n",
                "    axes[1].axis(\"off\")\n",
                "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Convert to DataFrame\n",
                "df = pd.DataFrame(metrics_list)\n",
                "print(\"\\n=== Metrics Summary ===\")\n",
                "print(df.describe())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
                "from usl_models.flood_ml import constants\n",
                "from usl_models.flood_ml import customloss\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
                "from skimage.metrics import structural_similarity as ssim\n",
                "import pandas as pd\n",
                "\n",
                "# Parameters\n",
                "loss_scale = 200.0\n",
                "timestep = 3\n",
                "n_samples = 20\n",
                "\n",
                "# Paths to models\n",
                "model_path_1 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/attention/model\"\n",
                "model_path_2 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250612-010926/model\"\n",
                "\n",
                "# Loss function\n",
                "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
                "\n",
                "# Load models\n",
                "model_1 = tf.keras.models.load_model(model_path_1, custom_objects={\"loss_fn\": loss_fn})\n",
                "model_2 = tf.keras.models.load_model(model_path_2, custom_objects={\"loss_fn\": loss_fn})\n",
                "\n",
                "# Load validation dataset (ensure it's already prepared)\n",
                "# Example:\n",
                "# validation_dataset = load_dataset_windowed(...)\n",
                "\n",
                "metrics_list = []\n",
                "\n",
                "for i, (input_data, ground_truth) in enumerate(train_dataset.take(n_samples)):\n",
                "    ground_truth = ground_truth.numpy().squeeze()\n",
                "\n",
                "    pred_1 = model_1(input_data).numpy().squeeze()\n",
                "    pred_2 = model_2(input_data).numpy().squeeze()\n",
                "\n",
                "    gt_t = ground_truth[timestep]\n",
                "    pred_1_t = pred_1[timestep]\n",
                "    pred_2_t = pred_2[timestep]\n",
                "    vmax_val = np.nanpercentile([gt_t, pred_1_t, pred_2_t], 99.5)\n",
                "\n",
                "    mask = ~np.isnan(gt_t)\n",
                "    gt_flat = gt_t[mask].flatten()\n",
                "    pred_1_flat = pred_1_t[mask].flatten()\n",
                "    pred_2_flat = pred_2_t[mask].flatten()\n",
                "\n",
                "    # Compute metrics\n",
                "    metrics_list.append({\n",
                "        \"Sample\": i+1,\n",
                "        \"MAE_1\": mean_absolute_error(gt_flat, pred_1_flat),\n",
                "        \"RMSE_1\": np.sqrt(mean_squared_error(gt_flat, pred_1_flat)),\n",
                "        \"Bias_1\": np.mean(pred_1_flat) - np.mean(gt_flat),\n",
                "        \"IoU_1\": np.logical_and(gt_flat > 0.1, pred_1_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_1_flat > 0.1).sum()),\n",
                "        \"SSIM_1\": ssim(gt_t, pred_1_t, data_range=gt_t.max() - gt_t.min()),\n",
                "\n",
                "        \"MAE_2\": mean_absolute_error(gt_flat, pred_2_flat),\n",
                "        \"RMSE_2\": np.sqrt(mean_squared_error(gt_flat, pred_2_flat)),\n",
                "        \"Bias_2\": np.mean(pred_2_flat) - np.mean(gt_flat),\n",
                "        \"IoU_2\": np.logical_and(gt_flat > 0.1, pred_2_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_2_flat > 0.1).sum()),\n",
                "        \"SSIM_2\": ssim(gt_t, pred_2_t, data_range=gt_t.max() - gt_t.min()),\n",
                "    })\n",
                "\n",
                "    # Plotting\n",
                "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
                "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
                "\n",
                "    axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[0].set_title(\"Ground Truth\")\n",
                "    axes[0].axis(\"off\")\n",
                "\n",
                "    axes[1].imshow(pred_1_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[1].set_title(\"attention\")\n",
                "    axes[1].axis(\"off\")\n",
                "\n",
                "    axes[2].imshow(pred_2_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
                "    axes[2].set_title(\"without attention\")\n",
                "    axes[2].axis(\"off\")\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "# Summary metrics\n",
                "df = pd.DataFrame(metrics_list)\n",
                "print(\"\\n=== Metrics Summary ===\")\n",
                "print(df.describe())\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
