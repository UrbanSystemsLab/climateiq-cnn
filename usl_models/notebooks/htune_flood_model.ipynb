{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Model Training Notebook\n",
    "\n",
    "Train a Flood ConvLSTM Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 17:59:11.744914: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-15 17:59:11.796899: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-15 17:59:11.796950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-15 17:59:11.798448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-15 17:59:11.806469: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from usl_models.flood_ml.dataset import  load_dataset_windowed_cached\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import time\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import logging\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "import pathlib\n",
    "# === CONFIG ===\n",
    "# Set random seeds and GPU memory growth\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"logs/training_{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 14:14:01.567913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38364 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "# :package: Download, :steam_locomotive: Train, and :floppy_disk: Save FloodML model from cached dataset\n",
    "filecache_dir = pathlib.Path(\"/home/shared/climateiq/filecache\")\n",
    "city_config_mapping = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "    # \"Atlanta\": \"Atlanta_config\",\n",
    "    # \"Phoenix_SM\": \"PHX_SM\",\n",
    "    # \"Phoenix_PV\": \"PHX_PV\",\n",
    "    # \"Phoenix_central\": \"PHX_CCC\"\n",
    "    # \"Atlanta_Prediction\": \"Atlanta_config\",\n",
    "    \n",
    "}\n",
    "# Rainfall files you want\n",
    "rainfall_files = [7,5,13,11,9,16,15,10,12,2,3]  # Only 5 and 6\n",
    "# rainfall_files = [5]  # Only 5 and 6\n",
    "dataset_splits = [\"test\", \"train\", \"val\"]\n",
    "n_flood_maps = 5\n",
    "m_rainfall = 6\n",
    "batch_size = 10\n",
    "epochs = 2\n",
    "# Generate sim_names\n",
    "sim_names = []\n",
    "for city, config in city_config_mapping.items():\n",
    "    for rain_id in rainfall_files:\n",
    "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
    "        sim_names.append(sim_name)\n",
    "\n",
    "# === STEP 1: DOWNLOAD DATASET TO FILECACHE ===\n",
    "# print(\"Downloading simulations into local cache\")\n",
    "# download_dataset(\n",
    "#     sim_names=sim_names,\n",
    "#     output_path=filecache_dir,\n",
    "#     dataset_splits=dataset_splits,\n",
    "#    include_labels=True  \n",
    "# )\n",
    "\n",
    "\n",
    "# print(\":white_check_mark: Download complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fatser loading during hyperparameter tuning use this function\n",
    "def get_datasets(batch_size=4):\n",
    "    filecache_dir = pathlib.Path(\"/home/shared/climateiq/filecache\")\n",
    "    city_config_mapping = {\"Manhattan\": \"Manhattan_config\"}\n",
    "    rainfall_files = [7, 5, 13, 11, 9, 16, 15, 10, 12, 2, 3]\n",
    "    n_flood_maps, m_rainfall = 5, 6\n",
    "\n",
    "    sim_names = []\n",
    "    for city, config in city_config_mapping.items():\n",
    "        for rain_id in rainfall_files:\n",
    "            sim_names.append(f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\")\n",
    "\n",
    "    train_ds = load_dataset_windowed_cached(\n",
    "        filecache_dir=filecache_dir,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=\"train\",\n",
    "        batch_size=batch_size,\n",
    "        n_flood_maps=n_flood_maps,\n",
    "        m_rainfall=m_rainfall,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = load_dataset_windowed_cached(\n",
    "        filecache_dir=filecache_dir,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=\"val\",\n",
    "        batch_size=batch_size,\n",
    "        n_flood_maps=n_flood_maps,\n",
    "        m_rainfall=m_rainfall,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    return train_ds.prefetch(8), val_ds.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this step if dataset is already downloaded\n",
    "# # === STEP 2: LOAD CACHED WINDOWED DATASETS ===\n",
    "# print(\"open_file_folder: Loading datasets from cache\")\n",
    "train_dataset = load_dataset_windowed_cached(\n",
    "    filecache_dir=filecache_dir,\n",
    "    sim_names=sim_names,\n",
    "    dataset_split=\"train\",\n",
    "    batch_size=batch_size,\n",
    "    n_flood_maps=n_flood_maps,\n",
    "    m_rainfall=m_rainfall,\n",
    "    shuffle=True\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "# train_dataset = train_dataset.cache(\"/tmp/train_cache\")\n",
    "# dataset = train_dataset.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# train_dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "validation_dataset = load_dataset_windowed_cached(\n",
    "    filecache_dir=filecache_dir,\n",
    "    sim_names=sim_names,\n",
    "    dataset_split=\"val\",\n",
    "    batch_size=batch_size,\n",
    "    n_flood_maps=n_flood_maps,\n",
    "    m_rainfall=m_rainfall,\n",
    "    shuffle=True\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "# validation_dataset = validation_dataset.cache(\"/tmp/val_cache\")\n",
    "# validation_dataset = validation_dataset.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (x, y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = load_dataset_windowed_cached(\n",
    "    filecache_dir=filecache_dir,\n",
    "    sim_names=sim_names,\n",
    "    dataset_split=\"test\",\n",
    "    batch_size=batch_size,\n",
    "    n_flood_maps=n_flood_maps,\n",
    "    m_rainfall=m_rainfall,\n",
    "    shuffle=True\n",
    ").prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning train_dataset...\n",
      "\n",
      "Batch 0:\n",
      "  geospatial: (2, 1000, 1000, 9)\n",
      "  temporal: (2, 5, 6)\n",
      "  spatiotemporal: (2, 5, 1000, 1000, 1)\n",
      "  labels: (2, 1000, 1000)\n",
      "\n",
      "Batch 1:\n",
      "  geospatial: (2, 1000, 1000, 9)\n",
      "  temporal: (2, 5, 6)\n",
      "  spatiotemporal: (2, 5, 1000, 1000, 1)\n",
      "  labels: (2, 1000, 1000)\n",
      "  Processed 100 batches so far...\n",
      "  Processed 200 batches so far...\n",
      "  Processed 300 batches so far...\n",
      "  Processed 400 batches so far...\n",
      "  Processed 500 batches so far...\n",
      "  Processed 600 batches so far...\n",
      "  Processed 700 batches so far...\n",
      "  Processed 800 batches so far...\n",
      "  Processed 900 batches so far...\n",
      "  Processed 1000 batches so far...\n",
      "  Processed 1100 batches so far...\n",
      "  Processed 1200 batches so far...\n",
      "  Processed 1300 batches so far...\n",
      "\n",
      "‚úÖ Dataset scan complete.\n",
      "Total samples (windows): 2748\n",
      "Example tensor shapes: {'geospatial': TensorShape([2, 1000, 1000, 9]), 'temporal': TensorShape([2, 5, 6]), 'spatiotemporal': TensorShape([2, 5, 1000, 1000, 1]), 'labels': TensorShape([2, 1000, 1000])}\n"
     ]
    }
   ],
   "source": [
    "# === Debug dataset structure and counts ===\n",
    "num_samples = 0\n",
    "example_shapes = None\n",
    "unique_chunks = set()\n",
    "\n",
    "print(\"üîç Scanning train_dataset...\")\n",
    "for i, (inputs, labels) in enumerate(train_dataset):\n",
    "    num_samples += inputs[\"geospatial\"].shape[0]\n",
    "    example_shapes = {\n",
    "        \"geospatial\": inputs[\"geospatial\"].shape,\n",
    "        \"temporal\": inputs[\"temporal\"].shape,\n",
    "        \"spatiotemporal\": inputs[\"spatiotemporal\"].shape,\n",
    "        \"labels\": labels.shape,\n",
    "    }\n",
    "    # Optional: show the first few batches\n",
    "    if i < 2:\n",
    "        print(f\"\\nBatch {i}:\")\n",
    "        print(f\"  geospatial: {inputs['geospatial'].shape}\")\n",
    "        print(f\"  temporal: {inputs['temporal'].shape}\")\n",
    "        print(f\"  spatiotemporal: {inputs['spatiotemporal'].shape}\")\n",
    "        print(f\"  labels: {labels.shape}\")\n",
    "    if i % 100 == 0 and i > 0:\n",
    "        print(f\"  Processed {i} batches so far...\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset scan complete.\")\n",
    "print(f\"Total samples (windows): {num_samples}\")\n",
    "print(f\"Example tensor shapes: {example_shapes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[DEBUG] Labels shape before windowing: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in train_dataset.take(1):\n",
    "    print(inputs[\"geospatial\"].shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If working locally, comment out this block\n",
    "\n",
    "\n",
    "# Cities and their config folders\n",
    "city_config_mapping = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "    # \"Atlanta\": \"Atlanta_config\",\n",
    "    # \"Phoenix_SM\": \"PHX_SM\",\n",
    "    # \"Phoenix_PV\": \"PHX_PV\",\n",
    "}\n",
    "\n",
    "# Rainfall files you want\n",
    "rainfall_files = [5]  # Only 5 and 6\n",
    "\n",
    "# Generate sim_names\n",
    "sim_names = []\n",
    "for city, config in city_config_mapping.items():\n",
    "    for rain_id in rainfall_files:\n",
    "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
    "        sim_names.append(sim_name)\n",
    "\n",
    "print(f\"Training on {len(sim_names)} simulations.\")\n",
    "for s in sim_names:\n",
    "    print(s)\n",
    "\n",
    "# Now load dataset\n",
    "train_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=2, dataset_split=\"train\"\n",
    ").cache()\n",
    "\n",
    "validation_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=2, dataset_split=\"val\"\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 04m 58s]\n",
      "val_loss: 0.0009991672122851014\n",
      "\n",
      "Best val_loss So Far: 0.0009991672122851014\n",
      "Total elapsed time: 00h 04m 58s\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).pre_attention.conv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).pre_attention.conv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).attention.conv.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).attention.conv.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.25\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.26\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.27\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.28\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.29\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.30\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.31\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.32\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.33\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.34\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.35\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.36\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.37\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.38\n",
      "Best hyperparameters: {'lstm_units': 64, 'lstm_kernel_size': 3, 'lstm_dropout': 0.3, 'lstm_recurrent_dropout': 0.3, 'n_flood_maps': 5, 'm_rainfall': 6}\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=1,  # increase if you want more search\n",
    "    project_name=log_dir,\n",
    ")\n",
    "\n",
    "tb_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0,\n",
    "    profile_batch=0,\n",
    ")\n",
    "\n",
    "def tuner_search(batch_size=4):\n",
    "    # Clear any leftover sessions\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    train_ds, val_ds = get_datasets(batch_size=batch_size)\n",
    "\n",
    "    tuner.search(\n",
    "        train_ds.take(200),\n",
    "        validation_data=val_ds.take(50),\n",
    "        epochs=10,\n",
    "        callbacks=[],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "tuner_search(batch_size=2)\n",
    "\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "print(\"Best hyperparameters:\", best_hp.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 6\n",
      "lstm_units (Choice)\n",
      "{'default': 32, 'conditions': [], 'values': [32, 64, 128], 'ordered': True}\n",
      "lstm_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
      "lstm_dropout (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3], 'ordered': True}\n",
      "lstm_recurrent_dropout (Choice)\n",
      "{'default': 0.2, 'conditions': [], 'values': [0.2, 0.3], 'ordered': True}\n",
      "n_flood_maps (Choice)\n",
      "{'default': 5, 'conditions': [], 'values': [5], 'ordered': True}\n",
      "m_rainfall (Choice)\n",
      "{'default': 6, 'conditions': [], 'values': [6], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=1,\n",
    "    project_name=f\"logs/htune_project_{timestamp}\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/htune_project_20251015-141132\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "64                |64                |lstm_units\n",
      "3                 |3                 |lstm_kernel_size\n",
      "0.3               |0.3               |lstm_dropout\n",
      "0.3               |0.3               |lstm_recurrent_dropout\n",
      "5                 |5                 |n_flood_maps\n",
      "6                 |6                 |m_rainfall\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 14:13:15.070899: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inflood_conv_lstm/conv_lstm/conv_lstm2d/while/body/_1/flood_conv_lstm/conv_lstm/conv_lstm2d/while/dropout_7/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2025-10-15 14:13:18.007358: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n",
      "2025-10-15 14:13:24.118440: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f3560d58e40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-15 14:13:24.118512: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2025-10-15 14:13:24.126012: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1760537604.247225   20519 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "log_dir = f\"logs/htune_project_{timestamp}\"\n",
    "print(log_dir)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "tuner.search(\n",
    "    train_dataset.take(200),\n",
    "    epochs=10,\n",
    "    validation_data=validation_dataset.take(50),\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 16:50:56.078717: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inflood_conv_lstm_3/conv_lstm/conv_lstm2d_3/while/body/_1/flood_conv_lstm_3/conv_lstm/conv_lstm2d_3/while/dropout_7/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1374/Unknown - 178s 125ms/step - loss: 0.0019 - mean_absolute_error: 0.0137 - root_mean_squared_error: 0.0696"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 16:54:20.263860: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3258415898515464409\n",
      "2025-10-15 16:54:20.263930: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 14801390994140985373\n",
      "2025-10-15 16:54:20.263940: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 4185487910489117445\n",
      "2025-10-15 16:54:20.263952: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10507816670147353428\n",
      "2025-10-15 16:54:20.263973: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 16856470375749547650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/training_20251015-164325/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/training_20251015-164325/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374/1374 [==============================] - 213s 150ms/step - loss: 0.0019 - mean_absolute_error: 0.0137 - root_mean_squared_error: 0.0696 - val_loss: 0.0014 - val_mean_absolute_error: 0.0123 - val_root_mean_squared_error: 0.0582\n",
      "Epoch 2/2\n",
      "1374/1374 [==============================] - ETA: 0s - loss: 0.0012 - mean_absolute_error: 0.0108 - root_mean_squared_error: 0.0516INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/training_20251015-164325/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/training_20251015-164325/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374/1374 [==============================] - 203s 148ms/step - loss: 0.0012 - mean_absolute_error: 0.0108 - root_mean_squared_error: 0.0516 - val_loss: 0.0011 - val_mean_absolute_error: 0.0111 - val_root_mean_squared_error: 0.0504\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cd3e50>, 140089889807856), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914cdedd0>, 140089889808192), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b1eb50>, 140089300006848), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7f6914b39a50>, 140089300007184), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/training_20251015-164325/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/training_20251015-164325/model/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "train_ds, val_ds = get_datasets(batch_size=2)\n",
    "# Define final parameters and model\n",
    "final_params_dict = best_hp.values.copy()\n",
    "final_params = FloodModel.Params(**final_params_dict)\n",
    "model = FloodModel(params=final_params)\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    ModelCheckpoint(\n",
    "        filepath=log_dir + \"/checkpoint\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_format=\"tf\",\n",
    "    ),\n",
    "    EarlyStopping(  # <--- ADD THIS\n",
    "        monitor=\"val_loss\",  # What to monitor\n",
    "        patience=100,  # Number of epochs with no improvement to wait\n",
    "        restore_best_weights=True,  # Restore model weights from best epoch\n",
    "        mode=\"min\",  # \"min\" because lower val_loss is better\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train\n",
    "model.fit(train_ds, val_ds, epochs=2, callbacks=callbacks)\n",
    "\n",
    "# Save final model\n",
    "model.save_model(log_dir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1000, 1000, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Test calling the model on some data.\n",
    "inputs, labels_ = next(iter(train_dataset))\n",
    "prediction = model.call(inputs)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Prediction mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction mode\n",
    "from usl_models.flood_ml import dataset\n",
    "# Parameters\n",
    "filecache_dir = pathlib.Path(\"/home/shared/climateiq/filecache\")\n",
    "# prediction\n",
    "sim_name = [\"Atlanta_Prediction\"]\n",
    "rainfall_sim = \"Atlanta-Atlanta_config/Rainfall_Data_22.txt\"\n",
    "\n",
    "\n",
    "\n",
    "# Download (prediction mode)\n",
    "# dataset.download_dataset(\n",
    "#     sim_names=sim_name,          # study area\n",
    "#     output_path=filecache_dir,\n",
    "#     include_labels=False,                      # no labels\n",
    "#     rainfall_sim_name=rainfall_sim,  # simulation for temporal vector\n",
    "#     allow_missing_sim=True                     # skip temporal if missing\n",
    "# )\n",
    "# prediction mode\n",
    "# # # Load dataset\n",
    "full_dataset = dataset.load_dataset_cached(\n",
    "    filecache_dir=filecache_dir,\n",
    "    sim_names=sim_name,            # study area\n",
    "    dataset_split=None,                          # no split for prediction\n",
    "    batch_size=2,\n",
    "    include_labels=False,\n",
    "    rainfall_sim_name=rainfall_sim  # actual rainfall sim\n",
    ")\n",
    "\n",
    "\n",
    "# Download (training mode)\n",
    "# dataset_splits = [\"test\", \"train\", \"val\"]\n",
    "# dataset.download_dataset(\n",
    "#     sim_names=[\"Atlanta-Atlanta_config/Rainfall_Data_22.txt\"],  # normal simulations\n",
    "#     output_path=filecache_dir,\n",
    "#     dataset_splits=dataset_splits,               # train/val/test splits\n",
    "#     include_labels=True                        # get labels too\n",
    "# )\n",
    "\n",
    "# full_dataset = dataset.load_dataset_cached(\n",
    "#     filecache_dir=filecache_dir,\n",
    "#     sim_names=[\"Atlanta-Atlanta_config/Rainfall_Data_20.txt\"],\n",
    "#     dataset_split=\"train\",\n",
    "#     include_labels=True\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"flood_conv_lstm_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " spatiotemporal_cnn (Sequen  (None, None, 250, 250,    3424      \n",
      " tial)                       16)                                 \n",
      "                                                                 \n",
      " geospatial_cnn (Sequential  (None, 250, 250, 64)      29280     \n",
      " )                                                               \n",
      "                                                                 \n",
      " spatial_attention_8 (Spati  multiple                  99        \n",
      " alAttention)                                                    \n",
      "                                                                 \n",
      " conv_lstm (Sequential)      (None, 250, 250, 64)      345856    \n",
      "                                                                 \n",
      " spatial_attention_9 (Spati  multiple                  99        \n",
      " alAttention)                                                    \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 1000, 1000, 1)     8209      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 386967 (1.48 MB)\n",
      "Trainable params: 386967 (1.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 4, 1000, 1000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
    "# Path to your saved model\n",
    "N_steps = 4\n",
    "model_path = \"/home/se2890/climateiq-cnn-9/logs/training_20251015-164325/model\"\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "loaded_model.summary()\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "# model = FloodModel.from_checkpoint(model_path)\n",
    "\n",
    "from usl_models.flood_ml.model import SpatialAttention\n",
    "custom_objects = {'SpatialAttention': SpatialAttention}\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects=custom_objects,\n",
    "    compile=False\n",
    ")\n",
    "model.set_weights(loaded_model.get_weights())\n",
    "\n",
    "# # Test calling the model for n predictions\n",
    "# full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
    "inputs, labels, _ = next(iter(full_dataset))\n",
    "predictions = model.call_n(inputs, n=N_steps)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_scale = best_hp.get(\"loss_scale\")\n",
    "# print(\"Loss scale used during training:\", loss_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml import customloss\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 200.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/se2890/climateiq-cnn-5/logs/htune_project_20250801-155126/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "# Number of samples to visualize\n",
    "n_samples = 20\n",
    "\n",
    "# Loop through the dataset and predict\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    print(f\"\\nSample {i+1} Prediction Stats:\")\n",
    "    print(\"  Min:\", prediction.min())\n",
    "    print(\"  Max:\", prediction.max())\n",
    "    print(\"  Mean:\", prediction.mean())\n",
    "\n",
    "    # Choose timestep to plot\n",
    "    timestep = 3\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = max(gt_t.max(), pred_t.max())\n",
    "\n",
    "    # Plot Ground Truth and Prediction\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 150.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "# Assuming validation_dataset is already defined\n",
    "# Example:\n",
    "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "n_samples = 20\n",
    "timestep = 2\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
    "\n",
    "    # Mask out NaNs\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_flat = pred_t[mask].flatten()\n",
    "\n",
    "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
    "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
    "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum())\n",
    "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
    "\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Bias\": bias,\n",
    "        \"IoU > 0.1\": iou,\n",
    "        \"SSIM\": ssim_val\n",
    "    })\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml import customloss\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "loss_scale = 200.0\n",
    "timestep = 3\n",
    "n_samples = 20\n",
    "\n",
    "# Paths to models\n",
    "model_path_1 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/attention/model\"\n",
    "model_path_2 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250612-010926/model\"\n",
    "\n",
    "# Loss function\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load models\n",
    "model_1 = tf.keras.models.load_model(model_path_1, custom_objects={\"loss_fn\": loss_fn})\n",
    "model_2 = tf.keras.models.load_model(model_path_2, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "# Load validation dataset (ensure it's already prepared)\n",
    "# Example:\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(train_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "\n",
    "    pred_1 = model_1(input_data).numpy().squeeze()\n",
    "    pred_2 = model_2(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_1_t = pred_1[timestep]\n",
    "    pred_2_t = pred_2[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_1_t, pred_2_t], 99.5)\n",
    "\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_1_flat = pred_1_t[mask].flatten()\n",
    "    pred_2_flat = pred_2_t[mask].flatten()\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE_1\": mean_absolute_error(gt_flat, pred_1_flat),\n",
    "        \"RMSE_1\": np.sqrt(mean_squared_error(gt_flat, pred_1_flat)),\n",
    "        \"Bias_1\": np.mean(pred_1_flat) - np.mean(gt_flat),\n",
    "        \"IoU_1\": np.logical_and(gt_flat > 0.1, pred_1_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_1_flat > 0.1).sum()),\n",
    "        \"SSIM_1\": ssim(gt_t, pred_1_t, data_range=gt_t.max() - gt_t.min()),\n",
    "\n",
    "        \"MAE_2\": mean_absolute_error(gt_flat, pred_2_flat),\n",
    "        \"RMSE_2\": np.sqrt(mean_squared_error(gt_flat, pred_2_flat)),\n",
    "        \"Bias_2\": np.mean(pred_2_flat) - np.mean(gt_flat),\n",
    "        \"IoU_2\": np.logical_and(gt_flat > 0.1, pred_2_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_2_flat > 0.1).sum()),\n",
    "        \"SSIM_2\": ssim(gt_t, pred_2_t, data_range=gt_t.max() - gt_t.min()),\n",
    "    })\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pred_1_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"attention\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(pred_2_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[2].set_title(\"without attention\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary metrics\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
