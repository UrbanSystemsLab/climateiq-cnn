{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Model Training Notebook\n",
    "\n",
    "Train a Flood ConvLSTM Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2 simulations.\n",
      "Manhattan-Manhattan_config/Rainfall_Data_5.txt\n",
      "Atlanta-Atlanta_config/Rainfall_Data_5.txt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import time\n",
    "import keras\n",
    "import logging\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.model_params import FloodModelParams\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed, load_dataset\n",
    "from usl_models.flood_ml import customloss, dataset\n",
    "\n",
    "# Setup\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Cities and their config folders\n",
    "city_config_mapping = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "    \"Atlanta\": \"Atlanta_config\",\n",
    "    # \"Phoenix_SM\": \"PHX_SM\",\n",
    "    # \"Phoenix_PV\": \"PHX_PV\",\n",
    "}\n",
    "\n",
    "# Rainfall files you want\n",
    "rainfall_files = [5]  # Only 5 and 6\n",
    "\n",
    "# Generate sim_names\n",
    "sim_names = []\n",
    "for city, config in city_config_mapping.items():\n",
    "    for rain_id in rainfall_files:\n",
    "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
    "        sim_names.append(sim_name)\n",
    "\n",
    "print(f\"Training on {len(sim_names)} simulations.\")\n",
    "for s in sim_names:\n",
    "    print(s)\n",
    "\n",
    "# Now load dataset\n",
    "ds_config = dataset.Config(\n",
    "    input_height=5, input_width=5,\n",
    "    output_height=5, output_width=5\n",
    ")\n",
    "\n",
    "train_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"train\", ds_config=ds_config\n",
    ").cache()\n",
    "\n",
    "validation_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 01m 33s]\n",
      "val_loss: 0.006333703175187111\n",
      "\n",
      "Best val_loss So Far: 0.006333703175187111\n",
      "Total elapsed time: 00h 01m 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lstm_units': 64,\n",
       " 'lstm_kernel_size': 3,\n",
       " 'lstm_dropout': 0.3,\n",
       " 'lstm_recurrent_dropout': 0.3,\n",
       " 'n_flood_maps': 5,\n",
       " 'm_rainfall': 6,\n",
       " 'loss_scale': 200.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir = f\"logs/htune_project_{timestamp}\"\n",
    "print(log_dir)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "        loss_scale=[50.0, 100.0, 200.0],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=1,\n",
    "    project_name=log_dir,\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "        loss_scale=[50.0, 100.0, 200.0],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=10,\n",
    "    project_name=f\"logs/htune_project_{timestamp}\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/htune_project_{timestamp}\"\n",
    "print(log_dir)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 21:59:24.669143: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inflood_conv_lstm_1/conv_lstm/conv_lstm2d_1/while/body/_1/flood_conv_lstm_1/conv_lstm/conv_lstm2d_1/while/dropout_7/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     91/Unknown - 8s 25ms/step - loss: 0.0059 - mean_absolute_error: 0.0284 - root_mean_squared_error: 0.1124"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 21:59:28.858249: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1054651404552106284\n",
      "2025-06-23 21:59:29.582333: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 8832367692934736833\n",
      "2025-06-23 21:59:29.582373: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 17072165894091048457\n",
      "2025-06-23 21:59:29.582399: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 13720671101245028242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250623-212821/checkpoint/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250623-212821/checkpoint/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 13s 81ms/step - loss: 0.0059 - mean_absolute_error: 0.0284 - root_mean_squared_error: 0.1124 - val_loss: 0.0063 - val_mean_absolute_error: 0.0244 - val_root_mean_squared_error: 0.1340\n",
      "Epoch 2/4\n",
      "91/91 [==============================] - 2s 27ms/step - loss: 0.0059 - mean_absolute_error: 0.0284 - root_mean_squared_error: 0.1124 - val_loss: 0.0063 - val_mean_absolute_error: 0.0244 - val_root_mean_squared_error: 0.1340\n",
      "Epoch 3/4\n",
      "91/91 [==============================] - 2s 27ms/step - loss: 0.0059 - mean_absolute_error: 0.0284 - root_mean_squared_error: 0.1124 - val_loss: 0.0063 - val_mean_absolute_error: 0.0244 - val_root_mean_squared_error: 0.1340\n",
      "Epoch 4/4\n",
      "91/91 [==============================] - 2s 27ms/step - loss: 0.0059 - mean_absolute_error: 0.0284 - root_mean_squared_error: 0.1124 - val_loss: 0.0063 - val_mean_absolute_error: 0.0244 - val_root_mean_squared_error: 0.1340\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6066d0d0>, 140377254388080), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab6067bfd0>, 140377254385392), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(7, 7, 2, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606c8350>, 140374968462208), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7fab606cf3d0>, 140374968462544), {}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250623-212821/model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: logs/htune_project_20250623-212821/model/assets\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define final parameters and model\n",
    "final_params_dict = best_hp.values.copy()\n",
    "loss_scale = final_params_dict.pop(\"loss_scale\", 100.0)\n",
    "final_params = FloodModel.Params(**final_params_dict)\n",
    "model = FloodModel(params=final_params, loss_scale=loss_scale)\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    ModelCheckpoint(\n",
    "        filepath=log_dir + \"/checkpoint\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_format=\"tf\",\n",
    "    ),\n",
    "    EarlyStopping(  # <--- ADD THIS\n",
    "        monitor=\"val_loss\",  # What to monitor\n",
    "        patience=100,  # Number of epochs with no improvement to wait\n",
    "        restore_best_weights=True,  # Restore model weights from best epoch\n",
    "        mode=\"min\",  # \"min\" because lower val_loss is better\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train\n",
    "model.fit(train_dataset, validation_dataset, epochs=4, callbacks=callbacks)\n",
    "\n",
    "model.save_model(log_dir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test calling the model on some data.\n",
    "inputs, labels_ = next(iter(train_dataset))\n",
    "prediction = model.call(inputs)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test calling the model for n predictions\n",
    "# full_dataset = load_dataset(sim_names=sim_names, batch_size=1)\n",
    "# inputs, labels = next(iter(full_dataset))\n",
    "# predictions = model.call_n(inputs, n=4)\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_scale = best_hp.get(\"loss_scale\")\n",
    "print(\"Loss scale used during training:\", loss_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 22:00:51.720840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * {'geospatial': <tf.Tensor 'input:0' shape=(4, 5, 5, 9) dtype=float32>,\n 'spatiotemporal': <tf.Tensor 'input_2:0' shape=(4, 5, 5, 5, 1) dtype=float32>,\n 'temporal': <tf.Tensor 'input_1:0' shape=(4, 5, 6) dtype=float32>}\n  Keyword arguments: {'training': False}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * {'geospatial': TensorSpec(shape=(None, 10, 10, 9), dtype=tf.float32, name='geospatial'),\n 'spatiotemporal': TensorSpec(shape=(None, 5, 10, 10, 1), dtype=tf.float32, name='spatiotemporal'),\n 'temporal': TensorSpec(shape=(None, 5, 6), dtype=tf.float32, name='temporal')}\n  Keyword arguments: {'training': True}\n\nOption 2:\n  Positional arguments (1 total):\n    * {'geospatial': TensorSpec(shape=(None, 10, 10, 9), dtype=tf.float32, name='geospatial'),\n 'spatiotemporal': TensorSpec(shape=(None, 5, 10, 10, 1), dtype=tf.float32, name='spatiotemporal'),\n 'temporal': TensorSpec(shape=(None, 5, 6), dtype=tf.float32, name='temporal')}\n  Keyword arguments: {'training': False}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_data, ground_truth) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(validation_dataset\u001b[38;5;241m.\u001b[39mtake(n_samples)):\n\u001b[1;32m     28\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m---> 29\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Prediction Stats:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Min:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction\u001b[38;5;241m.\u001b[39mmin())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/saved_model/function_deserialization.py:335\u001b[0m, in \u001b[0;36mrecreate_function.<locals>.restored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m   positional, keyword \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mstructured_input_signature\n\u001b[1;32m    332\u001b[0m   signature_descriptions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    333\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOption \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword arguments: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    334\u001b[0m           index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, _pretty_format_positional(positional), keyword))\n\u001b[0;32m--> 335\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find matching concrete function to call loaded from the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSavedModel. Got:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_pretty_format_positional(args)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Keyword \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Expected these arguments to match one of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollowing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(saved_function\u001b[38;5;241m.\u001b[39mconcrete_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m option(s):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m10\u001b[39m))\u001b[38;5;241m.\u001b[39mjoin(signature_descriptions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find matching concrete function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * {'geospatial': <tf.Tensor 'input:0' shape=(4, 5, 5, 9) dtype=float32>,\n 'spatiotemporal': <tf.Tensor 'input_2:0' shape=(4, 5, 5, 5, 1) dtype=float32>,\n 'temporal': <tf.Tensor 'input_1:0' shape=(4, 5, 6) dtype=float32>}\n  Keyword arguments: {'training': False}\n\n Expected these arguments to match one of the following 2 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * {'geospatial': TensorSpec(shape=(None, 10, 10, 9), dtype=tf.float32, name='geospatial'),\n 'spatiotemporal': TensorSpec(shape=(None, 5, 10, 10, 1), dtype=tf.float32, name='spatiotemporal'),\n 'temporal': TensorSpec(shape=(None, 5, 6), dtype=tf.float32, name='temporal')}\n  Keyword arguments: {'training': True}\n\nOption 2:\n  Positional arguments (1 total):\n    * {'geospatial': TensorSpec(shape=(None, 10, 10, 9), dtype=tf.float32, name='geospatial'),\n 'spatiotemporal': TensorSpec(shape=(None, 5, 10, 10, 1), dtype=tf.float32, name='spatiotemporal'),\n 'temporal': TensorSpec(shape=(None, 5, 6), dtype=tf.float32, name='temporal')}\n  Keyword arguments: {'training': False}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 200.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/elhajjas/climateiq-cnn-11/logs/htune_project_20250623-212821/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "\n",
    "# Number of samples to visualize\n",
    "n_samples = 20\n",
    "\n",
    "# Loop through the dataset and predict\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    print(f\"\\nSample {i+1} Prediction Stats:\")\n",
    "    print(\"  Min:\", prediction.min())\n",
    "    print(\"  Max:\", prediction.max())\n",
    "    print(\"  Mean:\", prediction.mean())\n",
    "\n",
    "    # Choose timestep to plot\n",
    "    timestep = 3\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = max(gt_t.max(), pred_t.max())\n",
    "\n",
    "    # Plot Ground Truth and Prediction\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 150.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "# Assuming validation_dataset is already defined\n",
    "# Example:\n",
    "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "n_samples = 20\n",
    "timestep = 2\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
    "\n",
    "    # Mask out NaNs\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_flat = pred_t[mask].flatten()\n",
    "\n",
    "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
    "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
    "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum())\n",
    "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
    "\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Bias\": bias,\n",
    "        \"IoU > 0.1\": iou,\n",
    "        \"SSIM\": ssim_val\n",
    "    })\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml import customloss\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "loss_scale = 200.0\n",
    "timestep = 3\n",
    "n_samples = 20\n",
    "\n",
    "# Paths to models\n",
    "model_path_1 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/attention/model\"\n",
    "model_path_2 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250612-010926/model\"\n",
    "\n",
    "# Loss function\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load models\n",
    "model_1 = tf.keras.models.load_model(model_path_1, custom_objects={\"loss_fn\": loss_fn})\n",
    "model_2 = tf.keras.models.load_model(model_path_2, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "# Load validation dataset (ensure it's already prepared)\n",
    "# Example:\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(train_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "\n",
    "    pred_1 = model_1(input_data).numpy().squeeze()\n",
    "    pred_2 = model_2(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_1_t = pred_1[timestep]\n",
    "    pred_2_t = pred_2[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_1_t, pred_2_t], 99.5)\n",
    "\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_1_flat = pred_1_t[mask].flatten()\n",
    "    pred_2_flat = pred_2_t[mask].flatten()\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_list.append({\n",
    "        \"Sample\": i+1,\n",
    "        \"MAE_1\": mean_absolute_error(gt_flat, pred_1_flat),\n",
    "        \"RMSE_1\": np.sqrt(mean_squared_error(gt_flat, pred_1_flat)),\n",
    "        \"Bias_1\": np.mean(pred_1_flat) - np.mean(gt_flat),\n",
    "        \"IoU_1\": np.logical_and(gt_flat > 0.1, pred_1_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_1_flat > 0.1).sum()),\n",
    "        \"SSIM_1\": ssim(gt_t, pred_1_t, data_range=gt_t.max() - gt_t.min()),\n",
    "\n",
    "        \"MAE_2\": mean_absolute_error(gt_flat, pred_2_flat),\n",
    "        \"RMSE_2\": np.sqrt(mean_squared_error(gt_flat, pred_2_flat)),\n",
    "        \"Bias_2\": np.mean(pred_2_flat) - np.mean(gt_flat),\n",
    "        \"IoU_2\": np.logical_and(gt_flat > 0.1, pred_2_flat > 0.1).sum() / max(1, np.logical_or(gt_flat > 0.1, pred_2_flat > 0.1).sum()),\n",
    "        \"SSIM_2\": ssim(gt_t, pred_2_t, data_range=gt_t.max() - gt_t.min()),\n",
    "    })\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pred_1_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"attention\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(pred_2_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[2].set_title(\"without attention\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary metrics\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
