{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Model Training Notebook\n",
    "\n",
    "Train a Flood ConvLSTM Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:08:10.464694: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-09 17:08:10.515807: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-09 17:08:10.515838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-09 17:08:10.517134: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-09 17:08:10.525683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 2 simulations.\n",
      "Manhattan-Manhattan_config/Rainfall_Data_5.txt\n",
      "Atlanta-Atlanta_config/Rainfall_Data_5.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:08:15.583242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38364 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "import time\n",
    "import keras\n",
    "import logging\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.model_params import FloodModelParams\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed, load_dataset\n",
    "from usl_models.flood_ml import customloss, dataset\n",
    "\n",
    "# Setup\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Cities and their config folders\n",
    "city_config_mapping = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "    \"Atlanta\": \"Atlanta_config\",\n",
    "    # \"Phoenix_SM\": \"PHX_SM\",\n",
    "    # \"Phoenix_PV\": \"PHX_PV\",\n",
    "}\n",
    "\n",
    "# Rainfall files you want\n",
    "rainfall_files = [5]  # Only 5 and 6\n",
    "\n",
    "# Generate sim_names\n",
    "sim_names = []\n",
    "for city, config in city_config_mapping.items():\n",
    "    for rain_id in rainfall_files:\n",
    "        sim_name = f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\"\n",
    "        sim_names.append(sim_name)\n",
    "\n",
    "print(f\"Training on {len(sim_names)} simulations.\")\n",
    "for s in sim_names:\n",
    "    print(s)\n",
    "\n",
    "# Now load dataset\n",
    "ds_config = dataset.Config(\n",
    "    input_height=10, input_width=10, output_height=10, output_width=10\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"train\", ds_config=ds_config\n",
    ").cache()\n",
    "\n",
    "validation_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"logs/htune_project_{timestamp}\"\n",
    "print(log_dir)\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    FloodModel.get_hypermodel(\n",
    "        lstm_units=[32, 64, 128],\n",
    "        lstm_kernel_size=[3, 5],\n",
    "        lstm_dropout=[0.2, 0.3],\n",
    "        lstm_recurrent_dropout=[0.2, 0.3],\n",
    "        n_flood_maps=[5],\n",
    "        m_rainfall=[6],\n",
    "        loss_scale=[50.0, 100.0, 200.0],\n",
    "    ),\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=1,\n",
    "    project_name=log_dir,\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    train_dataset,\n",
    "    epochs=2,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[tb_callback],\n",
    ")\n",
    "\n",
    "best_model, best_hp = tuner.get_best_models()[0], tuner.get_best_hyperparameters()[0]\n",
    "best_hp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final parameters and model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define final parameters and model\n",
    "final_params_dict = best_hp.values.copy()\n",
    "loss_scale = final_params_dict.pop(\"loss_scale\", 100.0)\n",
    "final_params = FloodModel.Params(**final_params_dict)\n",
    "model = FloodModel(params=final_params, loss_scale=loss_scale)\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    ModelCheckpoint(\n",
    "        filepath=log_dir + \"/checkpoint\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_format=\"tf\",\n",
    "    ),\n",
    "    EarlyStopping(  # <--- ADD THIS\n",
    "        monitor=\"val_loss\",  # What to monitor\n",
    "        patience=100,  # Number of epochs with no improvement to wait\n",
    "        restore_best_weights=True,  # Restore model weights from best epoch\n",
    "        mode=\"min\",  # \"min\" because lower val_loss is better\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Train\n",
    "model.fit(train_dataset, validation_dataset, epochs=2, callbacks=callbacks)\n",
    "\n",
    "model.save_model(log_dir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and make predictions\n",
    "# Load the validation dataset with the correct configuration\n",
    "ds_configa = dataset.Config(\n",
    "    input_height=1000, input_width=1000, output_height=1000, output_width=1000\n",
    ")\n",
    "validation_dataset = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=4, dataset_split=\"val\", ds_config=ds_configa\n",
    ").cache()\n",
    "\n",
    "loss_scale = 200.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = log_dir + \"/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "inputs, labels_ = next(iter(validation_dataset))\n",
    "prediction = model.call(inputs)\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:08:22.436584: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> EXAMPLE VALIDATION DATASET SHAPES\n",
      "geospatial: (1, 1000, 1000, 9)\n",
      "temporal: (1, 5, 6)\n",
      "spatiotemporal: (1, 5, 1000, 1000, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:08:24.200758: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> STITCHED INPUT SHAPES\n",
      "geospatial: (1, 1300, 1300, 9)\n",
      "spatiotemporal: (1, 5, 1300, 1300, 1)\n",
      "temporal: (1, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "# This code is used to stitch together chunks of geospatial and spatiotemporal data\n",
    "# into a full-sized input for the FloodConvLSTM model.\n",
    "# It assumes that the validation dataset is already loaded and consists of chunks of data.\n",
    "# It stitches the chunks together to create a full input tensor for the model.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from usl_models.flood_ml.dataset import _generate_temporal_tensor\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.model import FloodConvLSTM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from usl_models.flood_ml import dataset, customloss\n",
    "from usl_models.flood_ml.model import FloodConvLSTM\n",
    "# Constants\n",
    "chunks_per_row = 1\n",
    "chunks_per_col = 1\n",
    "chunk_h, chunk_w = 1000, 1000\n",
    "n_flood_maps = 5\n",
    "geo_features = 9\n",
    "m_rainfall = 6\n",
    "batch_size = 1\n",
    "stitched_temp_full = None  # will be (T_max, 6)\n",
    "\n",
    "# === Configuration ===\n",
    "chunk_h, chunk_w = 1000, 1000\n",
    "# === Load validation dataset ===\n",
    "ds_config = dataset.Config(\n",
    "    input_height=chunk_h,\n",
    "    input_width=chunk_w,\n",
    "    output_height=chunk_h,\n",
    "    output_width=chunk_w,\n",
    ")\n",
    "validation_dataset = dataset.load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=1, dataset_split=\"val\", ds_config=ds_config\n",
    ").cache()\n",
    "# === Collect one sample to print its shape ===\n",
    "sample_input, _ = next(iter(validation_dataset))\n",
    "print(\"===> EXAMPLE VALIDATION DATASET SHAPES\")\n",
    "for k, v in sample_input.items():\n",
    "    print(f\"{k}: {v.shape}\")\n",
    "# === Initialize stitched arrays\n",
    "stitched_geo = np.zeros((chunks_per_col * chunk_h + 300 , chunks_per_row * chunk_w + 300, geo_features), dtype=np.float32)\n",
    "stitched_st = np.zeros((n_flood_maps, chunks_per_col * chunk_h + 300, chunks_per_row * chunk_w + 300, 1), dtype=np.float32)\n",
    "# Fill in stitched inputs from chunks\n",
    "row_idx = 0\n",
    "col_idx = 0\n",
    "for inputs, _ in validation_dataset:\n",
    "    geo = inputs[\"geospatial\"].numpy().squeeze(0)        # shape (H, W, F)\n",
    "    st = inputs[\"spatiotemporal\"].numpy().squeeze(0)     # shape (N, H, W, 1)\n",
    "    r = row_idx * chunk_h\n",
    "    c = col_idx * chunk_w\n",
    "    temp = inputs[\"temporal\"].numpy()[0]  \n",
    "    if stitched_temp_full is None:\n",
    "        stitched_temp_full = temp  # (T_max, 6)\n",
    "    stitched_geo[r:r+chunk_h, c:c+chunk_w, :] = geo\n",
    "    stitched_st[:, r:r+chunk_h, c:c+chunk_w, :] = st\n",
    "    col_idx += 1\n",
    "    if col_idx == chunks_per_row:\n",
    "        col_idx = 0\n",
    "        row_idx += 1\n",
    "    if row_idx == chunks_per_col:\n",
    "        break\n",
    "# === Get full-length temporal input\n",
    "from usl_models.flood_ml import metastore\n",
    "from google.cloud import firestore, storage\n",
    "firestore_client = firestore.Client()\n",
    "storage_client = storage.Client()\n",
    "temporal, _ = _generate_temporal_tensor(\n",
    "    metastore.get_temporal_feature_metadata(firestore_client, sim_names[0]),\n",
    "    storage_client,\n",
    "    sim_names[0],\n",
    "    m_rainfall,\n",
    ")\n",
    "temporal = tf.convert_to_tensor(temporal[tf.newaxis, ...])  # shape (1, 864, 6)\n",
    "# === Final temporal window ===\n",
    "t = 4  # Must be >= n_flood_maps - 1\n",
    "\n",
    "temporal_window = FloodConvLSTM._get_temporal_window(\n",
    "    tf.convert_to_tensor(stitched_temp_full[None, ...]),  # shape (1, T_max, M)\n",
    "    t=t,\n",
    "    n=n_flood_maps,\n",
    ")  # â†’ shape (1, 5, 6)\n",
    "# === Prepare model input\n",
    "input_dict = {\n",
    "    \"geospatial\": tf.convert_to_tensor(stitched_geo[None, ...]),        # (1, H, W, F)\n",
    "    \"spatiotemporal\": tf.convert_to_tensor(stitched_st[None, ...]),     # (1, N, H, W, 1)\n",
    "    \"temporal\": temporal_window,                                        # (1, 5, 6)\n",
    "}\n",
    "print(\"\\n===> STITCHED INPUT SHAPES\")\n",
    "for k, v in input_dict.items():\n",
    "    print(f\"{k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 17:08:30.858396: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction shape: (1, 1300, 1300, 1)\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/se2890/climateiq-cnn-2/logs/htune_project_20250708-184915/model\"\n",
    "loss_fn = customloss.make_hybrid_loss(scale=200.0)\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "# === Run model prediction\n",
    "prediction = model.call(input_dict)\n",
    "print(\"\\nPrediction shape:\", prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test calling the model for n predictions\n",
    "# full_dataset = load_dataset(sim_names=sim_names, batch_size=1)\n",
    "# inputs, labels = next(iter(full_dataset))\n",
    "# predictions = model.call_n(inputs, n=4)\n",
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 200.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = log_dir + \"/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "# Number of samples to visualize\n",
    "n_samples = 20\n",
    "\n",
    "# Loop through the dataset and predict\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    print(f\"\\nSample {i+1} Prediction Stats:\")\n",
    "    print(\"  Min:\", prediction.min())\n",
    "    print(\"  Max:\", prediction.max())\n",
    "    print(\"  Mean:\", prediction.mean())\n",
    "\n",
    "    # Choose timestep to plot\n",
    "    timestep = 1\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = max(gt_t.max(), pred_t.max())\n",
    "\n",
    "    # Plot Ground Truth and Prediction\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Path to trained model\n",
    "# Known value used during training\n",
    "loss_scale = 150.0\n",
    "\n",
    "# Path to trained model\n",
    "model_path = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250611-205219/model\"\n",
    "\n",
    "# Create the loss function with the correct scale\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load model with custom loss function\n",
    "model = tf.keras.models.load_model(model_path, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "\n",
    "# Assuming validation_dataset is already defined\n",
    "# Example:\n",
    "# from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "n_samples = 20\n",
    "timestep = 2\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(validation_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "    prediction = model(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_t = prediction[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_t], 99.5)\n",
    "\n",
    "    # Mask out NaNs\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_flat = pred_t[mask].flatten()\n",
    "\n",
    "    mae = mean_absolute_error(gt_flat, pred_flat)\n",
    "    rmse = np.sqrt(mean_squared_error(gt_flat, pred_flat))\n",
    "    bias = np.mean(pred_flat) - np.mean(gt_flat)\n",
    "    iou = np.logical_and(gt_flat > 0.1, pred_flat > 0.1).sum() / max(\n",
    "        1, np.logical_or(gt_flat > 0.1, pred_flat > 0.1).sum()\n",
    "    )\n",
    "    ssim_val = ssim(gt_t, pred_t, data_range=gt_t.max() - gt_t.min())\n",
    "\n",
    "    metrics_list.append(\n",
    "        {\n",
    "            \"Sample\": i + 1,\n",
    "            \"MAE\": mae,\n",
    "            \"RMSE\": rmse,\n",
    "            \"Bias\": bias,\n",
    "            \"IoU > 0.1\": iou,\n",
    "            \"SSIM\": ssim_val,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    im1 = axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "    plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "    im2 = axes[1].imshow(pred_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"Prediction\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im2, ax=axes[1], shrink=0.8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml import customloss\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "loss_scale = 200.0\n",
    "timestep = 3\n",
    "n_samples = 20\n",
    "\n",
    "# Paths to models\n",
    "model_path_1 = (\n",
    "    \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/attention/model\"\n",
    ")\n",
    "model_path_2 = \"/home/elhajjas/climateiq-cnn-11/usl_models/notebooks/logs/htune_project_20250612-010926/model\"\n",
    "\n",
    "# Loss function\n",
    "loss_fn = customloss.make_hybrid_loss(scale=loss_scale)\n",
    "\n",
    "# Load models\n",
    "model_1 = tf.keras.models.load_model(model_path_1, custom_objects={\"loss_fn\": loss_fn})\n",
    "model_2 = tf.keras.models.load_model(model_path_2, custom_objects={\"loss_fn\": loss_fn})\n",
    "\n",
    "# Load validation dataset (ensure it's already prepared)\n",
    "# Example:\n",
    "# validation_dataset = load_dataset_windowed(...)\n",
    "\n",
    "metrics_list = []\n",
    "\n",
    "for i, (input_data, ground_truth) in enumerate(train_dataset.take(n_samples)):\n",
    "    ground_truth = ground_truth.numpy().squeeze()\n",
    "\n",
    "    pred_1 = model_1(input_data).numpy().squeeze()\n",
    "    pred_2 = model_2(input_data).numpy().squeeze()\n",
    "\n",
    "    gt_t = ground_truth[timestep]\n",
    "    pred_1_t = pred_1[timestep]\n",
    "    pred_2_t = pred_2[timestep]\n",
    "    vmax_val = np.nanpercentile([gt_t, pred_1_t, pred_2_t], 99.5)\n",
    "\n",
    "    mask = ~np.isnan(gt_t)\n",
    "    gt_flat = gt_t[mask].flatten()\n",
    "    pred_1_flat = pred_1_t[mask].flatten()\n",
    "    pred_2_flat = pred_2_t[mask].flatten()\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_list.append(\n",
    "        {\n",
    "            \"Sample\": i + 1,\n",
    "            \"MAE_1\": mean_absolute_error(gt_flat, pred_1_flat),\n",
    "            \"RMSE_1\": np.sqrt(mean_squared_error(gt_flat, pred_1_flat)),\n",
    "            \"Bias_1\": np.mean(pred_1_flat) - np.mean(gt_flat),\n",
    "            \"IoU_1\": np.logical_and(gt_flat > 0.1, pred_1_flat > 0.1).sum()\n",
    "            / max(1, np.logical_or(gt_flat > 0.1, pred_1_flat > 0.1).sum()),\n",
    "            \"SSIM_1\": ssim(gt_t, pred_1_t, data_range=gt_t.max() - gt_t.min()),\n",
    "            \"MAE_2\": mean_absolute_error(gt_flat, pred_2_flat),\n",
    "            \"RMSE_2\": np.sqrt(mean_squared_error(gt_flat, pred_2_flat)),\n",
    "            \"Bias_2\": np.mean(pred_2_flat) - np.mean(gt_flat),\n",
    "            \"IoU_2\": np.logical_and(gt_flat > 0.1, pred_2_flat > 0.1).sum()\n",
    "            / max(1, np.logical_or(gt_flat > 0.1, pred_2_flat > 0.1).sum()),\n",
    "            \"SSIM_2\": ssim(gt_t, pred_2_t, data_range=gt_t.max() - gt_t.min()),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "    fig.suptitle(f\"Sample {i+1} - Timestep {timestep}\", fontsize=16)\n",
    "\n",
    "    axes[0].imshow(gt_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(pred_1_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[1].set_title(\"attention\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(pred_2_t, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "    axes[2].set_title(\"without attention\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Summary metrics\n",
    "df = pd.DataFrame(metrics_list)\n",
    "print(\"\\n=== Metrics Summary ===\")\n",
    "print(df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
