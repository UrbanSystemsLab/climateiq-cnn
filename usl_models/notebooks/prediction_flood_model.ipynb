{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d09546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import logging\n",
    "# === CONFIG ===\n",
    "# Set random seeds and GPU memory growth\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "keras.utils.set_random_seed(812)\n",
    "\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7141a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction mode\n",
    "from usl_models.flood_ml import dataset\n",
    "import pathlib\n",
    "\n",
    "# Parameters\n",
    "filecache_dir = pathlib.Path(\"/home/shared/climateiq/filecache\")\n",
    "# prediction\n",
    "# sim_name = [\"Atlanta_Prediction\"]\n",
    "# rainfall_sim = \"Atlanta-Atlanta_config/Rainfall_Data_22.txt\"\n",
    "\n",
    "batch_size = 2\n",
    "dataset_split=\"train\"\n",
    "# Download (prediction mode)\n",
    "# dataset.download_dataset(\n",
    "#     sim_names=sim_name,          # study area\n",
    "#     output_path=filecache_dir,\n",
    "#     include_labels=False,                      # no labels\n",
    "#     rainfall_sim_name=rainfall_sim,  # simulation for temporal vector\n",
    "#     allow_missing_sim=True                     # skip temporal if missing\n",
    "# )\n",
    "# prediction mode\n",
    "# # # Load dataset\n",
    "# full_dataset = dataset.load_dataset_cached(\n",
    "#     filecache_dir=filecache_dir,\n",
    "#     sim_names=sim_name,            # study area\n",
    "#     dataset_split=None,                          # no split for prediction\n",
    "#     batch_size=batch_size,\n",
    "#     include_labels=False,\n",
    "#     shuffle=False,\n",
    "#     rainfall_sim_name=rainfall_sim  # actual rainfall sim\n",
    "# )\n",
    "\n",
    "\n",
    "# Download (training mode)\n",
    "# dataset_splits = [\"test\", \"train\", \"val\"]\n",
    "# dataset.download_dataset(\n",
    "#     sim_names=[\"Atlanta-Atlanta_config/Rainfall_Data_22.txt\"],  # normal simulations\n",
    "#     output_path=filecache_dir,\n",
    "#     dataset_splits=dataset_splits,               # train/val/test splits\n",
    "#     include_labels=True                        # get labels too\n",
    "# )\n",
    "\n",
    "full_dataset = dataset.load_dataset_cached(\n",
    "    filecache_dir=filecache_dir,\n",
    "    sim_names=[\"Manhattan-Manhattan_config/Rainfall_Data_22.txt\"],\n",
    "    dataset_split=dataset_split,\n",
    "    batch_size=batch_size,\n",
    "    include_labels=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b434e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from usl_models.flood_ml.model import SpatialAttention\n",
    "# Path to your saved model\n",
    "model_path = \"/home/se2890/climateiq-cnn-9/logs/htune_project_20251008-204114/model\"\n",
    "N_STEPS = 2  # Number of time steps the model wants to predict ahead\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "loaded_model.summary()\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "# model = FloodModel.from_checkpoint(model_path)\n",
    "custom_objects = {'SpatialAttention': SpatialAttention}\n",
    "loaded_model = tf.keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects=custom_objects,\n",
    "    compile=False\n",
    ")\n",
    "model.set_weights(loaded_model.get_weights())\n",
    "\n",
    "# Test calling the model for n predictions\n",
    "# full_dataset = load_dataset(sim_names=sim_names, batch_size=4, dataset_split= \"train\")\n",
    "inputs, labels,  name = next(iter(full_dataset))\n",
    "predictions = model.call_n(inputs, n=N_STEPS)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5805bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "all_max_preds_np = []    # list of [B, H, W] NumPy arrays\n",
    "all_max_labels_np = []   # only filled if true labels exist\n",
    "all_chunk_names = []\n",
    "\n",
    "for i, (inputs, labels, metadata) in enumerate(full_dataset):\n",
    "    current_bs = inputs[\"spatiotemporal\"].shape[0]\n",
    "    chunk_names = metadata[\"feature_chunk\"].numpy().astype(str).tolist()\n",
    "    all_chunk_names.extend(chunk_names)\n",
    "\n",
    "    print(f\"\\n[Batch {i}] Chunk names: {chunk_names}\")\n",
    "\n",
    "    # --- Handle incomplete batches ---\n",
    "    if current_bs < batch_size:\n",
    "        repeats = batch_size - current_bs\n",
    "\n",
    "        def pad_tensor(t):\n",
    "            return tf.concat([t, tf.repeat(t[-1:], repeats=repeats, axis=0)], axis=0)\n",
    "\n",
    "        padded_inputs = {k: pad_tensor(v) for k, v in inputs.items()}\n",
    "        preds = model.call_n(padded_inputs, n=N_STEPS)[:current_bs]\n",
    "    else:\n",
    "        preds = model.call_n(inputs, n=N_STEPS)\n",
    "\n",
    "    print(f\"[Batch {i}] Prediction shape: {preds.shape}\")\n",
    "\n",
    "    # --- Convert predictions immediately to NumPy (CPU-safe) ---\n",
    "    preds_np = preds.numpy()\n",
    "    max_pred_np = np.max(preds_np, axis=1)   # [B, H, W]\n",
    "    all_max_preds_np.append(max_pred_np)\n",
    "\n",
    "    # --- Only store real labels (skip dummy zeros in prediction mode) ---\n",
    "    if tf.reduce_sum(tf.cast(labels != 0.0, tf.int32)) > 0:\n",
    "        labels_np = labels.numpy()\n",
    "        max_label_np = np.max(labels_np, axis=1)\n",
    "        all_max_labels_np.append(max_label_np)\n",
    "\n",
    "# --- Final NumPy arrays on CPU ---\n",
    "max_preds_all_np = np.concatenate(all_max_preds_np, axis=0)            # [N, H, W]\n",
    "max_labels_all_np = (\n",
    "    np.concatenate(all_max_labels_np, axis=0) if all_max_labels_np else None\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Aggregation complete:\")\n",
    "print(f\"  - Chunks: {len(all_chunk_names)}\")\n",
    "print(f\"  - max_preds_all_np: {max_preds_all_np.shape}\")\n",
    "print(f\"  - max_labels_all_np: {'None' if max_labels_all_np is None else max_labels_all_np.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501dfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i = min(10, len(all_chunk_names) - 1)  # safe index\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# --- Prediction map ---\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(max_preds_all_np[i], cmap=\"Blues\")\n",
    "plt.title(f\"Predicted Max Flood ({all_chunk_names[i]})\")\n",
    "plt.colorbar()\n",
    "\n",
    "# --- Ground truth or “no labels” notice ---\n",
    "plt.subplot(1, 2, 2)\n",
    "if max_labels_all_np is not None:\n",
    "    plt.imshow(max_labels_all_np[i], cmap=\"Blues\")\n",
    "    plt.title(\"Ground Truth Max Flood\")\n",
    "    plt.colorbar()\n",
    "else:\n",
    "    plt.axis(\"off\")\n",
    "    plt.text(\n",
    "        0.5, 0.5,\n",
    "        \"No Ground Truth Available\\n(Prediction Mode)\",\n",
    "        ha=\"center\", va=\"center\",\n",
    "        fontsize=12,\n",
    "        color=\"gray\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c36e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "num_to_plot = min(5, len(max_preds_all_np))  # limit to available samples\n",
    "has_labels = max_labels_all_np is not None   # prediction vs training mode\n",
    "\n",
    "# --- Layout ---\n",
    "ncols = 2 if has_labels else 1\n",
    "fig, axes = plt.subplots(\n",
    "    num_to_plot,\n",
    "    ncols,\n",
    "    figsize=(8 if has_labels else 4, num_to_plot * 4),\n",
    "    squeeze=False,  # always 2D array of axes\n",
    ")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.25)\n",
    "\n",
    "for i in range(num_to_plot):\n",
    "    pred = max_preds_all_np[i]\n",
    "\n",
    "    # Dynamic color scaling (robust to outliers)\n",
    "    if has_labels:\n",
    "        truth = max_labels_all_np[i]\n",
    "        combined = np.concatenate([pred.flatten(), truth.flatten()])\n",
    "        vmin = np.percentile(combined, 2)\n",
    "        vmax = np.percentile(combined, 98)\n",
    "    else:\n",
    "        vmin = np.percentile(pred, 2)\n",
    "        vmax = np.percentile(pred, 98)\n",
    "\n",
    "    # --- Prediction map ---\n",
    "    ax_pred = axes[i, 0]\n",
    "    im_pred = ax_pred.imshow(pred, cmap=\"cubehelix\", vmin=vmin, vmax=vmax)\n",
    "    ax_pred.set_title(f\"ML Prediction {i}\", fontsize=11)\n",
    "    ax_pred.axis(\"off\")\n",
    "\n",
    "    # --- Ground truth / placeholder ---\n",
    "    if has_labels:\n",
    "        ax_truth = axes[i, 1]\n",
    "        ax_truth.imshow(truth, cmap=\"cubehelix\", vmin=vmin, vmax=vmax)\n",
    "        ax_truth.set_title(f\"Ground Truth {i}\", fontsize=11)\n",
    "        ax_truth.axis(\"off\")\n",
    "    else:\n",
    "        # Add placeholder notice next to the prediction\n",
    "        ax_note = ax_pred.twinx()  # overlay note to the right of the image\n",
    "        ax_note.axis(\"off\")\n",
    "        ax_note.text(\n",
    "            1.05, 0.5,\n",
    "            \"No Ground Truth\\nAvailable\",\n",
    "            transform=ax_pred.transAxes,\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            fontsize=11,\n",
    "            color=\"gray\",\n",
    "        )\n",
    "\n",
    "# --- Shared colorbar ---\n",
    "cbar_ax = fig.add_axes([0.93, 0.3, 0.015, 0.4])\n",
    "fig.colorbar(im_pred, cax=cbar_ax, label=\"Maximum Water Depth (m)\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Rainfall Scenario 1 (1-hour storm) – Location: Atlanta\",\n",
    "    fontsize=14,\n",
    "    y=0.98,\n",
    ")\n",
    "plt.tight_layout(rect=[0, 0, 0.91, 0.97])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d00462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b39db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import matplotlib.pyplot as plt\n",
    "from google.cloud import storage\n",
    "import tempfile\n",
    "import os\n",
    "# === Notebook inputs ===\n",
    "sim_name = [\"MNHT_test_Prediction\"]\n",
    "rainfall_sim = \"MNHT_test_Prediction/Rainfall_Data_22.txt\"\n",
    "# === Geo metadata ===\n",
    "pixel_size = 1\n",
    "top_left_x = 0\n",
    "top_left_y = 0\n",
    "transform = from_origin(top_left_x, top_left_y, pixel_size, pixel_size)\n",
    "crs = \"EPSG:4326\"\n",
    "\n",
    "# === GCS setup ===\n",
    "client = storage.Client()\n",
    "bucket_name = \"mloutputstest\"\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "\n",
    "\n",
    "rainfall_scenario = rainfall_sim.split(\"/\")[-1]  # e.g. \"Rainfall_Data_22.txt\"\n",
    "study_area = sim_name[0]                         # e.g. \"Manhattan-Manhattan_config_Prediction\"\n",
    "\n",
    "# === Define GCS folder structure ===\n",
    "base_folder = f\"{study_area}/scenario_{rainfall_scenario}\"\n",
    "tif_folder = f\"{base_folder}/tif\"\n",
    "png_folder = f\"{base_folder}/png\"\n",
    "\n",
    "# === Final arrays on CPU ===\n",
    "N = max_preds_all_np.shape[0]\n",
    "has_labels = max_labels_all_np is not None\n",
    "\n",
    "print(f\"\\nSaving {N} predicted chunks to gs://{bucket_name}/{base_folder}\")\n",
    "print(f\"Ground-truth labels detected: {has_labels} (will NOT be uploaded)\")\n",
    "\n",
    "for idx in range(N):\n",
    "    chunk_name = all_chunk_names[idx] if idx < len(all_chunk_names) else f\"chunk_{idx}\"\n",
    "    sample = max_preds_all_np[idx]  # Already NumPy on CPU\n",
    "\n",
    "    # --- Save GeoTIFF (prediction only) ---\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".tif\") as tmp_tif:\n",
    "        with rasterio.open(\n",
    "            tmp_tif.name,\n",
    "            \"w\",\n",
    "            driver=\"GTiff\",\n",
    "            height=sample.shape[0],\n",
    "            width=sample.shape[1],\n",
    "            count=1,\n",
    "            dtype=sample.dtype,\n",
    "            crs=crs,\n",
    "            transform=transform,\n",
    "        ) as dst:\n",
    "            dst.write(sample, 1)\n",
    "\n",
    "        tif_blob_path = f\"{tif_folder}/{chunk_name}.tif\"\n",
    "        bucket.blob(tif_blob_path).upload_from_filename(tmp_tif.name)\n",
    "        print(f\"✅ Uploaded GeoTIFF: gs://{bucket_name}/{tif_blob_path}\")\n",
    "\n",
    "    # --- Save PNG (prediction only) ---\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".png\") as tmp_png:\n",
    "        plt.imsave(tmp_png.name, sample, cmap=\"Blues\")\n",
    "        png_blob_path = f\"{png_folder}/{chunk_name}.png\"\n",
    "        bucket.blob(png_blob_path).upload_from_filename(tmp_png.name)\n",
    "        print(f\"✅ Uploaded PNG: gs://{bucket_name}/{png_blob_path}\")\n",
    "\n",
    "print(\"\\n✅ All prediction files uploaded successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
