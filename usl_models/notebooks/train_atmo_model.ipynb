{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmo Model Training Notebook\n",
    "\n",
    "Train an Atmo Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import keras\n",
    "import os, time\n",
    "import pathlib\n",
    "from usl_models.atmo_ml.model import AtmoModel, AtmoModelParams\n",
    "from usl_models.atmo_ml import dataset\n",
    "from google.cloud import storage\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "data_bucket_name = \"climateiq-study-area-feature-chunks\"\n",
    "label_bucket_name = \"climateiq-study-area-label-chunks\"\n",
    "batch_size = 2\n",
    "\n",
    "sim_dirs = [\n",
    "    (\n",
    "        \"NYC_Heat_Test\",\n",
    "        [\n",
    "            \"NYC_summer_2000_01p\",\n",
    "            # 'NYC_summer_2010_99p',\n",
    "            # 'NYC_summer_2015_50p',\n",
    "            # 'NYC_summer_2017_25p',\n",
    "            # 'NYC_summer_2018_75p'\n",
    "        ],\n",
    "    ),\n",
    "    (\n",
    "        \"PHX_Heat_Test\",\n",
    "        [\n",
    "            # 'PHX_summer_2008_25p',\n",
    "            # 'PHX_summer_2009_50p',\n",
    "            # 'PHX_summer_2011_99p',\n",
    "            # 'PHX_summer_2015_75p',\n",
    "            # 'PHX_summer_2020_01p'\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "filecache_dir = pathlib.Path(\"/home/shared/climateiq/filecache\")\n",
    "\n",
    "sim_names = []\n",
    "for sim_dir, subdirs in sim_dirs:\n",
    "    for subdir in subdirs:\n",
    "        sim_names.append(sim_dir + \"/\" + subdir)\n",
    "\n",
    "print(sim_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Atmo Model\n",
    "model_params = AtmoModelParams()\n",
    "model = AtmoModel(model_params)\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "example_keys = None  # To load entire dataset\n",
    "# example_keys = [(\"NYC_Heat_Test/NYC_summer_2000_01p\", \"2000-05-25\")]  # To test on single example\n",
    "train_frac = 0.8\n",
    "\n",
    "train_ds = dataset.load_dataset_cached(\n",
    "    filecache_dir, example_keys=example_keys, hash_range=(0, train_frac)\n",
    ").batch(batch_size=batch_size)\n",
    "val_ds = dataset.load_dataset_cached(\n",
    "    filecache_dir, example_keys=example_keys, hash_range=(train_frac, 1.0)\n",
    ").batch(batch_size=batch_size)\n",
    "\n",
    "\n",
    "log_dir = os.path.join(\"./logs\", \"run_\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "print(\"Tensorboard log directory:\", log_dir)\n",
    "\n",
    "model.fit(train_ds, val_ds, epochs=150, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions from the validation set\n",
    "predictions = model._model.predict(val_ds)  # Use the underlying Keras model\n",
    "\n",
    "# Assuming the structure of val_ds returns (input_data, ground_truth)\n",
    "for input_data, ground_truth in val_ds.take(1):  # Taking just one batch from val_ds\n",
    "    # Get predicted labels\n",
    "    predicted_labels = model._model.predict(input_data)\n",
    "\n",
    "    # Compute shared vmin and vmax for consistent color range\n",
    "    vmin = min(\n",
    "        np.min(ground_truth[:, 0, :, :, 0]), np.min(predicted_labels[:, 0, :, :, 0])\n",
    "    )\n",
    "    vmax = max(\n",
    "        np.max(ground_truth[:, 0, :, :, 0]), np.max(predicted_labels[:, 0, :, :, 0])\n",
    "    )\n",
    "\n",
    "    # Visualize the first sample\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), dpi=150)  # Higher DPI for quality\n",
    "\n",
    "    # Ground Truth Visualization\n",
    "    img1 = axes[0].imshow(\n",
    "        ground_truth[0, 0, :, :, 0], cmap=\"viridis\", vmin=90, vmax=100\n",
    "    )\n",
    "    axes[0].set_title(\"Ground Truth\")\n",
    "    plt.colorbar(img1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Prediction Visualization\n",
    "    img2 = axes[1].imshow(\n",
    "        predicted_labels[0, 0, :, :, 0], cmap=\"viridis\", vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    axes[1].set_title(\"Predicted Labels\")\n",
    "    plt.colorbar(img2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break  # Break after visualizing one batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
