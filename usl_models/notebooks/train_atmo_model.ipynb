{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmo Model Training Notebook\n",
    "\n",
    "Train an Atmo Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 18:44:41.833683: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-24 18:44:41.883726: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-24 18:44:41.883762: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-24 18:44:41.884970: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-24 18:44:41.892910: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NYC_Heat_Test/NYC_summer_2000_01p']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from datetime import datetime, timedelta\n",
    "import os, time\n",
    "from usl_models.atmo_ml.model import AtmoModel, AtmoModelParams\n",
    "from usl_models.atmo_ml import dataset\n",
    "from google.cloud import storage\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# climateiq-study-area-feature-chunks/NYC_Heat/NYC_summer_2000_01p\n",
    "# Define bucket names and folder paths\n",
    "data_bucket_name = \"climateiq-study-area-feature-chunks\"\n",
    "label_bucket_name = \"climateiq-study-area-label-chunks\"\n",
    "time_steps_per_day = 6\n",
    "batch_size = 4\n",
    "\n",
    "sim_dirs = [\n",
    "    ('NYC_Heat_Test', [\n",
    "        'NYC_summer_2000_01p',\n",
    "        # 'NYC_summer_2010_99p',\n",
    "        # 'NYC_summer_2015_50p',\n",
    "        # 'NYC_summer_2017_25p',\n",
    "        # 'NYC_summer_2018_75p'\n",
    "    ]),\n",
    "    ('PHX_Heat_Test', [\n",
    "        # 'PHX_summer_2008_25p',\n",
    "        # 'PHX_summer_2009_50p',\n",
    "        # 'PHX_summer_2011_99p',\n",
    "        # 'PHX_summer_2015_75p',\n",
    "        # 'PHX_summer_2020_01p'\n",
    "    ])\n",
    "]\n",
    "\n",
    "sim_names = []\n",
    "for sim_dir, subdirs in sim_dirs:\n",
    "    for subdir in subdirs:\n",
    "        sim_names.append(sim_dir + '/' + subdir)\n",
    "\n",
    "print(sim_names)\n",
    "client = storage.Client(project=\"climateiq\")\n",
    "feature_bucket = client.bucket(data_bucket_name)\n",
    "label_bucket = client.bucket(label_bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 18:44:44.536905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 37027 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
      "WARNING:root:load_day (2000-05-25, NYC_Heat_Test/NYC_summer_2000_01p)\n",
      "WARNING:root:label shape: (8, 200, 200, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 200, 200, 5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, label = dataset.load_day(\n",
    "    date=datetime.strptime(\"2000-05-25\", dataset.DATE_FORMAT),\n",
    "    sim_name=sim_names[0],\n",
    "    feature_bucket=feature_bucket,\n",
    "    label_bucket=label_bucket,\n",
    ")\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total simulation days before filtering: 1\n",
      "INFO:root:Selected 1/1 days (100.00%) based on hash range (0.0, 1.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_name_dates [('NYC_Heat_Test/NYC_summer_2000_01p', '2000-05-25')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:load_day (2000-05-25, NYC_Heat_Test/NYC_summer_2000_01p)\n",
      "WARNING:root:label shape: (8, 200, 200, 5)\n",
      "INFO:root:Total generated samples: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 200, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "train_frac = 0.8\n",
    "\n",
    "# Create training dataset with fused spatiotemporal data\n",
    "train_ds = dataset.load_dataset(\n",
    "    data_bucket_name=data_bucket_name,\n",
    "    label_bucket_name=label_bucket_name,\n",
    "    sim_names=sim_names,\n",
    "    dates=[\"2000-05-25\"],\n",
    ").batch(batch_size=1)\n",
    "for inputs, labels in train_ds:\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Total simulation days before filtering: 1\n",
      "INFO:root:Selected 1/1 days (100.00%) based on hash range (0.0, 1.0).\n",
      "INFO:root:Total simulation days before filtering: 1\n",
      "INFO:root:Selected 1/1 days (100.00%) based on hash range (0.0, 1.0).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sim_name_dates [('NYC_Heat_Test/NYC_summer_2000_01p', '2000-05-25')]\n",
      "sim_name_dates [('NYC_Heat_Test/NYC_summer_2000_01p', '2000-05-25')]\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset with fused spatiotemporal data\n",
    "train_ds = dataset.load_dataset(\n",
    "    data_bucket_name=data_bucket_name,\n",
    "    label_bucket_name=label_bucket_name,\n",
    "    sim_names=sim_names,\n",
    "    dates=[\"2000-05-25\"],\n",
    ").batch(batch_size=batch_size)\n",
    "\n",
    "# Create validation dataset with fused spatiotemporal data\n",
    "val_ds = dataset.load_dataset(\n",
    "    data_bucket_name=data_bucket_name,\n",
    "    label_bucket_name=label_bucket_name,\n",
    "    sim_names=sim_names,\n",
    "    dates=[\"2000-05-25\"],\n",
    ").batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Atmo Model\n",
    "model_params = AtmoModelParams()\n",
    "model = AtmoModel(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs/run_20250124-184506\n"
     ]
    }
   ],
   "source": [
    "# Create a unique log directory by appending the current timestamp\n",
    "log_dir = os.path.join(\"./logs\", \"run_\" + time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# Set up TensorBoard callback\n",
    "tb_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "Output shape: TensorShape([None, 8, 50, 50, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-24 18:46:21.277779: W tensorflow/core/framework/op_kernel.cc:1827] INVALID_ARGUMENT: TypeError: load_day() got an unexpected keyword argument 'storage_client'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 276, in data_generator\n",
      "    load_result = load_day(\n",
      "                  ^^^^^^^^^\n",
      "\n",
      "TypeError: load_day() got an unexpected keyword argument 'storage_client'\n",
      "\n",
      "\n",
      "2025-01-24 18:46:21.277963: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 2774554904366885483\n",
      "2025-01-24 18:46:21.277992: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7148468704701060519\n",
      "2025-01-24 18:46:21.278002: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 11417785844146237309\n",
      "2025-01-24 18:46:21.278010: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 1836144450642005364\n",
      "2025-01-24 18:46:21.278065: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9532272681054199738\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: load_day() got an unexpected keyword argument 'storage_client'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 276, in data_generator\n    load_result = load_day(\n                  ^^^^^^^^^\n\nTypeError: load_day() got an unexpected keyword argument 'storage_client'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  TypeError: load_day() got an unexpected keyword argument 'storage_client'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 276, in data_generator\n    load_result = load_day(\n                  ^^^^^^^^^\n\nTypeError: load_day() got an unexpected keyword argument 'storage_client'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_7684]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climateiq-cnn/usl_models/usl_models/atmo_ml/model.py:126\u001b[0m, in \u001b[0;36mAtmoModel.fit\u001b[0;34m(self, train_dataset, val_dataset, epochs, steps_per_epoch, early_stopping, callbacks)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m early_stopping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    121\u001b[0m         keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m    122\u001b[0m             monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39mearly_stopping\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: load_day() got an unexpected keyword argument 'storage_client'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 276, in data_generator\n    load_result = load_day(\n                  ^^^^^^^^^\n\nTypeError: load_day() got an unexpected keyword argument 'storage_client'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  TypeError: load_day() got an unexpected keyword argument 'storage_client'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 276, in data_generator\n    load_result = load_day(\n                  ^^^^^^^^^\n\nTypeError: load_day() got an unexpected keyword argument 'storage_client'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_7684]"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_ds, val_ds, epochs=150, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get predictions from the validation set\n",
    "predictions = model._model.predict(val_ds)  # Use the underlying Keras model\n",
    "\n",
    "# Assuming the structure of val_ds returns (input_data, ground_truth)\n",
    "for input_data, ground_truth in val_ds.take(1):  # Taking just one batch from val_ds\n",
    "    # Get predicted labels\n",
    "    predicted_labels = model._model.predict(input_data)\n",
    "    \n",
    "    # Compute shared vmin and vmax for consistent color range\n",
    "    vmin = min(\n",
    "        np.min(ground_truth[:, 0, :, :, 0]), \n",
    "        np.min(predicted_labels[:, 0, :, :, 0])\n",
    "    )\n",
    "    vmax = max(\n",
    "        np.max(ground_truth[:, 0, :, :, 0]), \n",
    "        np.max(predicted_labels[:, 0, :, :, 0])\n",
    "    )\n",
    "    \n",
    "    # Visualize the first sample\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), dpi=150)  # Higher DPI for quality\n",
    "    \n",
    "    # Ground Truth Visualization\n",
    "    img1 = axes[0].imshow(\n",
    "        ground_truth[0, 0, :, :, 0], \n",
    "        cmap='viridis', \n",
    "        vmin=90, \n",
    "        vmax=100\n",
    "    )\n",
    "    axes[0].set_title('Ground Truth')\n",
    "    plt.colorbar(img1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Prediction Visualization\n",
    "    img2 = axes[1].imshow(\n",
    "        predicted_labels[0, 0, :, :, 0], \n",
    "        cmap='viridis', \n",
    "        vmin=vmin, \n",
    "        vmax=vmax\n",
    "    )\n",
    "    axes[1].set_title('Predicted Labels')\n",
    "    plt.colorbar(img2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break  # Break after visualizing one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(train_ds))\n",
    "{key: tensor.shape for key, tensor in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test calling the model on some training data\n",
    "inputs, labels = next(iter(train_ds))\n",
    "prediction = model.call(inputs)\n",
    "print(\"Prediction shape:\", prediction.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
