{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmo Model Training Notebook\n",
    "\n",
    "Train an Atmo Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from usl_models.atmo_ml.model import AtmoModel, AtmoModelParams\n",
    "from usl_models.atmo_ml import dataset\n",
    "from google.cloud import storage\n",
    "\n",
    "# climateiq-study-area-feature-chunks/NYC_Heat/NYC_summer_2000_01p\n",
    "# Define bucket names and folder paths\n",
    "data_bucket_name = \"climateiq-study-area-feature-chunks\"\n",
    "label_bucket_name = \"climateiq-atmospheric-label-chunks\"\n",
    "sim_names = ['NYC_Heat_Test/NYC_summer_2000_01p']\n",
    "time_steps_per_day = 6\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset using create_atmo_dataset with sim_names\n",
    "client = storage.Client(project=\"climateiq\")\n",
    "ds = dataset.load_dataset(\n",
    "    data_bucket_name=data_bucket_name,\n",
    "    label_bucket_name=label_bucket_name,\n",
    "    sim_names=sim_names,\n",
    "    time_steps_per_day=time_steps_per_day,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "train_dataset, val_dataset, test_dataset = dataset.split_dataset(\n",
    "    ds, train_frac=0.7, val_frac=0.15, test_frac=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure of the dataset\n",
    "for model_input, labels in train_dataset.take(1):\n",
    "    print(f\"Spatiotemporal data shape: {model_input['spatiotemporal'].shape}\")\n",
    "    print(f\"Spatial data shape: {model_input['spatial'].shape}\")\n",
    "    print(f\"LU Index data shape: {model_input['lu_index'].shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Atmo Model\n",
    "model_params = AtmoModelParams()\n",
    "model = AtmoModel(model_params)\n",
    "# Compile the model with appropriate optimizer and loss\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test calling the model on some training data\n",
    "inputs, labels = next(iter(train_dataset))\n",
    "prediction = model.call(inputs)\n",
    "print(\"Prediction shape:\", prediction.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
