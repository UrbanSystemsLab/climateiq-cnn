{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmo Model Training Notebook\n",
    "\n",
    "Train an Atmo Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow as tf\n",
    "from usl_models.atmo_ml.model import AtmoModel, AtmoModelParams\n",
    "from usl_models.atmo_ml import dataset\n",
    "from google.cloud import storage\n",
    "\n",
    "# climateiq-study-area-feature-chunks/NYC_Heat/NYC_summer_2000_01p\n",
    "# Define bucket names and folder paths\n",
    "data_bucket_name = \"climateiq-study-area-feature-chunks\"\n",
    "label_bucket_name = \"climateiq-study-area-label-chunks\"\n",
    "time_steps_per_day = 6\n",
    "batch_size = 2\n",
    "\n",
    "sim_dirs = [\n",
    "    ('NYC_Heat_Test', [\n",
    "        'NYC_summer_2000_01p',\n",
    "        'NYC_summer_2010_25p',\n",
    "        'NYC_summer_2015_50p',\n",
    "        'NYC_summer_2017_75p',\n",
    "        'NYC_summer_2018_99p'\n",
    "    ]),\n",
    "    ('PHX_Heat_Test', [\n",
    "        'PHX_summer_2008_25p',\n",
    "        'PHX_summer_2009_50p',\n",
    "        'PHX_summer_2011_99p',\n",
    "        'PHX_summer_2015_75p',\n",
    "        'PHX_summer_2020_01p'\n",
    "    ])\n",
    "]\n",
    "\n",
    "sim_names = []\n",
    "for sim_dir, subdirs in sim_dirs:\n",
    "    for subdir in subdirs:\n",
    "        sim_names.append(sim_dir + '/' + subdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 20:58:12.893772: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:20: Filling up shuffle buffer (this may take a while): 1 of 1000\n",
      "2024-12-02 20:58:14.439274: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 0 of dimension 0 out of bounds.\n",
      "2024-12-02 20:58:14.440822: W tensorflow/core/framework/op_kernel.cc:1827] UNKNOWN: InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 68, in data_generator\n",
      "    lu_index_data = load_folder_from_cloud(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n",
      "    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 68, in data_generator\n    lu_index_data = load_folder_from_cloud(\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mClient(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclimateiq\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mload_dataset(\n\u001b[1;32m      4\u001b[0m     data_bucket_name\u001b[38;5;241m=\u001b[39mdata_bucket_name,\n\u001b[1;32m      5\u001b[0m     label_bucket_name\u001b[38;5;241m=\u001b[39mlabel_bucket_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     max_blobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mbatch(batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m---> 10\u001b[0m train_dataset, val_dataset, test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py:272\u001b[0m, in \u001b[0;36msplit_dataset\u001b[0;34m(dataset, train_frac, val_frac, test_frac)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a tf.data.Dataset into training, validation, and test sets.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    train, validation, and test data.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_frac \u001b[38;5;241m+\u001b[39m val_frac \u001b[38;5;241m+\u001b[39m test_frac \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFractions must sum to 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 272\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate the total dataset size\u001b[39;00m\n\u001b[1;32m    273\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_frac \u001b[38;5;241m*\u001b[39m total_size)\n\u001b[1;32m    274\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(val_frac \u001b[38;5;241m*\u001b[39m total_size)\n",
      "File \u001b[0;32m~/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py:272\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Splits a tf.data.Dataset into training, validation, and test sets.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    train, validation, and test data.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m train_frac \u001b[38;5;241m+\u001b[39m val_frac \u001b[38;5;241m+\u001b[39m test_frac \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFractions must sum to 1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 272\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate the total dataset size\u001b[39;00m\n\u001b[1;32m    273\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_frac \u001b[38;5;241m*\u001b[39m total_size)\n\u001b[1;32m    274\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(val_frac \u001b[38;5;241m*\u001b[39m total_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_4_device_/job:localhost/replica:0/task:0/device:CPU:0}} InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/josiahkp/climateiq-cnn/usl_models/usl_models/atmo_ml/dataset.py\", line 68, in data_generator\n    lu_index_data = load_folder_from_cloud(\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py\", line 5883, in raise_from_not_ok_status\n    raise core._status_to_exception(e) from None  # pylint: disable=protected-access\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:GPU:0}} slice index 0 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "# Create the training dataset using create_atmo_dataset with sim_names\n",
    "client = storage.Client(project=\"climateiq\")\n",
    "ds = dataset.load_dataset(\n",
    "    data_bucket_name=data_bucket_name,\n",
    "    label_bucket_name=label_bucket_name,\n",
    "    sim_names=sim_names,\n",
    "    shuffle=None,\n",
    "    timesteps_per_day=time_steps_per_day,\n",
    "    max_blobs=None\n",
    ").batch(batch_size=batch_size)\n",
    "train_dataset, val_dataset, test_dataset = dataset.split_dataset(\n",
    "    ds, train_frac=0.7, val_frac=0.15, test_frac=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Atmo Model\n",
    "model_params = AtmoModelParams()\n",
    "model = AtmoModel(model_params)\n",
    "# model._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split shape: (None, 50, 50, 5, 2, 256)\n",
      "Separated shape (pre-slice): (None, 50, 50, 10, 256)\n",
      "Reduced shape after removing outermost time steps: (None, 50, 50, 8, 256)\n",
      "Split shape: (None, 50, 50, 5, 2, 256)\n",
      "Separated shape (pre-slice): (None, 50, 50, 10, 256)\n",
      "Reduced shape after removing outermost time steps: (None, 50, 50, 8, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 20:46:21.683588: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inatmo_conv_lstm/conv_lstm/conv_lstm2d/while/body/_1/atmo_conv_lstm/conv_lstm/conv_lstm2d/while/dropout_7/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: TensorShape([None, 8, 50, 50, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 20:46:22.551002: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 11s 11s/step - loss: 15631.8096 - mean_absolute_error: 63.2611 - root_mean_squared_error: 125.0272Output shape: TensorShape([None, 8, 50, 50, 256])\n",
      "      2/Unknown - 11s 120ms/step - loss: 12389.4902 - mean_absolute_error: 50.5707 - root_mean_squared_error: 111.3081Output shape: TensorShape([None, 8, 50, 50, 256])\n",
      "2/2 [==============================] - 11s 227ms/step - loss: 12389.4902 - mean_absolute_error: 50.5707 - root_mean_squared_error: 111.3081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd0c03b1310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(train_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spatiotemporal': TensorShape([2, 6, 200, 200, 12]),\n",
       " 'spatial': TensorShape([2, 200, 200, 22]),\n",
       " 'lu_index': TensorShape([2, 200, 200])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_dataset))\n",
    "{key: tensor.shape for key, tensor in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"atmo_conv_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  488       \n",
      "                                                                 \n",
      " spatial_cnn (Sequential)    (None, 50, 50, 128)       252992    \n",
      "                                                                 \n",
      " spatiotemporal_cnn (Sequen  (None, None, 50, 50, 64   30480     \n",
      " tial)                       )                                   \n",
      "                                                                 \n",
      " conv_lstm (Sequential)      (None, None, 50, 50, 51   45877248  \n",
      "                             2)                                  \n",
      "                                                                 \n",
      " t2_output_cnn (Sequential)  (None, None, 200, 200,    69729     \n",
      "                             1)                                  \n",
      "                                                                 \n",
      " rh2_output_cnn (Sequential  (None, None, 200, 200,    69729     \n",
      " )                           1)                                  \n",
      "                                                                 \n",
      " wspd10_output_cnn (Sequent  (None, None, 200, 200,    69729     \n",
      " ial)                        1)                                  \n",
      "                                                                 \n",
      " wdir10_output_cnn (Sequent  (None, None, 200, 200,    69746     \n",
      " ial)                        2)                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46440141 (177.16 MB)\n",
      "Trainable params: 46440141 (177.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split shape: (2, 50, 50, 5, 2, 256)\n",
      "Separated shape (pre-slice): (2, 50, 50, 10, 256)\n",
      "Reduced shape after removing outermost time steps: (2, 50, 50, 8, 256)\n",
      "Output shape: TensorShape([2, 8, 50, 50, 256])\n",
      "Prediction shape: (2, 8, 200, 200, 5)\n"
     ]
    }
   ],
   "source": [
    "# Test calling the model on some training data\n",
    "inputs, labels = next(iter(train_dataset))\n",
    "prediction = model.call(inputs)\n",
    "print(\"Prediction shape:\", prediction.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
