{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloodML Prediction Notebook\n",
    "\n",
    "Clean, single-purpose notebook for running flood predictions.\n",
    "\n",
    "**Sections:**\n",
    "1. Configuration - model path, data settings\n",
    "2. Environment Setup\n",
    "3. Load Model\n",
    "4. Download Data (Optional) - skip if data already cached\n",
    "5. Load Data - supports both training data (with labels) and prediction-only data\n",
    "6. Run Predictions\n",
    "7. Visualize Results\n",
    "8. Compute Metrics (if labels available)\n",
    "9. Export Results (GeoTIFF, PNG, GCS upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Edit these values\n",
    "# ============================================================================\n",
    "\n",
    "# --- Model ---\n",
    "MODEL_PATH = \"logs/flood_training_XXXXXXXX-XXXXXX/final_model.keras\"  # <-- UPDATE THIS\n",
    "\n",
    "# --- Data paths ---\n",
    "FILECACHE_DIR = \"/home/shared/climateiq/filecache\"\n",
    "\n",
    "# --- Download settings ---\n",
    "# Set to True ONLY if you need to download data from GCS\n",
    "DOWNLOAD_DATA = False\n",
    "\n",
    "# --- Prediction mode ---\n",
    "# INCLUDE_LABELS = True  -> Validation mode (has ground truth to compare)\n",
    "# INCLUDE_LABELS = False -> Pure prediction mode (no ground truth)\n",
    "INCLUDE_LABELS = True\n",
    "\n",
    "# ===== FOR VALIDATION MODE (INCLUDE_LABELS=True) =====\n",
    "# Specify simulation names with labels\n",
    "CITY_CONFIG = {\n",
    "    \"Manhattan\": \"Manhattan_config\",\n",
    "}\n",
    "RAINFALL_IDS = [5]  # Which rainfall scenarios to predict on\n",
    "DATASET_SPLIT = \"val\"  # \"train\", \"val\", or \"test\"\n",
    "\n",
    "# ===== FOR PREDICTION MODE (INCLUDE_LABELS=False) =====\n",
    "# Specify study area and rainfall scenario\n",
    "STUDY_AREA = \"Atlanta_Prediction\"  # Study area name (for features)\n",
    "RAINFALL_SIM = \"Atlanta-Atlanta_config/Rainfall_Data_22.txt\"  # For temporal features\n",
    "\n",
    "# --- Prediction settings ---\n",
    "N_STEPS = 13              # Number of timesteps to predict\n",
    "BATCH_SIZE = 2            # Reduce if OOM errors\n",
    "MAX_CHUNKS = None         # None = all chunks, or set integer for quick tests\n",
    "\n",
    "# --- Export settings ---\n",
    "EXPORT_RESULTS = False    # Set True to export GeoTIFF/PNG\n",
    "GCS_BUCKET = \"mloutputstest\"  # GCS bucket for uploads (if EXPORT_RESULTS=True)\n",
    "\n",
    "# Model params (should match training)\n",
    "N_FLOOD_MAPS = 5\n",
    "M_RAINFALL = 6\n",
    "\n",
    "print(f\"Mode: {'Validation (with labels)' if INCLUDE_LABELS else 'Prediction (no labels)'}\")\n",
    "print(f\"Download data: {DOWNLOAD_DATA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import logging\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keras.utils.set_random_seed(42)\n",
    "\n",
    "for gpu in tf.config.list_physical_devices(\"GPU\"):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from usl_models.flood_ml.model import FloodModel, SpatialAttention\n",
    "from usl_models.flood_ml.dataset import load_dataset_cached, download_dataset\n",
    "from usl_models.flood_ml import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build simulation names based on mode\n",
    "if INCLUDE_LABELS:\n",
    "    # Validation mode: build from city/rainfall config\n",
    "    sim_names = []\n",
    "    for city, config in CITY_CONFIG.items():\n",
    "        for rain_id in RAINFALL_IDS:\n",
    "            sim_names.append(f\"{city}-{config}/Rainfall_Data_{rain_id}.txt\")\n",
    "    print(f\"Validation mode - {len(sim_names)} simulations:\")\n",
    "    for s in sim_names:\n",
    "        exists = (pathlib.Path(FILECACHE_DIR) / s).exists()\n",
    "        print(f\"  [{'CACHED' if exists else 'NOT CACHED'}] {s}\")\n",
    "else:\n",
    "    # Prediction mode: study area + rainfall sim\n",
    "    sim_names = [STUDY_AREA]\n",
    "    rainfall_dir = pathlib.Path(FILECACHE_DIR) / STUDY_AREA / pathlib.Path(RAINFALL_SIM).name\n",
    "    print(f\"Prediction mode:\")\n",
    "    print(f\"  Study area: {STUDY_AREA}\")\n",
    "    print(f\"  Rainfall: {RAINFALL_SIM}\")\n",
    "    print(f\"  Cache exists: {rainfall_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "# Custom objects needed for loading\n",
    "custom_objects = {'SpatialAttention': SpatialAttention}\n",
    "\n",
    "try:\n",
    "    # Try loading as FloodModel\n",
    "    model = FloodModel.from_checkpoint(MODEL_PATH)\n",
    "    print(\"Loaded via FloodModel.from_checkpoint()\")\n",
    "except Exception as e:\n",
    "    print(f\"FloodModel.from_checkpoint failed: {e}\")\n",
    "    print(\"Trying direct keras load...\")\n",
    "    \n",
    "    # Fallback: direct keras load\n",
    "    loaded_model = tf.keras.models.load_model(\n",
    "        MODEL_PATH,\n",
    "        custom_objects=custom_objects,\n",
    "        compile=False\n",
    "    )\n",
    "    \n",
    "    # Wrap in FloodModel\n",
    "    params = FloodModel.Params(\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "    )\n",
    "    model = FloodModel(params=params)\n",
    "    model._model.set_weights(loaded_model.get_weights())\n",
    "    print(\"Loaded via keras and wrapped in FloodModel\")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model summary\n",
    "model._model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Download Data (Optional)\n",
    "\n",
    "**Skip this section if data is already cached.**\n",
    "\n",
    "Set `DOWNLOAD_DATA = True` in Configuration to enable download from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from GCS to local filecache\n",
    "# This cell is skipped if DOWNLOAD_DATA = False\n",
    "\n",
    "if DOWNLOAD_DATA:\n",
    "    print(\"=\"*60)\n",
    "    print(\"DOWNLOADING DATA FROM GCS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Target directory: {FILECACHE_DIR}\")\n",
    "    print(f\"Mode: {'Validation (with labels)' if INCLUDE_LABELS else 'Prediction (features only)'}\")\n",
    "    print()\n",
    "    \n",
    "    if INCLUDE_LABELS:\n",
    "        # Validation mode: download with labels\n",
    "        print(f\"Downloading {len(sim_names)} simulations with labels...\")\n",
    "        download_dataset(\n",
    "            sim_names=sim_names,\n",
    "            output_path=pathlib.Path(FILECACHE_DIR),\n",
    "            dataset_splits=[DATASET_SPLIT],\n",
    "            include_labels=True,\n",
    "        )\n",
    "    else:\n",
    "        # Prediction mode: download features only\n",
    "        print(f\"Downloading study area: {STUDY_AREA}\")\n",
    "        print(f\"Rainfall source: {RAINFALL_SIM}\")\n",
    "        download_dataset(\n",
    "            sim_names=[STUDY_AREA],\n",
    "            output_path=pathlib.Path(FILECACHE_DIR),\n",
    "            include_labels=False,\n",
    "            rainfall_sim_name=RAINFALL_SIM,\n",
    "            allow_missing_sim=True,\n",
    "        )\n",
    "    \n",
    "    print(\"\\nDownload complete!\")\n",
    "else:\n",
    "    print(\"Skipping download (DOWNLOAD_DATA=False)\")\n",
    "    print(\"Using existing cached data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset based on mode\n",
    "filecache_dir = pathlib.Path(FILECACHE_DIR)\n",
    "\n",
    "if INCLUDE_LABELS:\n",
    "    # VALIDATION MODE - has ground truth labels\n",
    "    print(f\"Loading data WITH labels (split: {DATASET_SPLIT})\")\n",
    "    print(f\"Simulations: {sim_names}\")\n",
    "    \n",
    "    dataset = load_dataset_cached(\n",
    "        filecache_dir=filecache_dir,\n",
    "        sim_names=sim_names,\n",
    "        dataset_split=DATASET_SPLIT,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "        max_chunks=MAX_CHUNKS,\n",
    "        include_labels=True,\n",
    "        shuffle=False,\n",
    "    )\n",
    "else:\n",
    "    # PREDICTION MODE - no ground truth\n",
    "    print(f\"Loading data WITHOUT labels (prediction mode)\")\n",
    "    print(f\"Study area: {STUDY_AREA}\")\n",
    "    print(f\"Rainfall sim: {RAINFALL_SIM}\")\n",
    "    \n",
    "    dataset = load_dataset_cached(\n",
    "        filecache_dir=filecache_dir,\n",
    "        sim_names=[STUDY_AREA],\n",
    "        dataset_split=None,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        n_flood_maps=N_FLOOD_MAPS,\n",
    "        m_rainfall=M_RAINFALL,\n",
    "        max_chunks=MAX_CHUNKS,\n",
    "        include_labels=False,\n",
    "        rainfall_sim_name=RAINFALL_SIM,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview one batch\n",
    "for inputs, labels, metadata in dataset.take(1):\n",
    "    print(\"Sample batch:\")\n",
    "    print(f\"  geospatial: {inputs['geospatial'].shape}\")\n",
    "    print(f\"  temporal: {inputs['temporal'].shape}\")\n",
    "    print(f\"  spatiotemporal: {inputs['spatiotemporal'].shape}\")\n",
    "    print(f\"  labels: {labels.shape}\")\n",
    "    print(f\"  chunk names: {metadata['feature_chunk'].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Run Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions on all chunks\n",
    "all_predictions = []   # List of [H, W] max-depth arrays\n",
    "all_labels = []        # List of [H, W] max-depth arrays (if available)\n",
    "all_chunk_names = []   # Chunk identifiers\n",
    "\n",
    "print(f\"Running predictions (n_steps={N_STEPS})...\")\n",
    "\n",
    "for batch_idx, (inputs, labels, metadata) in enumerate(dataset):\n",
    "    current_bs = inputs[\"geospatial\"].shape[0]\n",
    "    chunk_names = [s.decode() if isinstance(s, bytes) else s \n",
    "                   for s in metadata[\"feature_chunk\"].numpy()]\n",
    "    \n",
    "    # Handle incomplete final batch by padding\n",
    "    if current_bs < BATCH_SIZE:\n",
    "        def pad_tensor(t):\n",
    "            repeats = BATCH_SIZE - current_bs\n",
    "            return tf.concat([t, tf.repeat(t[-1:], repeats=repeats, axis=0)], axis=0)\n",
    "        \n",
    "        padded_inputs = {k: pad_tensor(v) for k, v in inputs.items()}\n",
    "        preds = model.call_n(padded_inputs, n=N_STEPS)[:current_bs]\n",
    "    else:\n",
    "        preds = model.call_n(inputs, n=N_STEPS)\n",
    "    \n",
    "    # Convert to numpy and compute max over time\n",
    "    preds_np = np.clip(preds.numpy(), 0, None)  # Clip negatives\n",
    "    max_pred = np.max(preds_np, axis=1)         # [B, H, W]\n",
    "    \n",
    "    # Store predictions\n",
    "    for i in range(current_bs):\n",
    "        all_predictions.append(max_pred[i])\n",
    "        all_chunk_names.append(chunk_names[i])\n",
    "    \n",
    "    # Store labels if available\n",
    "    if INCLUDE_LABELS and labels.numpy().max() > 0:\n",
    "        labels_np = labels.numpy()\n",
    "        max_label = np.max(labels_np, axis=1)\n",
    "        for i in range(current_bs):\n",
    "            all_labels.append(max_label[i])\n",
    "    \n",
    "    if (batch_idx + 1) % 5 == 0:\n",
    "        print(f\"  Processed {batch_idx + 1} batches ({len(all_predictions)} chunks)\")\n",
    "\n",
    "# Convert to arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels) if all_labels else None\n",
    "\n",
    "print(f\"\\nPrediction complete!\")\n",
    "print(f\"  Total chunks: {len(all_chunk_names)}\")\n",
    "print(f\"  Predictions shape: {all_predictions.shape}\")\n",
    "print(f\"  Labels available: {all_labels is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a single chunk\n",
    "idx = 0  # Change to view different chunks\n",
    "\n",
    "pred = all_predictions[idx]\n",
    "has_label = all_labels is not None and idx < len(all_labels)\n",
    "\n",
    "if has_label:\n",
    "    label = all_labels[idx]\n",
    "    vmax = max(pred.max(), label.max(), 0.1)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    im0 = axes[0].imshow(label, cmap='Blues', vmin=0, vmax=vmax)\n",
    "    axes[0].set_title(f'Ground Truth Max Depth\\n(max={label.max():.2f}m)')\n",
    "    axes[0].axis('off')\n",
    "    plt.colorbar(im0, ax=axes[0], shrink=0.8)\n",
    "    \n",
    "    im1 = axes[1].imshow(pred, cmap='Blues', vmin=0, vmax=vmax)\n",
    "    axes[1].set_title(f'Predicted Max Depth\\n(max={pred.max():.2f}m)')\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1], shrink=0.8)\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    im = ax.imshow(pred, cmap='Blues')\n",
    "    ax.set_title(f'Predicted Max Depth\\n(max={pred.max():.2f}m)')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle(f'Chunk: {all_chunk_names[idx]}', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple chunks side by side\n",
    "n_show = min(5, len(all_predictions))\n",
    "has_labels = all_labels is not None and len(all_labels) >= n_show\n",
    "\n",
    "ncols = 2 if has_labels else 1\n",
    "fig, axes = plt.subplots(n_show, ncols, figsize=(6*ncols, 5*n_show))\n",
    "if n_show == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i in range(n_show):\n",
    "    pred = all_predictions[i]\n",
    "    \n",
    "    if has_labels:\n",
    "        label = all_labels[i]\n",
    "        vmax = max(pred.max(), label.max(), 0.1)\n",
    "        \n",
    "        axes[i, 0].imshow(label, cmap='cubehelix', vmin=0, vmax=vmax)\n",
    "        axes[i, 0].set_title(f'GT: {all_chunk_names[i]}')\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        axes[i, 1].imshow(pred, cmap='cubehelix', vmin=0, vmax=vmax)\n",
    "        axes[i, 1].set_title(f'Pred (max={pred.max():.2f}m)')\n",
    "        axes[i, 1].axis('off')\n",
    "    else:\n",
    "        axes[i, 0].imshow(pred, cmap='cubehelix')\n",
    "        axes[i, 0].set_title(f'{all_chunk_names[i]} (max={pred.max():.2f}m)')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Compute Metrics (if labels available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_labels is not None:\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    \n",
    "    metrics_per_chunk = []\n",
    "    \n",
    "    for i in range(len(all_predictions)):\n",
    "        pred = all_predictions[i].flatten()\n",
    "        label = all_labels[i].flatten()\n",
    "        \n",
    "        # Mask NaNs\n",
    "        mask = ~np.isnan(label)\n",
    "        pred_m = pred[mask]\n",
    "        label_m = label[mask]\n",
    "        \n",
    "        mae = mean_absolute_error(label_m, pred_m)\n",
    "        rmse = np.sqrt(mean_squared_error(label_m, pred_m))\n",
    "        bias = pred_m.mean() - label_m.mean()\n",
    "        \n",
    "        # IoU for flooded pixels (threshold 0.1m)\n",
    "        thresh = 0.1\n",
    "        intersection = np.logical_and(label_m > thresh, pred_m > thresh).sum()\n",
    "        union = np.logical_or(label_m > thresh, pred_m > thresh).sum()\n",
    "        iou = intersection / max(union, 1)\n",
    "        \n",
    "        metrics_per_chunk.append({\n",
    "            'chunk': all_chunk_names[i],\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'bias': bias,\n",
    "            'iou': iou,\n",
    "        })\n",
    "    \n",
    "    # Summary\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(metrics_per_chunk)\n",
    "    \n",
    "    print(\"Per-chunk metrics:\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    print(\"\\nAggregate metrics:\")\n",
    "    print(f\"  MAE:  {df['mae'].mean():.4f} +/- {df['mae'].std():.4f} m\")\n",
    "    print(f\"  RMSE: {df['rmse'].mean():.4f} +/- {df['rmse'].std():.4f} m\")\n",
    "    print(f\"  Bias: {df['bias'].mean():.4f} +/- {df['bias'].std():.4f} m\")\n",
    "    print(f\"  IoU:  {df['iou'].mean():.4f} +/- {df['iou'].std():.4f}\")\n",
    "else:\n",
    "    print(\"No labels available - skipping metrics computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Results (Optional)\n",
    "\n",
    "Export predictions as GeoTIFF and PNG, optionally upload to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXPORT_RESULTS:\n",
    "    import tempfile\n",
    "    \n",
    "    # Try importing rasterio (for GeoTIFF)\n",
    "    try:\n",
    "        import rasterio\n",
    "        from rasterio.transform import from_origin\n",
    "        HAS_RASTERIO = True\n",
    "    except ImportError:\n",
    "        print(\"Warning: rasterio not installed. GeoTIFF export disabled.\")\n",
    "        print(\"Install with: pip install rasterio\")\n",
    "        HAS_RASTERIO = False\n",
    "    \n",
    "    # Local output directory\n",
    "    output_dir = pathlib.Path(\"prediction_output\")\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # GeoTIFF settings (update these for your coordinate system)\n",
    "    pixel_size = 10  # metres per pixel\n",
    "    top_left_x = 0\n",
    "    top_left_y = 0\n",
    "    crs = \"EPSG:32618\"  # UTM zone 18N (adjust for your area)\n",
    "    \n",
    "    print(f\"Exporting {len(all_predictions)} predictions...\")\n",
    "    \n",
    "    for i, (pred, chunk_name) in enumerate(zip(all_predictions, all_chunk_names)):\n",
    "        # Clean chunk name for filename\n",
    "        safe_name = chunk_name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\")\n",
    "        \n",
    "        # Save PNG\n",
    "        png_path = output_dir / f\"{safe_name}.png\"\n",
    "        plt.imsave(png_path, pred, cmap='Blues')\n",
    "        \n",
    "        # Save GeoTIFF\n",
    "        if HAS_RASTERIO:\n",
    "            tif_path = output_dir / f\"{safe_name}.tif\"\n",
    "            transform = from_origin(top_left_x, top_left_y, pixel_size, pixel_size)\n",
    "            \n",
    "            with rasterio.open(\n",
    "                tif_path, 'w',\n",
    "                driver='GTiff',\n",
    "                height=pred.shape[0],\n",
    "                width=pred.shape[1],\n",
    "                count=1,\n",
    "                dtype=pred.dtype,\n",
    "                crs=crs,\n",
    "                transform=transform,\n",
    "            ) as dst:\n",
    "                dst.write(pred, 1)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Exported {i + 1}/{len(all_predictions)}\")\n",
    "    \n",
    "    print(f\"\\nExported to: {output_dir.absolute()}\")\n",
    "else:\n",
    "    print(\"Export disabled. Set EXPORT_RESULTS=True to enable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to Google Cloud Storage\n",
    "UPLOAD_TO_GCS = False  # Set True to upload\n",
    "\n",
    "if EXPORT_RESULTS and UPLOAD_TO_GCS:\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(GCS_BUCKET)\n",
    "    \n",
    "    # Define GCS folder structure\n",
    "    if INCLUDE_LABELS:\n",
    "        gcs_folder = f\"predictions/{sim_names[0].replace('/', '_')}\"\n",
    "    else:\n",
    "        gcs_folder = f\"predictions/{STUDY_AREA}/{pathlib.Path(RAINFALL_SIM).name}\"\n",
    "    \n",
    "    print(f\"Uploading to gs://{GCS_BUCKET}/{gcs_folder}/...\")\n",
    "    \n",
    "    output_dir = pathlib.Path(\"prediction_output\")\n",
    "    for f in output_dir.glob(\"*\"):\n",
    "        blob_path = f\"{gcs_folder}/{f.name}\"\n",
    "        bucket.blob(blob_path).upload_from_filename(str(f))\n",
    "        print(f\"  Uploaded: {blob_path}\")\n",
    "    \n",
    "    print(f\"\\nUpload complete!\")\n",
    "else:\n",
    "    print(\"GCS upload disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Mode: {'Validation (with labels)' if INCLUDE_LABELS else 'Prediction only'}\")\n",
    "print(f\"Chunks processed: {len(all_predictions)}\")\n",
    "print(f\"Prediction steps: {N_STEPS}\")\n",
    "print(f\"Max depth range: {all_predictions.min():.3f} - {all_predictions.max():.3f} m\")\n",
    "\n",
    "if all_labels is not None:\n",
    "    overall_mae = np.abs(all_predictions - all_labels).mean()\n",
    "    print(f\"Overall MAE: {overall_mae:.4f} m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
