{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Model Training Notebook\n",
    "\n",
    "Train a Flood ConvLSTM Model using `usl_models` lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import keras\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from usl_models.flood_ml import constants\n",
    "from usl_models.flood_ml.model import FloodModel\n",
    "from usl_models.flood_ml.dataset import load_dataset_windowed\n",
    "\n",
    "# GPU memory growth setup\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Seed and logging\n",
    "keras.utils.set_random_seed(812)\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", patience=100000, restore_best_weights=True, min_delta=1e-5\n",
    ")\n",
    "\n",
    "\n",
    "# ===== DATA LOADING =====\n",
    "def remove_elevation_features(input_dict, label):\n",
    "    input_dict[\"geospatial\"] = input_dict[\"geospatial\"][..., :8]  # Keep channels 2-8\n",
    "    return input_dict, label\n",
    "\n",
    "\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "sim_names = [\"Atlanta-Atlanta_config/Rainfall_Data_1.txt\"]\n",
    "\n",
    "# Grab only the first 3 samples\n",
    "train_dataset_full = load_dataset_windowed(\n",
    "    sim_names=sim_names, batch_size=1, dataset_split=\"train\"\n",
    ").map(remove_elevation_features)\n",
    "\n",
    "for i, (x, y) in enumerate(train_dataset_full.take(20)):\n",
    "    if tf.reduce_mean(y).numpy() > 0.001:\n",
    "        print(f\"Sample {i} has mean flood depth {tf.reduce_mean(y).numpy():.4f}\")\n",
    "# train_dataset = train_dataset_full.skip(19).take(1).cache().repeat()\n",
    "\n",
    "\n",
    "selected_indices = [7, 12, 19]\n",
    "\n",
    "# Create a dataset of multiple manually selected samples\n",
    "selected_samples = []\n",
    "\n",
    "for idx in selected_indices:\n",
    "    ds = train_dataset_full.skip(idx).take(1)\n",
    "    selected_samples.append(ds)\n",
    "\n",
    "# Concatenate them together\n",
    "train_dataset = selected_samples[0]\n",
    "for ds in selected_samples[1:]:\n",
    "    train_dataset = train_dataset.concatenate(ds)\n",
    "\n",
    "# Cache and repeat\n",
    "train_dataset = train_dataset.cache().repeat()\n",
    "\n",
    "validation_data = (\n",
    "    load_dataset_windowed(sim_names=sim_names, batch_size=1, dataset_split=\"val\")\n",
    "    .map(remove_elevation_features)\n",
    "    .take(3)\n",
    "    .cache()\n",
    ")\n",
    "\n",
    "constants.GEO_FEATURES = 8\n",
    "\n",
    "# ===== MODEL SETUP =====\n",
    "params = FloodModel.Params(\n",
    "    num_features=constants.GEO_FEATURES,\n",
    "    lstm_units=128,\n",
    "    lstm_kernel_size=3,\n",
    "    lstm_dropout=0,\n",
    "    lstm_recurrent_dropout=0,\n",
    "    n_flood_maps=5,\n",
    "    m_rainfall=6,\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n",
    "model = FloodModel(params=params)\n",
    "\n",
    "# ===== TRAINING =====\n",
    "log_dir = f\"logs/training_{timestamp}\"\n",
    "print(f\"Training with 1 sample in {log_dir}\")\n",
    "\n",
    "steps_per_epoch = 10\n",
    "history = model._model.fit(\n",
    "    train_dataset,\n",
    "    epochs=800,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_data,\n",
    "    validation_steps=10,  # optional\n",
    "    callbacks=[keras.callbacks.TensorBoard(log_dir), early_stop],\n",
    ")\n",
    "\n",
    "\n",
    "# ===== SAVE MODEL =====\n",
    "model.save_model(log_dir + \"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# Path to your saved model\n",
    "# model_path = \"logs/Baseline-200epochs/model\"\n",
    "\n",
    "# Load the model\n",
    "# model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the same input used during training\n",
    "val_sample = next(iter(train_dataset))  # Already cached/repeated, returns the same one\n",
    "\n",
    "val_input = val_sample[0]\n",
    "val_gt = val_sample[1].numpy().squeeze()  # shape: (H, W)\n",
    "val_pred = model.call(val_input).numpy().squeeze()  # shape: (H, W)\n",
    "# val_pred = model.call(val_sample[0])\n",
    "print(\n",
    "    \"Prediction stats:\",\n",
    "    tf.reduce_min(val_pred).numpy(),\n",
    "    tf.reduce_max(val_pred).numpy(),\n",
    "    tf.reduce_mean(val_pred).numpy(),\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "vmax_val = max(val_gt.max(), val_pred.max())  # auto scale color\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(val_gt, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "plt.title(\"Ground Truth (Flood Depth)\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(val_pred, cmap=\"Blues\", vmin=0, vmax=vmax_val)\n",
    "plt.title(\"Model Prediction\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(np.abs(val_gt - val_pred), cmap=\"hot\")\n",
    "plt.title(\"Absolute Error\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAE: {np.mean(np.abs(val_gt - val_pred))}\")\n",
    "print(f\"RMSE: {np.sqrt(np.mean((val_gt - val_pred) ** 2))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_sample in train_dataset_full.take(3):\n",
    "    val_input = val_sample[0]\n",
    "    val_gt = val_sample[1].numpy()\n",
    "    val_pred = tf.squeeze(model.call(val_input), axis=-1).numpy()\n",
    "\n",
    "    mae = mean_absolute_error(val_gt.flatten(), val_pred.flatten())\n",
    "    rmse = np.sqrt(np.mean((val_gt.flatten() - val_pred.flatten()) ** 2))\n",
    "    print(f\"MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(val_gt[0], cmap=\"Blues\")\n",
    "    plt.title(\"GT\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(val_pred[0], cmap=\"Blues\")\n",
    "    plt.title(\"Prediction\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(np.abs(val_pred[0] - val_gt[0]), cmap=\"hot\")\n",
    "    plt.title(\"Error\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean GT:\", val_gt.mean(), \"Mean Pred:\", val_pred.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(val_pred.flatten(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample = next(iter(train_dataset))\n",
    "\n",
    "# === Unpack validation input/output ===\n",
    "val_input = val_sample[0]\n",
    "val_gt = val_sample[1]  # shape: (batch, H, W)\n",
    "val_pred = model.call(val_input)  # shape: (batch, H, W, 1)\n",
    "\n",
    "# Remove channels dimension from prediction\n",
    "val_pred = tf.squeeze(val_pred, axis=-1).numpy()  # shape: (batch, H, W)\n",
    "val_gt = val_gt.numpy()\n",
    "\n",
    "# === Compute metrics across all batch samples ===\n",
    "mae_list = []\n",
    "rmse_list = []\n",
    "binary_acc_list = []\n",
    "\n",
    "threshold = 0.01\n",
    "batch_size = val_gt.shape[0]\n",
    "\n",
    "for i in range(batch_size):\n",
    "    gt = val_gt[i]\n",
    "    pred = val_pred[i]\n",
    "\n",
    "    # Compute MAE and RMSE\n",
    "    mae = mean_absolute_error(gt.flatten(), pred.flatten())\n",
    "    rmse = np.sqrt(np.mean((gt.flatten() - pred.flatten()) ** 2))\n",
    "    mae_list.append(mae)\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "    # Threshold maps\n",
    "    gt_bin = (gt > threshold).astype(np.uint8)\n",
    "    pred_bin = (pred > threshold).astype(np.uint8)\n",
    "\n",
    "    # Binary accuracy\n",
    "    correct = (gt_bin == pred_bin).sum()\n",
    "    total = gt_bin.size\n",
    "    binary_acc = correct / total\n",
    "    binary_acc_list.append(binary_acc)\n",
    "\n",
    "# === Report aggregate metrics ===\n",
    "print(f\"Average MAE over batch: {np.mean(mae_list):.4f}\")\n",
    "print(f\"Average RMSE over batch: {np.mean(rmse_list):.4f}\")\n",
    "print(f\"Average Binary Accuracy: {np.mean(binary_acc_list):.4f}\")\n",
    "\n",
    "# Optional: Show basic stats for first few samples\n",
    "print(\"GT max:\", np.max(val_gt))\n",
    "print(\"GT min:\", np.min(val_gt))\n",
    "print(\"Prediction max:\", np.max(val_pred))\n",
    "print(\"Prediction min:\", np.min(val_pred))\n",
    "print(\"GT mean:\", np.mean(val_gt))\n",
    "print(\"GT unique values (sample):\", np.unique(val_gt[0]))\n",
    "\n",
    "# === Visualization for first few samples ===\n",
    "num_samples = min(10, batch_size)  # visualize up to 3 samples\n",
    "for i in range(num_samples):\n",
    "    gt = val_gt[i]\n",
    "    pred = val_pred[i]\n",
    "\n",
    "    print(f\"\\nSample {i}\")\n",
    "    print(\"GT max:\", np.max(gt), \"GT min:\", np.min(gt))\n",
    "    print(\"Prediction max:\", np.max(pred), \"Prediction min:\", np.min(pred))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axs[0].imshow(gt, cmap=\"Blues\", vmin=0, vmax=0.08)\n",
    "    axs[0].set_title(f\"GT Flood Map #{i}\")\n",
    "    axs[1].imshow(pred, cmap=\"Blues\", vmin=0, vmax=0.08)\n",
    "    axs[1].set_title(f\"Prediction #{i}\")\n",
    "    axs[2].imshow(np.abs(gt - pred), cmap=\"hot\")\n",
    "    axs[2].set_title(\"Error Map\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
